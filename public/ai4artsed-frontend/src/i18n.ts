import { createI18n } from 'vue-i18n'

export const SUPPORTED_LANGUAGES = [
  { code: 'de', label: 'Deutsch' },
  { code: 'en', label: 'English' },
  { code: 'tr', label: 'Türkçe' },
] as const

export type SupportedLanguage = typeof SUPPORTED_LANGUAGES[number]['code']

/** Localized string object — en is mandatory, all others optional with fallback */
export type LocalizedString = { en: string; [key: string]: string }

/** Resolve a localized string for the given locale, falling back to English */
export function localized(obj: Record<string, string>, locale: string): string {
  return obj[locale] || obj.en || ''
}

const messages = {
  de: {
    app: {
      title: 'UCDCAE AI LAB',
      subtitle: 'Kreative KI-Transformationen'
    },
    form: {
      inputLabel: 'Dein Text',
      inputPlaceholder: 'z.B. Eine Blume auf der Wiese',
      schemaLabel: 'Transformationsstil',
      executeModeLabel: 'Ausführungsmodus',
      safetyLabel: 'Sicherheitsstufe',
      generateButton: 'Generieren'
    },
    schemas: {
      dada: 'Dada (Zufällig & Absurd)',
      bauhaus: 'Bauhaus (Geometrisch)',
      stillepost: 'Stille Post (Iterativ)'
    },
    executionModes: {
      eco: 'Eco (Schnell)',
      fast: 'Fast (Ausgewogen)',
      best: 'Best (Qualität)'
    },
    safetyLevels: {
      kids: 'Kinder',
      youth: 'Jugend',
      adult: 'Erwachsene',
      research: 'Forschung'
    },
    stages: {
      pipeline_starting: 'Pipeline startet',
      translation_and_safety: 'Übersetzung & Sicherheit',
      interception: 'Transformation',
      pre_output_safety: 'Ausgabe-Sicherheit',
      media_generation: 'Bild-Generierung',
      completed: 'Abgeschlossen'
    },
    status: {
      idle: 'Bereit',
      executing: 'Pipeline läuft...',
      connectionSlow: 'Verbindung langsam, Versuch läuft...',
      completed: 'Pipeline abgeschlossen!',
      error: 'Fehler aufgetreten'
    },
    entities: {
      input: 'Eingabe',
      translation: 'Übersetzung',
      safety: 'Sicherheitscheck',
      interception: 'Transformation',
      safety_pre_output: 'Ausgabe-Sicherheit',
      media: 'Generiertes Bild'
    },
    properties: {
      chill: 'chillig',
      chaotic: 'wild',
      narrative: 'Geschichten erzählen',
      algorithmic: 'nach Regeln gehen',
      historical: 'Geschichte',
      contemporary: 'Gegenwart',
      explore: 'KI austesten',
      create: 'Kunst machen',
      playful: 'bisschen verrückt',
      serious: 'eher ernst'
    },
    phase2: {
      title: 'Prompt-Eingabe',
      userInput: 'Dein Input',
      yourInput: 'Dein Input',
      yourIdea: 'Deine Idee: Um WAS soll es hier gehen?',
      rules: 'Deine Regeln: WIE soll Deine Idee umgesetzt werden?',
      yourInstructions: 'Deine Anweisungen',
      what: 'WAS',
      how: 'WIE',
      userInputPlaceholder: 'z.B. Eine Blume auf der Wiese',
      inputPlaceholder: 'Dein Text erscheint hier...',
      metaPrompt: 'Künstlerische Anweisung',
      instruction: 'Instruction',
      transformation: 'Künstlerische Transformation',
      metaPromptPlaceholder: 'Beschreibe die Transformation...',
      result: 'Ergebnis',
      expectedResult: 'Erwartetes Ergebnis',
      execute: 'Pipeline ausführen',
      executing: 'Läuft...',
      transforming: 'LLM transformiert...',
      startTransformation: 'Transformation starten',
      letsGo: 'Ok, leg los!',
      modified: 'Geändert',
      reset: 'Zurücksetzen',
      loadingConfig: 'Lade Konfiguration...',
      loadingMetaPrompt: 'Lade Meta-Prompt...',
      errorLoadingConfig: 'Fehler beim Laden der Konfiguration',
      errorLoadingMetaPrompt: 'Fehler beim Laden des Meta-Prompts',
      threeForces: '3 Kräfte wirken zusammen',
      twoForces: 'WAS + WIE → LLM → Ergebnis',
      yourPrompt: 'Dein Prompt:',
      writeYourText: 'Schreibe deinen Text...',
      examples: 'Beispiele',
      estimatedTime: '~12 Sekunden',
      stage12Time: '~5-10 Sekunden',
      willAppearAfterExecution: 'Wird nach Ausführung erscheinen...',
      back: 'Zurück',
      retry: 'Erneut versuchen',
      transformedPrompt: 'Transformierter Prompt',
      notYetTransformed: 'Noch nicht transformiert...',
      transform: 'Transformieren',
      reTransform: 'Noch mal anders',
      startAI: 'KI, bearbeite meine Eingabe',
      aiWorking: 'KI arbeitet...',
      continueToMedia: 'Weiter zum Bild generieren',
      readyForMedia: 'Bereit für Bildgenerierung',
      stage1: 'Stage 1: Übersetzung + Sicherheit...',
      stage2: 'Stage 2: Transformation...',
      selectMedia: 'Wähle dein Medium:',
      mediaImage: 'Bild',
      mediaAudio: 'Sound',
      mediaVideo: 'Video',
      media3D: '3D',
      comingSoon: 'Bald verfügbar',
      generateMedia: 'Start!'
    },
    phase3: {
      generating: 'Bild wird generiert...',
      generatingHint: '~30 Sekunden'
    },
    common: {
      back: 'Zurück',
      loading: 'Lädt...',
      error: 'Fehler',
      retry: 'Erneut versuchen',
      cancel: 'Abbrechen'
    },
    gallery: {
      title: 'Favoriten',  // Session 145: "Meine" redundant mit Switch
      empty: 'Noch keine Favoriten',
      favorite: 'Zu Favoriten',
      unfavorite: 'Aus Favoriten entfernen',
      continue: 'Weiterentwickeln',
      restore: 'Wiederherstellen',
      viewMine: 'Meine Favoriten',  // Session 145
      viewAll: 'Alle Favoriten'  // Session 145
    },
    settings: {
      authRequired: 'Authentifizierung erforderlich',
      authPrompt: 'Bitte geben Sie das Passwort ein, um auf die Einstellungen zuzugreifen:',
      passwordPlaceholder: 'Passwort eingeben...',
      authenticate: 'Anmelden',
      authenticating: 'Authentifiziere...'
    },
    pipeline: {
      yourInput: 'Dein Input',
      result: 'Ergebnis',
      generatedMedia: 'Erzeugtes Bild'
    },
    landing: {
      subtitlePrefix: 'Pädagogisch-künstlerische Experimentierplattform des',
      subtitleSuffix: 'für den explorativen Einsatz von generativer KI in der kulturell-ästhetischen Medienbildung',
      research: '',
      features: {
        textTransformation: {
          title: 'Text-Transformation',
          description: 'Perspektivwechsel durch KI: Finde und verändere Deine Ideen durch künstlerische Haltungen und Verfremdungen.'
        },
        imageTransformation: {
          title: 'Bild-Transformation',
          description: 'Bilder durch verschiedene Modelle und Perspektiven in neue Bilder und Videos verwandeln.'
        },
        multiImage: {
          title: 'Bildfusion',
          description: 'Mehrere Bilder kombinieren und durch KI-Modelle zu neuen Bild-Kompositionen verschmelzen.'
        },
        canvas: {
          title: 'Canvas Workflow',
          description: 'Visuelle Workflow-Komposition — Module per Drag & Drop zu eigenen KI-Pipelines verbinden.'
        },
        music: {
          title: 'Musikgenerierung',
          description: 'Experimentiere mit Musik, Sound und Lyrics.'
        },
        latentLab: {
          title: 'Latent Lab',
          description: 'Vektorraum-Forschung — Surrealisierung, Dimensionselimination, Embedding-Interpolation.'
        }
      }
    },
    research: {
      locked: 'Nur im Forschungsmodus verfügbar',
      lockedHint: 'Erfordert Safety-Level „Erwachsene" oder „Forschung" (config.py)',
      complianceTitle: 'Hinweis zum Forschungsmodus',
      complianceWarning: 'Im Forschungsmodus sind keine Sicherheitsfilter für Prompts und generierte Bilder aktiv. Es können unerwartete oder unangemessene Ergebnisse entstehen.',
      complianceAge: 'Dieser Modus ist nicht empfohlen für Personen unter 16 Jahren.',
      complianceConfirm: 'Ich bestätige, dass ich die Hinweise verstanden habe',
      complianceCancel: 'Abbrechen',
      complianceProceed: 'Fortfahren'
    },
    presetOverlay: {
      title: 'Perspektive wählen',
      close: 'Schließen'
    },
    imageUpload: {
      clickHere: 'Klicke hier',
      orDragImage: 'oder ziehe ein Bild hierher',
      formatHint: 'PNG, JPG, WEBP (max 10MB)',
      invalidFormat: 'Ungültiges Dateiformat. Nur PNG, JPG und WEBP erlaubt.',
      fileTooLarge: 'Datei zu groß. Maximum: {max}MB',
      uploadFailed: 'Upload fehlgeschlagen',
      infoOriginal: 'Original:',
      infoSize: 'Größe:'
    },
    mediaInput: {
      choosePreset: 'Perspektive wählen',
      translateToEnglish: 'Ins Englische übersetzen',
      copy: 'Kopieren',
      paste: 'Einfügen',
      delete: 'Löschen',
      loading: 'Lädt...',
      contentBlocked: 'Inhalt blockiert'
    },
    nav: {
      about: 'Über das Projekt',
      impressum: 'Impressum',
      privacy: 'Datenschutz',
      docs: 'Dokumentation',
      language: 'Sprache wechseln',
      settings: 'Einstellungen',
      canvas: 'Canvas Workflow'
    },
    canvas: {
      title: 'Canvas Workflow',
      newWorkflow: 'Neuer Workflow',
      importWorkflow: 'Importieren',
      exportWorkflow: 'Exportieren',
      execute: 'Ausführen',
      ready: 'Bereit',
      errors: 'Fehler',
      discardWorkflow: 'Aktuellen Workflow verwerfen?',
      importError: 'Fehler beim Importieren der Datei',
      selectTransformation: 'Transformation wählen',
      selectOutput: 'Ausgabe-Modell wählen',
      search: 'Suchen...',
      noResults: 'Keine Ergebnisse gefunden',
      dragHint: 'Klicke oder ziehe Module auf die Arbeitsfläche',
      editNameHint: '(doppelklicken zum Bearbeiten)',
      modules: 'Module',
      toggleSidebar: 'Sidebar ein/aus',
      dsgvoTooltip: 'Canvas-Workflows können externe LLM-APIs nutzen. Die DSGVO-Konformität liegt in der Verantwortung der Nutzer:innen.',
      batchExecute: 'Batch-Ausführung',
      batchExecution: 'Batch-Ausführung',
      batchAbort: 'Batch abbrechen',
      abort: 'Abbrechen',
      cancel: 'Abbrechen',
      loading: 'Laden...',
      executingWorkflow: 'Workflow wird ausgeführt...',
      starting: 'Starte...',
      nodes: 'Knoten',
      batchRunCount: 'Anzahl Runs',
      batchUseSeed: 'Basis-Seed verwenden',
      batchBaseSeed: 'Basis-Seed',
      batchSeedHint: 'Jeder Run: Seed + Index',
      batchStart: 'Batch starten',
      stage: {
        configSelectPlaceholder: 'Auswählen...',
        evaluationCriteriaFallback: 'Bewertungskriterien...',
        branchFalseDefault: 'Falsch',
        branchTrueDefault: 'Wahr',
        feedbackInputTitle: 'Feedback-Eingang',
        deleteTitle: 'Löschen',
        selectLlmPlaceholder: 'LLM wählen...',
        resizeTitle: 'Größe ändern',
        input: {
          promptPlaceholder: 'Dein Prompt...'
        },
        imageInput: {
          uploadLabel: 'Bild hochladen'
        },
        interception: {
          contextPromptLabel: 'Context-Prompt',
          contextPromptPlaceholder: 'Transformations-Anweisungen...'
        },
        translation: {
          translationPromptLabel: 'Übersetzungs-Prompt',
          translationPromptPlaceholder: 'Übersetzungsanweisungen...'
        },
        modelAdaption: {
          targetModelLabel: 'Zielmodell',
          noAdaptionOption: 'Keine Adaption',
          videoModelsOption: 'Video-Modelle',
          audioModelsOption: 'Audio-Modelle'
        },
        comparisonEvaluator: {
          criteriaLabel: 'Vergleichs-Kriterien',
          criteriaPlaceholder: 'z.B. Vergleiche nach Originalität, Klarheit, Detailreichtum...',
          infoText: 'Verbinde bis zu 3 Text-Outputs'
        },
        seed: {
          modeLabel: 'Modus',
          modeFixed: 'Fest',
          modeRandom: 'Zufällig',
          valueLabel: 'Wert',
          baseLabel: 'Basis'
        },
        resolution: {
          customOption: 'Benutzerdefiniert',
          widthLabel: 'Breite',
          heightLabel: 'Höhe'
        },
        collector: {
          emptyText: 'Warte auf Ausführung...'
        },
        evaluation: {
          typeLabel: 'Bewertungstyp',
          typeCreativity: 'Kreativität',
          typeQuality: 'Qualität',
          typeCustom: 'Eigene',
          criteriaLabel: 'Bewertungskriterien',
          outputTypeLabel: 'Ausgabe-Typ',
          outputCommentary: 'Kommentar + Binary',
          outputScore: 'Kommentar + Score + Binary',
          outputAll: 'Alle',
          enableBranching: 'Verzweigung aktivieren',
          branchConditionLabel: 'Verzweigungsbedingung',
          branchThresholdOption: 'Schwellwert (Score)',
          thresholdLabel: 'Schwellwert (0-10)',
          trueLabelFieldLabel: 'Label "Pass/True"',
          trueLabelDefault: 'Bestanden',
          trueLabelPlaceholder: 'z.B. Bestanden',
          falseLabelFieldLabel: 'Label "Fail/False"',
          falseLabelDefault: 'Revision nötig',
          falseLabelPlaceholder: 'z.B. Revision nötig',
          connectorPassthrough: 'Passthrough (OK - unverändert)',
          connectorCommented: 'Kommentiert (FAIL - mit Feedback)',
          connectorCommentary: 'Nur Kommentar (für Anzeige)'
        },
        imageEvaluation: {
          visionModelPlaceholder: 'Vision-Modell wählen...',
          frameworkLabel: 'Analyse-Framework',
          frameworkPanofsky: 'Kunsthistorisch (Panofsky)',
          frameworkEducational: 'Bildungstheoretisch',
          frameworkEthical: 'Ethisch',
          frameworkCritical: 'Kritisch/Dekolonial',
          frameworkCustom: 'Eigene Anweisung',
          customPromptLabel: 'Analyse-Prompt',
          customPromptPlaceholder: 'Beschreibe, wie das Bild analysiert werden soll...'
        },
        display: {
          imageAlt: 'Vorschau',
          emptyText: 'Vorschau (nach Ausführung)'
        }
      }
    },
    about: {
      title: 'Über das UCDCAE AI LAB',
      intro: 'Das UCDCAE AI LAB ist eine pädagogisch-künstlerische Experimentierplattform des UNESCO Chair in Digital Culture and Arts in Education für den explorativen Einsatz von generativer Künstlicher Intelligenz in der kulturell-ästhetischen Medienbildung. Es wurde im Rahmen der Projekte AI4ArtsEd und COMeARTS entwickelt.',
      project: {
        title: 'Das Projekt',
        description: 'KI verändert Gesellschaft und Arbeitswelt; sie wird zunehmend Thema der Bildung. Das Projekt sondiert Chancen, Bedingungen und Grenzen des pädagogischen Einsatzes künstlicher Intelligenz (KI) in kulturell diversitätssensiblen Settings der Kulturellen Bildung (KuBi).',
        paragraph2: 'In drei Teilprojekten – Allgemeinpädagogik (TPap), Informatik (TPinf) und Kunstpädagogik (TPkp) – greifen kreativitätsorientierte pädagogische KI-Praxisforschung und informatische KI-Konzeption und Programmierung in enger Kooperation ineinander. Das Projekt bezieht hierzu von Beginn an künstlerisch-pädagogische Praxisakteure in den Gestaltungsprozess systematisch ein; es agiert als Brücke zwischen der professionellen (qualitätsbezogenen, ästhetischen, ethischen und wertebezogenen) pädagogisch-praktischen Implementation einerseits und dem Umsetzungs- und Trainingsprozess des informatischen Teilprojekts andererseits.',
        paragraph3: 'Aus einem insgesamt ca. zweijährigen partizipativen Designprozess soll eine Opensource-KI-Technologie hervorgehen, die auslotet, inwieweit KI-Systeme unter günstigen Realbedingungen bereits auf ihrer Strukturebene künstlerisch-pädagogische Maßgaben einbeziehen können.',
        paragraph4: 'Dabei stehen a) die zukünftige Anwendbarkeit und der Mehrgewinn hochinnovativer Technologien für die Kulturelle Bildung im Zentrum, b) Reichweite und Grenzen der KI-Literacy von Lehrenden und Lernenden, sowie c) die übergreifende Frage nach der Bewertbarkeit und Bewertung der Transformation pädagogischer Settings durch komplexe nonhumane Akteure im Sinne einer pädagogischen Ethik und Technikfolgenabschätzung.',
        moreInfo: 'Weitere Informationen:'
      },
      subproject: {
        title: 'Teilprojekt "Allgemeine Pädagogik"',
        description: 'Das Teilprojekt "Allgemeine Pädagogik" beforscht im Rahmen der dem Verbundprojekt gemeinsamen Fragestellung Möglichkeiten und Grenzen eines auf partizipativer Praxisforschung aufsetzenden künstlerisch-pädagogischen KI-Designprozesses. Es führt zu diesem Zweck im ersten Projektjahr eine Serie von Recherchen, Analysen, Expert_innenworkshops und OpenSpaces durch. Die nachfolgende, in mehreren Zyklen als Feedback-Loop angelegte Projektphase erforscht den Einsatz eines Prototypen mit pädagogischen Prakter_innen und Artist-Educators v.a. der non-formalen kulturellen Bildung als relationalen und kollektiven transformativen Bildungsprozess.'
      },
      team: {
        title: 'Team',
        projectLead: 'Projektleitung',
        leadName: 'Prof. Dr. Benjamin Jörissen',
        leadInstitute: 'Institut für Pädagogik',
        leadChair: 'Lehrstuhl für Pädagogik mit dem Schwerpunkt Kultur und ästhetische Bildung',
        leadUnesco: 'UNESCO Chair in Digital Culture and Arts in Education',
        researcher: 'Wissenschaftliche Mitarbeiterin',
        researcherName: 'Vanessa Baumann',
        researcherInstitute: 'Institut für Pädagogik',
        researcherChair: 'Lehrstuhl für Pädagogik mit dem Schwerpunkt Kultur und ästhetische Bildung',
        researcherUnesco: 'UNESCO Chair in Digital Culture and Arts in Education'
      },
      funding: {
        title: 'Gefördert vom'
      }
    },
    legal: {
      impressum: {
        title: 'Impressum',
        publisher: 'Herausgeber',
        represented: 'Vertreten durch den Präsidenten',
        responsible: 'Inhaltlich verantwortlich gem. § 18 Abs. 2 MStV',
        authority: 'Zuständige Aufsichtsbehörde',
        moreInfo: 'Weitere Informationen',
        moreInfoText: 'Das vollständige Impressum der FAU:',
        funding: 'Gefördert vom'
      },
      privacy: {
        title: 'Datenschutzerklärung',
        notice: 'Hinweis: Generierte Inhalte werden zu Forschungszwecken auf dem Server gespeichert. Es werden keine User- oder IP-Daten erfasst. Hochgeladene Bilder werden nicht gespeichert.',
        usage: 'Die Nutzung dieser Plattform ist ausschließlich eingetragenen Kooperationspartnern des UCDCAE AI LAB erlaubt. Es gelten die in diesem Rahmen vereinbarten datenschutzbezogenen Absprachen. Haben Sie hierzu Fragen, melden Sie sich bitte bei vanessa.baumann@fau.de.'
      }
    },
    docs: {
      title: 'Dokumentation & Anleitung',
      intro: {
        title: 'Willkommen',
        content: 'Kreative Experimente mit KI-Transformationen.'
      },
      gettingStarted: {
        title: 'Erste Schritte',
        step1: 'Eigenschaften aus Quadranten wählen',
        step2: 'Text oder Bild eingeben',
        step3: 'Transformation starten'
      },
      modes: {
        title: 'Modi',
        mode1: { name: 'Direkt', desc: 'Schnelle Experimente' },
        mode2: { name: 'Text', desc: 'Textbasierte Transformationen' },
        mode3: { name: 'Bild', desc: 'Bildbasierte Verfahren' }
      },
      support: {
        title: 'Unterstützung',
        content: 'Bei Fragen:'
      },
      wikipedia: {
        title: 'Wikipedia-Recherche',
        subtitle: 'Wissen über die Welt als Teil künstlerischer Prozesse',
        feature: 'Künstlerische Prozesse erfordern nicht nur ästhetisches Wissen, sondern auch Wissen über Sachverhalte in der Welt. Die KI recherchiert während der Transformation auf Wikipedia, um faktische Informationen zu finden.',
        languages: 'Über 70 Sprachen werden unterstützt',
        languagesDesc: 'Die KI wählt automatisch die passende sprachliche Wikipedia für das jeweilige Thema:',
        examples: {
          nigeria: 'Thema über Nigeria → Hausa, Yoruba, Igbo oder Englisch',
          india: 'Thema über Indien → Hindi, Tamil, Bengali oder andere regionale Sprachen',
          indigenous: 'Indigene Kulturen → Quechua, Māori, Inuktitut usw.'
        },
        why: 'Transparenz: Was weiß die KI?',
        whyDesc: 'Das System zeigt alle Recherche-Versuche an: Sowohl gefundene Artikel (als anklickbare Links) als auch Begriffe, zu denen nichts gefunden wurde. So wird sichtbar, was die KI zu wissen meint – und was nicht.',
        culturalRespect: 'Einladung zum Selbst-Recherchieren',
        culturalRespectDesc: 'Die angezeigten Wikipedia-Links sind eine Einladung, selbst mehr zu erfahren. Klicken Sie auf die Links, um die Quellen zu prüfen und Ihr eigenes Wissen zu erweitern.',
        limitations: 'Die KI-Recherche ist ein Hilfsmittel, kein Ersatz für eigene Auseinandersetzung mit dem Thema.'
      }
    },
    multiImage: {
      image1Label: 'Bild 1',
      image2Label: 'Bild 2 (optional)',
      image3Label: 'Bild 3 (optional)',
      contextLabel: 'Sage was Du mit den Bildern machen möchtest',
      contextPlaceholder: 'z.B. Füge das Haus aus Bild 2 und das Pferd aus Bild 3 in Bild 1 ein. Behalte Farben und Stil von Bild 1 bei.',
      modeTitle: 'Mehrere Bilder → Bild',
      selectConfig: 'Wähle dein Modell:',
      generating: 'Bilder werden fusioniert...'
    },
    imageTransform: {
      imageLabel: 'Dein Bild',
      contextLabel: 'Sage was Du an dem Bild verändern möchtest',
      contextPlaceholder: 'z.B. Verwandle es in ein Ölgemälde... Mache es bunter... Füge einen Sonnenuntergang hinzu...'
    },
    textTransform: {
      inputLabel: 'Deine Idee = WAS?',
      inputTooltip: 'Hier trägst Du ein, worum es gehen soll.',
      inputPlaceholder: 'z.B. Ein Fest in meiner Straße: ...',
      contextLabel: 'Deine Regeln = WIE?',
      contextTooltip: 'Hier trägst Du ein, wie Deine Idee dargestellt werden soll, oder klicke auf das Kreis-Symbol!',
      contextPlaceholder: 'z.B. Beschreibe alles so, wie es die Vögel auf den Bäumen wahrnehmen!',
      resultLabel: 'Idee + Regeln = Prompt',
      resultPlaceholder: 'Prompt erscheint nach Start-Klick (oder eigenen Text eingeben)',
      optimizedLabel: 'Modell-Optimierter Prompt',
      optimizedPlaceholder: 'Der optimierte Prompt erscheint nach Modellauswahl.'
    },
    training: {
      info: {
        title: 'Hinweis zum LoRA-Training',
        studioDescription: 'Trainiere eigene LoRA-Modelle für Stable Diffusion 3.5 Large mit deinen Bildern.',
        description: 'Dieses eingebaute Training ist für schnelle Tests gedacht.',
        limitations: 'Einschränkungen',
        limitationDuration: 'Training dauert 1-3 Stunden',
        limitationBlocking: 'Blockiert die Bildgenerierung während des Trainings',
        limitationConfig: 'Begrenzte Konfigurationsmöglichkeiten',
        showMore: 'Mehr erfahren',
        showLess: 'Weniger anzeigen'
      },
      placeholders: {
        projectName: 'z.B. Unser Schulgebäude',
        triggerWords: 'z.B. unser_schulgebaeude, schulhof, klassenzimmer'
      },
      labels: {
        projectName: 'Projektname',
        triggerWords: 'Trigger-Wörter',
        triggerHelp: 'Kommagetrennte Tags. Erstes = Haupt-Trigger, Rest = zusätzliche Tags pro Bild.',
        images: 'Trainingsbilder (10–50 empfohlen)',
        dropZone: 'Bilder hierher ziehen oder klicken',
        imagesSelected: '{count} Bilder ausgewählt',
        logs: 'Trainings-Log',
        waiting: 'Warte auf Trainingsstart...'
      },
      buttons: {
        start: 'Training starten',
        stop: 'Stopp',
        inProgress: 'Training läuft...',
        delete: 'Projektdaten löschen (DSGVO)',
        cancel: 'Abbrechen'
      },
      vram: {
        title: 'GPU VRAM Prüfung',
        checking: 'Prüfe VRAM...',
        used: 'belegt',
        free: 'frei',
        notEnough: 'Nicht genügend freier VRAM für das Training (benötigt {gb} GB).',
        clearQuestion: 'VRAM freigeben um fortzufahren?',
        enough: 'Genügend VRAM für das Training verfügbar.',
        clearing: 'Gebe VRAM frei...',
        newFree: 'Neu verfügbar',
        clearBtn: 'ComfyUI + Ollama VRAM freigeben'
      }
    },
    safetyBadges: {
      '§86a': '§86a',
      '86a_filter': '§86a',
      age_filter: 'Altersfilter',
      dsgvo_ner: 'DSGVO',
      dsgvo_llm: 'DSGVO',
      translation: '\u2192 EN',
      fast_filter: 'Inhalt',
      llm_context_check: 'Inhalt (LLM)',
      llm_safety_check: 'Jugendschutz',
      llm_check_failed: 'Pr\u00FCfung fehlgeschlagen',
      disabled: '\u2014'
    },
    safetyBlocked: {
      vlm: 'Dein Prompt war in Ordnung, aber das erzeugte Bild wurde von einer Bildanalyse-KI als ungeeignet eingestuft. Das kann passieren \u2014 die Bildgenerierung ist nicht immer vorhersagbar. Versuche es einfach nochmal, jede Generierung ist anders!',
      para86a: 'Dein Prompt wurde blockiert, weil er Symbole oder Begriffe enth\u00E4lt, die nach deutschem Recht (\u00A786a StGB) verboten sind. Diese Regel sch\u00FCtzt uns alle vor Hass und Gewalt. Versuche es mit einem anderen Thema!',
      dsgvo: 'Dein Prompt wurde blockiert, weil er etwas enth\u00E4lt, das wie ein Personenname aussieht. Das ist durch die Datenschutzgrundverordnung (DSGVO) gesch\u00FCtzt. Verwende stattdessen Beschreibungen wie \"ein M\u00E4dchen\" oder \"ein alter Mann\" statt Namen.',
      kids: 'Dein Prompt wurde vom Kinder-Schutzfilter blockiert. Manche Begriffe sind f\u00FCr Kinder nicht geeignet, weil sie erschreckend oder verst\u00F6rend sein k\u00F6nnen. Versuche, deine Idee mit freundlicheren Worten zu beschreiben!',
      youth: 'Dein Prompt wurde vom Jugendschutzfilter blockiert. Manche Inhalte sind auch f\u00FCr Jugendliche nicht geeignet. Versuche, deine Idee anders zu formulieren!',
      generic: 'Dein Prompt wurde vom Sicherheitssystem blockiert. Das System sch\u00FCtzt dich vor ungeeigneten Inhalten. Versuche es mit einer anderen Formulierung!',
      inputImage: 'Das hochgeladene Bild wurde von einer Bildanalyse-KI als ungeeignet eingestuft. Bitte verwende ein anderes Bild.',
      vlmSaw: 'Die Bild-KI sah',
      systemUnavailable: 'Das Sicherheitssystem (Ollama) reagiert nicht, daher kann keine weitere Verarbeitung erfolgen. Bitte den Systemadministrator kontaktieren.',
      suggestionLoading: 'Moment, ich habe eine Idee...',
      suggestionError: 'Ich konnte gerade keinen Vorschlag generieren. Versuch es einfach nochmal anders!'
    },
    splitCombine: {
      infoTitle: 'Split & Combine - Semantische Vektorfusion',
      infoDescription: 'Dieser Workflow fusioniert zwei Prompts auf der Ebene semantischer Vektoren. Das Ergebnis ist keine einfache Mischung, sondern eine tiefere mathematische Verbindung der Bedeutungsräume.',
      purposeTitle: 'Pädagogischer Zweck',
      purposeText: 'Erkunde, wie KI-Modelle Bedeutung als Zahlenräume repräsentieren. Was passiert, wenn wir verschiedene Konzepte mathematisch verschmelzen?',
      techTitle: 'Technische Details',
      techText: 'Modell: SD3.5 Large | Encoder: DualCLIP (CLIP-G + T5-XXL)'
    },
    partialElimination: {
      infoTitle: 'Partial Elimination - Vektor-Dekonstruktion',
      infoDescription: 'Dieser Workflow manipuliert gezielt Teile des semantischen Vektors. Durch das Eliminieren bestimmter Dimensionen können wir beobachten, welche Aspekte der Bedeutung verloren gehen.',
      purposeTitle: 'Pädagogischer Zweck',
      purposeText: 'Verstehe, wie Bedeutung in verschiedenen Dimensionen des Vektorraums kodiert ist. Was bleibt übrig, wenn wir Teile "ausschalten"?',
      techTitle: 'Technische Details',
      techText: 'Modell: SD3.5 Large | Encoder: TripleCLIP (CLIP-L + CLIP-G + T5-XXL)',
      encoderLabel: 'Text-Encoder',
      modeLabel: 'Eliminationsmodus',
      dimensionRange: 'Dimensions-Bereich',
      selected: 'Ausgewählt',
      dimensions: 'Dimensionen',
      emptyTitle: 'Warte auf Generierung...',
      emptySubtitle: 'Die Ergebnisse erscheinen hier',
      referenceLabel: 'Referenzbild',
      referenceDesc: 'Unmanipulierte Ausgabe (Original)',
      innerLabel: 'Innerer Bereich eliminiert',
      outerLabel: 'Äußerer Bereich eliminiert'
    },
    surrealizer: {
      infoTitle: 'Hallucinator — Extrapolation jenseits des Bekannten',
      infoDescription: 'Zwei KI-"Gehirne" lesen deinen Text: CLIP-L versteht Sprache durch Bilder, T5 versteht sie rein sprachlich. Der Regler mischt nicht einfach zwischen beiden — er schiebt das Bild weit über das hinaus, was T5 allein erzeugen würde. Die KI muss dann Vektoren interpretieren, die sie im Training nie gesehen hat. Das Ergebnis: KI-Halluzinationen — Bilder, die kein Prompt direkt erzeugen könnte.',
      purposeTitle: 'Der Regler',
      purposeText: 'α < 0: CLIP-L wird verstärkt, T5 negiert — die oberen 3328 Dimensionen (wo CLIP-L nur Nullen hat) erhalten invertierte T5-Vektoren. Die Cross-Attention-Muster im Transformer kehren sich um: visuell getriebene Halluzinationen. ◆ α = 0: reines CLIP-L — normales Bild. ◆ α = 1: reines T5-XXL — noch normal, aber andere Qualität. ◆ α > 1: Extrapolation über T5 hinaus. Bei α = 20 schiebt die Formel das Embedding 19× über T5 hinweg in unerforschten Vektorraum — sprachlich getriebene Halluzinationen. ◆ Sweet Spot: α = 15–35.',
      techTitle: 'Wie es funktioniert',
      techText: 'Dein Prompt wird getrennt durch zwei Encoder geschickt: CLIP-L (visuell trainiert, 77 Tokens, 768 Dimensionen → aufgefüllt auf 4096) und T5-XXL (sprachlich trainiert, 512 Tokens, 4096 Dimensionen). Die ersten 77 Token-Positionen werden per Formel fusioniert: (1-α)·CLIP-L + α·T5. Die restlichen T5-Tokens (78–512) bleiben unverändert als semantischer Anker — sie halten das Bild an deinem Text fest, egal wie extrem α wird. Bei α > 1 entsteht keine Mischung, sondern Extrapolation: Vektoren, die kein Training je erzeugt hat. Bei α < 0 wird T5 negiert und CLIP-L verstärkt — qualitativ andere Halluzinationen, weil die Cross-Attention-Muster im Transformer invertiert werden.',
      sliderLabel: 'Extrapolation (α)',
      sliderNormal: 'normal',
      sliderWeird: 'weird',
      sliderCrazy: 'crazy',
      sliderExtremeWeird: 'super weird',
      sliderExtremeCrazy: 'super crazy',
      sliderHint: "α<0: über CLIP hinaus {'|'} α=0: reines CLIP {'|'} α=1: reines T5 {'|'} α>1: über T5 hinaus",
      expandLabel: 'Prompt für T5 erweitern',
      expandSuggest: 'Kurzer Prompt erkannt — T5-Erweiterung verbessert die Ergebnisse bei wenigen Wörtern deutlich.',
      expandHint: 'Dein Prompt hat wenige Wörter (~{count} CLIP-Tokens). Für optimale Halluzinationen kann die KI den T5-Kontext narrativ erweitern.',
      expandActive: 'Erweitere Prompt...',
      expandResultLabel: 'T5-Erweiterung (nur für T5-Encoder)',
      advancedLabel: 'Weitere Einstellungen',
      negativeLabel: 'Negativ-Prompt',
      negativeHint: 'Wird mit gleichem α extrapoliert. Bestimmt, woVON das Bild weg-extrapoliert wird — verschiedene Negativ-Prompts erzeugen grundlegend verschiedene Bildästhetiken.',
      cfgLabel: 'CFG Scale',
      cfgHint: 'Classifier-Free Guidance: Stärke des Prompt-Einflusses. Höher = stärkerer Effekt, weniger Variation.'
    },
    musicGeneration: {
      infoTitle: 'Musik-Generierung',
      infoDescription: 'Erstelle Musik aus Texten und Style-Tags. Die KI generiert Melodien, Rhythmen und Harmonien basierend auf deinen Lyrics und Genre-Angaben.',
      purposeTitle: 'Pädagogischer Zweck',
      purposeText: 'Erkunde wie KI musikalische Konzepte interpretiert. Wie beeinflusst die Wortwahl in den Lyrics die Melodie?',
      lyricsLabel: 'Lyrics (Text)',
      lyricsPlaceholder: '[Verse]\nDeine Lyrics hier...\n\n[Chorus]\nRefrain...',
      tagsLabel: 'Style Tags',
      tagsPlaceholder: 'pop, piano, upbeat, female vocal, 120bpm',
      selectModel: 'Wähle ein Musik-Modell:',
      generate: 'Musik generieren',
      generating: 'Musik wird generiert...'
    },
    musicGen: {
      simpleMode: 'Einfach',
      advancedMode: 'Erweitert',
      lyricsLabel: 'Lyrics',
      lyricsPlaceholder: 'Schreibe deine Song-Lyrics mit Strukturmarkern wie [Verse], [Chorus], [Bridge]...\n\nBeispiel:\n[Verse]\nde doo doo doo\nde blaa blaa blaa\n\n[Chorus]\nis all I want to sing to you',
      tagsLabel: 'Style Tags',
      tagsPlaceholder: 'Genre, Stimmung, Instrumente...\n\nBeispiel: ska, aggressive, upbeat, high definition, bass and sax trio',
      refineButton: 'Lyrics & Tags verfeinern',
      refinedLyricsLabel: 'Verfeinerte Lyrics',
      refinedLyricsPlaceholder: 'Hier erscheinen deine verfeinerten Lyrics...',
      refiningLyricsMessage: 'Die KI verfeinert deine Lyrics...',
      refinedTagsLabel: 'Verfeinerte Tags',
      refinedTagsPlaceholder: 'Hier erscheinen die verfeinerten Style Tags...',
      refiningTagsMessage: 'Die KI generiert passende Style Tags...',
      selectModel: 'Wähle ein Musik-Modell',
      generateButton: 'Musik generieren',
      quality: 'Qualität'
    },
    musicGenV2: {
      lyricsWorkshop: 'Lyrics Workshop',
      lyricsInput: 'Dein Text',
      lyricsPlaceholder: 'Schreibe Lyrics, ein Thema, Stichworte oder eine Stimmung...',
      themeToLyrics: 'Stichworte zu Songtext',
      refineLyrics: 'Songtext strukturieren',
      resultLabel: 'Ergebnis',
      resultPlaceholder: 'Hier erscheinen deine Lyrics...',
      expandingTheme: 'Die KI schreibt einen Songtext aus deinen Stichworten...',
      refiningLyrics: 'Die KI strukturiert deinen Songtext...',
      soundExplorer: 'Sound Explorer',
      suggestFromLyrics: 'Aus Lyrics vorschlagen',
      suggestingTags: 'Die KI analysiert deine Lyrics...',
      mostImportant: 'wichtigste',
      dimGenre: 'Genre',
      dimTimbre: 'Klangfarbe',
      dimGender: 'Stimme',
      dimMood: 'Stimmung',
      dimInstrument: 'Instrumente',
      dimScene: 'Szene',
      dimRegion: 'Region (UNESCO)',
      dimTopic: 'Thema',
      audioLength: 'Audio-Länge',
      generateButton: 'Musik generieren',
      selectModel: 'Modell',
      customTags: 'Eigene Tags',
      customTagsPlaceholder: 'z.B. acoustic,dreamy,summer_vibes'
    },
    latentLab: {
      tabs: {
        attention: 'Attention Cartography',
        probing: 'Feature Probing',
        algebra: 'Concept Algebra',
        fusion: 'Encoder Fusion',
        archaeology: 'Denoising Archaeology',
        textlab: 'Latent Text Lab',
        crossmodal: 'Crossmodal Lab'
      },
      comingSoon: 'Dieses Tool wird in einer zukünftigen Version implementiert.',
      attention: {
        headerTitle: 'Attention Cartography — Welches Wort steuert welche Bildregion?',
        headerSubtitle: 'Für jedes Wort im Prompt zeigt eine Heatmap-Überlagerung auf dem generierten Bild, WO im Bild dieses Wort den größten Einfluss hatte. So wird sichtbar, wie das Modell semantische Konzepte räumlich verteilt.',
        explanationToggle: 'Ausführliche Erklärung anzeigen',
        explainWhatTitle: 'Was zeigt dieses Tool?',
        explainWhatText: 'Wenn ein Diffusionsmodell ein Bild erzeugt, liest es den Prompt nicht Wort für Wort ab wie eine Bauanleitung. Stattdessen verteilt ein Mechanismus namens „Attention" den Einfluss jedes Wortes auf verschiedene Bildregionen. Das Wort „Haus" beeinflusst hauptsächlich die Region, in der das Haus entsteht — aber auch benachbarte Bereiche, weil das Modell den Kontext der gesamten Szene versteht. Dieses Tool macht diese Verteilung sichtbar: Klicke auf ein Wort und sieh, welche Bildregionen aufleuchten.',
        explainHowTitle: 'Wie lese ich die Heatmap?',
        explainHowText: 'Helle, intensive Farbe = starker Einfluss des Wortes auf diese Region. Dunkle oder fehlende Farbe = wenig Einfluss. Wenn du mehrere Wörter auswählst, erscheinen sie in verschiedenen Farben. Beachte: Die Karten sind NICHT perfekt scharf begrenzt — das ist kein Fehler, sondern zeigt, dass das Modell Konzepte kontextuell und nicht isoliert verarbeitet. Ein „Haus" in einer Bauernhof-Szene hat auch etwas Einfluss auf Tiere und Felder, weil das Modell die Szene als Ganzes versteht.',
        explainReadTitle: 'Was verraten die zwei Regler?',
        explainReadText: 'Der Entrauschungsschritt-Regler zeigt, WANN im 25-schrittigen Erzeugungsprozess du die Attention betrachtest. Frühe Schritte zeigen die grobe Layoutplanung, späte die Detailzuordnung. Der Netzwerktiefe-Regler zeigt, WO im Transformer die Attention gemessen wird: Flache Schichten (nahe am Eingang) zeigen globale Kompositionsplanung, mittlere die semantische Zuordnung, tiefe die Feinabstimmung. Beide Achsen sind unabhängig — es lohnt sich, systematisch verschiedene Kombinationen zu erkunden.',
        techTitle: 'Technische Details',
        techText: 'SD3.5 verwendet einen MMDiT (Multimodal Diffusion Transformer) mit Joint Attention: Bild- und Text-Tokens bearbeiten sich gegenseitig in 24 Transformer-Blöcken. Wir ersetzen den Standard-SDPA-Prozessor durch einen manuellen Softmax(QK^T/√d)-Prozessor an 3 ausgewählten Blöcken, um die Text→Bild-Attention-Submatrix zu extrahieren. Die Maps haben 64x64 Auflösung (Patch-Grid) und werden per bilinearer Interpolation auf die Bildauflösung hochskaliert. SD3.5 nutzt zwei Text-Encoder: CLIP-L (BPE, 77 Tokens) und T5-XXL (SentencePiece, 512 Tokens). Beide können hier umgeschaltet werden, um zu sehen, wie unterschiedliche Tokenisierungen die Attention beeinflussen.',
        promptLabel: 'Prompt',
        promptPlaceholder: 'z.B. Ein Haus steht in einer Landschaft, umgeben von landwirtschaftlichen Flächen, Natur und Tieren. Es sind einige Menschen zu sehen.',
        generate: 'Generieren + Analyse',
        generating: 'Bild wird generiert und Attention wird extrahiert...',
        emptyHint: 'Gib einen Prompt ein und klicke auf Generieren, um die Attention-Karten des Modells zu visualisieren.',
        advancedLabel: 'Erweiterte Einstellungen',
        negativeLabel: 'Negativ-Prompt',
        stepsLabel: 'Steps',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        tokensLabel: 'Tokens',
        tokensHint: 'Klicke auf ein oder mehrere Wörter. Subwort-Tokens (z.B. "Ku"+"gel") werden automatisch zusammengefasst. Mehrere Wörter erscheinen in verschiedenen Farben.',
        timestepLabel: 'Entrauschungsschritt',
        timestepHint: 'Diffusionsmodelle erzeugen Bilder in 25 Schritten vom Rauschen zum Bild. Frühe Schritte legen die grobe Struktur fest, späte verfeinern Details. Dieser Regler zeigt, worauf das Modell bei welchem Schritt achtet.',
        step: 'Schritt',
        layerLabel: 'Netzwerktiefe',
        layerHint: 'Bei jedem Entrauschungsschritt durchläuft das Signal alle 24 Schichten des Transformers. Flache Schichten (nahe am Eingang) erfassen globale Komposition, mittlere die semantische Zuordnung, tiefe (nahe am Ausgang) feine Details. Beide Regler sind unabhängig: Schritt = wann im Prozess, Tiefe = wo im Netzwerk.',
        layerEarly: 'Flach (Komposition)',
        layerMid: 'Mittel (Semantik)',
        layerLate: 'Tief (Detail)',
        opacityLabel: 'Heatmap',
        opacityHint: 'Stärke der farbigen Überlagerung auf dem Bild.',
        baseImageLabel: 'Basisbild',
        baseColor: 'Farbe',
        baseBW: 'S/W',
        baseOff: 'Aus',
        baseImageHint: 'Farbe zeigt das Originalbild. S/W entsättigt es, damit Heatmap-Farben klar erkennbar sind. Aus blendet das Bild aus und zeigt nur die Attention-Karte.',
        encoderLabel: 'Text-Encoder',
        encoderClipL: 'CLIP-L (77 Tokens)',
        encoderT5: 'T5-XXL (512 Tokens)',
        encoderHint: 'SD3.5 nutzt zwei Text-Encoder mit unterschiedlicher Tokenisierung. CLIP-L verwendet BPE (Byte-Pair-Encoding), T5-XXL SentencePiece. Vergleiche, wie beide Encoder denselben Prompt verarbeiten und welche Bildregionen sie jeweils steuern.',
        download: 'Bild herunterladen'
      },
      probing: {
        headerTitle: 'Feature Probing — Welche Dimensionen kodieren was?',
        headerSubtitle: 'Vergleiche zwei Prompts und finde heraus, welche Embedding-Dimensionen den semantischen Unterschied kodieren. Übertrage gezielt einzelne Dimensionen, um zu sehen, wie sie das Bild verändern.',
        explanationToggle: 'Ausführliche Erklärung anzeigen',
        explainWhatTitle: 'Was zeigt dieses Tool?',
        explainWhatText: 'Jedes Wort wird vom Text-Encoder in einen hochdimensionalen Vektor umgewandelt (z.B. 4096 Dimensionen bei T5). Wenn du ein Wort im Prompt änderst — z.B. „rotes" zu „blaues" — ändern sich bestimmte Dimensionen stärker als andere. Dieses Tool zeigt dir, WELCHE Dimensionen sich am meisten ändern und lässt dich gezielt einzelne Dimensionen von Prompt B in Prompt A übertragen.',
        explainHowTitle: 'Wie funktioniert die Übertragung?',
        explainHowText: 'Im Balkendiagramm siehst du alle Dimensionen sortiert nach Differenzgröße. Mit den Rang-Reglern (Von/Bis) wählst du einen Bereich aus — z.B. nur die Top-100 oder gezielt Rang 880–920. Beim Klick auf „Übertragen" wird das Bild mit denselben Einstellungen (gleicher Seed!) neu generiert — aber mit den ausgewählten Dimensionen aus Prompt B. So siehst du exakt, was diese Dimensionen „kodieren".',
        explainReadTitle: 'Wie lese ich das Balkendiagramm?',
        explainReadText: 'Jeder Balken repräsentiert eine Embedding-Dimension. Die Länge zeigt, wie stark sich diese Dimension zwischen Prompt A und B unterscheidet. Dimensionen mit großem Unterschied sind die wahrscheinlichsten Träger der semantischen Änderung. Aber: Embeddings sind verteilt — oft braucht es mehrere Dimensionen zusammen, um eine sichtbare Änderung zu bewirken.',
        techTitle: 'Technische Details',
        techText: 'SD3.5 verwendet drei Text-Encoder: CLIP-L (768d), CLIP-G (1280d) und T5-XXL (4096d). Du kannst jeden einzeln proben. Die Differenz wird als mittlere absolute Abweichung über alle Token-Positionen berechnet: mean(abs(B-A), dim=tokens). Die Übertragung ersetzt die ausgewählten Dimensionen in allen Token-Positionen gleichzeitig.',
        promptALabel: 'Prompt A (Original)',
        promptBLabel: 'Prompt B (Vergleich)',
        promptAPlaceholder: 'z.B. Ein rotes Haus am See',
        promptBPlaceholder: 'z.B. Ein blaues Haus am See',
        encoderLabel: 'Encoder',
        encoderAll: 'Alle (empfohlen)',
        encoderClipL: 'CLIP-L (768d)',
        encoderClipG: 'CLIP-G (1280d)',
        encoderT5: 'T5-XXL (4096d)',
        analyzeBtn: 'Analysieren',
        analyzing: 'Prompts werden kodiert und verglichen...',
        transferBtn: 'Übertrage die ausgewählten Vektor-Dimensionen von Prompt B in das erzeugte Bild',
        transferring: 'Bild mit modifiziertem Embedding wird erzeugt...',
        rankFromLabel: 'Von Rang',
        rankToLabel: 'Bis Rang',
        sliderLabel: 'Dimensionen von Prompt B auswählen',
        range1Label: 'Bereich 1',
        range2Label: 'Bereich 2',
        addRange: 'Bereich hinzufügen',
        selectionDesc: '{count} Dimensionen von Prompt B ausgewählt (Rang {ranges} von {total})',
        listTitle: 'Die {count} Dimensionen von Prompt B mit der größten Differenz zu Prompt A',
        sortAsc: 'Aufsteigend sortiert',
        sortDesc: 'Absteigend sortiert',
        originalLabel: 'Original (Prompt A)',
        modifiedLabel: 'Modifiziert (Transfer von Prompt B)',
        modifiedHint: 'Wähle unten einen Rangbereich und klicke „Übertragen" — hier erscheint dann Prompt A mit den übertragenen Dimensionen aus B (gleicher Seed).',
        noDifference: 'Die Embeddings sind identisch — ändere Prompt B.',
        advancedLabel: 'Erweiterte Einstellungen',
        negativeLabel: 'Negativ-Prompt',
        stepsLabel: 'Steps',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        selectAll: 'Alle',
        selectNone: 'Keine',
        downloadOriginal: 'Original herunterladen',
        downloadModified: 'Modifiziert herunterladen'
      },
      algebra: {
        headerTitle: 'Concept Algebra \u2014 Vektor-Arithmetik auf Bild-Embeddings',
        headerSubtitle: 'Wende die ber\u00fchmte Word2Vec-Analogie auf Bildgenerierung an: K\u00f6nig \u2212 Mann + Frau \u2248 K\u00f6nigin. Drei Prompts werden kodiert und algebraisch kombiniert.',
        explanationToggle: 'Ausf\u00fchrliche Erkl\u00e4rung anzeigen',
        explainWhatTitle: 'Was zeigt dieses Tool?',
        explainWhatText: 'Mikolov zeigte 2013, dass Word-Embeddings semantische Beziehungen als lineare Richtungen kodieren: Der Vektor von \u201eK\u00f6nig\u201c minus \u201eMann\u201c plus \u201eFrau\u201c ergibt einen Vektor nahe \u201eK\u00f6nigin\u201c. Dieses Tool \u00fcbertr\u00e4gt diese Idee auf die Text-Encoder von SD3.5: Statt einzelner W\u00f6rter manipulierst du ganze Prompt-Embeddings. Das Ergebnis ist ein Bild, das das Konzept A enth\u00e4lt, aber B durch C ersetzt hat.',
        explainHowTitle: 'Wie funktioniert die Algebra \u2014 und warum nicht einfach ein Negativ-Prompt?',
        explainHowText: 'Du gibst drei Prompts ein: A (Basis), B (subtrahieren) und C (addieren). Die Formel ist: Ergebnis = A \u2212 Skalierung\u2081\u00d7B + Skalierung\u2082\u00d7C. Mit den Skalierungsreglern steuerst du die Intensit\u00e4t: Bei 1.0 wird B vollst\u00e4ndig subtrahiert und C vollst\u00e4ndig addiert. Bei 0.5 nur zur H\u00e4lfte. Werte \u00fcber 1.0 verst\u00e4rken den Effekt. \u2014 Warum nicht einfach \u201eA + C\u201c als Prompt und \u201eB\u201c als Negativ-Prompt verwenden? Weil das etwas fundamental anderes tut: Ein Negativ-Prompt steuert den Entrauschungsprozess bei JEDEM der 25 Schritte weg von B \u2014 das Modell entscheidet Schritt f\u00fcr Schritt, wie es \u201enicht B\u201c interpretiert. Concept Algebra dagegen berechnet einen neuen Vektor VOR der Bildgenerierung: Die Subtraktion passiert im Embedding-Raum, nicht im Diffusionsprozess. Das Ergebnis ist ein einziger Vektor, der \u201eA ohne B-heit plus C-heit\u201c direkt kodiert. Der Negativ-Prompt sagt \u201emach das nicht\u201c. Die Algebra sagt \u201enimm dieses Konzept heraus und setze jenes ein\u201c \u2014 eine chirurgische Operation im Bedeutungsraum statt einer schrittweisen Vermeidungsstrategie.',
        explainReadTitle: 'Was bedeuten die Ergebnisse?',
        explainReadText: 'Links siehst du das Referenzbild (nur Prompt A, gleicher Seed). Rechts das Ergebnis der Algebra. Wenn die Analogie funktioniert, sollte das rechte Bild das Konzept von A zeigen, aber mit der semantischen Ver\u00e4nderung B\u2192C. Beispiel: \u201eSonnenuntergang am Strand\u201c \u2212 \u201eStrand\u201c + \u201eBerge\u201c \u2248 \u201eSonnenuntergang \u00fcber Bergen\u201c. Die L2-Distanz zeigt, wie weit sich das Ergebnis vom Original entfernt hat. \u2014 Ist die Operation kommutativ? Nein. Die Subtraktion von B und die Addition von C finden relativ zum Vektor A statt. Die Richtung B\u2192C ist nur im Kontext von A sinnvoll: \u201eK\u00f6nig \u2212 Mann\u201c entfernt die \u201em\u00e4nnlichen\u201c Richtungen aus dem K\u00f6nig-Vektor, \u201e+ Frau\u201c erg\u00e4nzt die \u201eweiblichen\u201c Richtungen \u2014 das Ergebnis liegt nahe \u201eK\u00f6nigin\u201c. C wird dabei nicht gezielt an die Stelle von B gesetzt, sondern einfach addiert. Dass das trotzdem funktioniert, zeigt, dass semantische Beziehungen im Vektorraum als konsistente lineare Richtungen kodiert sind.',
        techTitle: 'Technische Details',
        techText: 'Die Algebra wird auf den gew\u00e4hlten Encoder-Embeddings durchgef\u00fchrt: CLIP-L (768d), CLIP-G (1280d), T5-XXL (4096d) oder alle zusammen (589 Tokens \u00d7 4096d). Dieselbe Operation wird auch auf die Pooled Embeddings (2048d) angewendet. Beide Bilder verwenden denselben Seed f\u00fcr faire Vergleichbarkeit.',
        promptALabel: 'Prompt A (Basis)',
        promptAPlaceholder: 'z.B. Sonnenuntergang am Strand mit Palmen',
        promptBLabel: 'Prompt B (Subtrahieren)',
        promptBPlaceholder: 'z.B. Strand mit Palmen',
        promptCLabel: 'Prompt C (Addieren)',
        promptCPlaceholder: 'z.B. Schneebedeckte Berge',
        formulaLabel: 'A \u2212 B + C = ?',
        encoderLabel: 'Encoder',
        encoderAll: 'Alle (empfohlen)',
        encoderClipL: 'CLIP-L (768d)',
        encoderClipG: 'CLIP-G (1280d)',
        encoderT5: 'T5-XXL (4096d)',
        generateBtn: 'Berechnen',
        generating: 'Embeddings werden berechnet und Bilder erzeugt...',
        referenceLabel: 'Referenz (Prompt A)',
        resultLabel: 'Ergebnis (A \u2212 B + C)',
        l2Label: 'L2-Distanz zum Original',
        advancedLabel: 'Erweiterte Einstellungen',
        negativeLabel: 'Negativ-Prompt',
        stepsLabel: 'Steps',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        scaleSubLabel: 'Subtraktions-Skalierung',
        scaleAddLabel: 'Additions-Skalierung',
        downloadReference: 'Referenz herunterladen',
        downloadResult: 'Ergebnis herunterladen',
        resultHint: 'Gib drei Prompts ein und klicke auf Berechnen \u2014 hier erscheint das Ergebnis der Vektor-Arithmetik.'
      },
      archaeology: {
        headerTitle: 'Denoising Archaeology \u2014 Wie wird aus Rauschen ein Bild?',
        headerSubtitle: 'Beobachte jeden einzelnen Entrauschungsschritt. Diffusionsmodelle arbeiten nicht links-nach-rechts, sondern gleichzeitig \u00fcberall \u2014 von groben Formen zu feinen Details.',
        explanationToggle: 'Ausf\u00fchrliche Erkl\u00e4rung anzeigen',
        explainWhatTitle: 'Was zeigt dieses Tool?',
        explainWhatText: 'Ein Diffusionsmodell erzeugt ein Bild, indem es schrittweise Rauschen entfernt. Dabei entsteht das Bild nicht wie beim Zeichnen von links nach rechts \u2014 stattdessen arbeitet das Modell an ALLEN Bildregionen gleichzeitig. In den ersten Schritten entstehen grobe Strukturen: Wo ist oben, wo unten? Wo ist der Horizont? In den mittleren Schritten kommen semantische Inhalte: Objekte, Formen, Farben. Die letzten Schritte verfeinern Texturen und Details. Dieses Tool macht jeden einzelnen Schritt sichtbar.',
        explainHowTitle: 'Wie benutze ich das Tool?',
        explainHowText: 'Gib einen Prompt ein und klicke auf Generieren. Das Modell erzeugt 25 Zwischenbilder (eins pro Entrauschungsschritt). Diese erscheinen als Filmstreifen unten. Klicke auf ein Thumbnail oder benutze den Zeitregler, um jeden Schritt in voller Gr\u00f6\u00dfe zu betrachten. Vergleiche fr\u00fche und sp\u00e4te Schritte: Wann \u201ewei\u00df\u201c das Modell, was es zeichnet?',
        explainReadTitle: 'Was verraten die drei Phasen?',
        explainReadText: 'Fr\u00fche Schritte (1\u20138): Globale Komposition \u2014 Grundstruktur, Farbverteilung, Layoutplanung. Mittlere Schritte (9\u201317): Semantische Emergenz \u2014 Objekte werden erkennbar, Formen kristallisieren sich heraus. Sp\u00e4te Schritte (18\u201325): Detail-Verfeinerung \u2014 Texturen, Kanten, feine Muster. Die \u00dcberg\u00e4nge sind flie\u00dfend, aber die Phasen zeigen deutlich: Das Modell \u201eplant\u201c zuerst global und verfeinert dann lokal. Besonders aufschlussreich: Der allererste Schritt zeigt keine feink\u00f6rnigen Pixel, sondern farbige Flecken. Das liegt daran, dass das Rauschen im Latent-Raum (128\u00d7128 bei 16 Kan\u00e4len) erzeugt wird, nicht im Pixel-Raum. Der VAE \u00fcbersetzt jeden Latent-Pixel in einen ~8\u00d78-Pixel-Patch \u2014 selbst pures Gau\u00dfsches Rauschen wird dadurch zu zusammenh\u00e4ngenden Farbclustern. Das Modell \u201edenkt\u201c nie in einzelnen Pixeln, sondern immer in diesem komprimierten Raum.',
        techTitle: 'Technische Details',
        techText: 'SD3.5 Large verwendet Rectified Flow als Scheduler mit 25 Standardschritten. Bei jedem Schritt werden die aktuellen Latent-Vektoren durch den VAE dekodiert (1024\u00d71024 JPEG). Der VAE (Variational Autoencoder) \u00fcbersetzt den mathematischen Latent-Raum in Pixel. Die Latent-Darstellung ist 128\u00d7128 bei 16 Kan\u00e4len \u2014 jeder Latent-Pixel entspricht einem ~8\u00d78-Pixel-Patch im Bild. Deshalb zeigt schon der erste Schritt farbige Cluster statt feines Pixelrauschen: Der VAE interpretiert zuf\u00e4llige 16-dimensionale Vektoren als koh\u00e4rente Farbfl\u00e4chen.',
        promptLabel: 'Prompt',
        promptPlaceholder: 'z.B. Ein Marktplatz in einer mittelalterlichen Stadt mit Menschen, Geb\u00e4uden und einem Brunnen',
        generate: 'Generieren',
        generating: 'Bild wird generiert \u2014 jeder Schritt wird aufgezeichnet...',
        emptyHint: 'Gib einen Prompt ein und klicke auf Generieren, um den Entrauschungsprozess zu visualisieren.',
        advancedLabel: 'Erweiterte Einstellungen',
        negativeLabel: 'Negativ-Prompt',
        stepsLabel: 'Steps',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        filmstripLabel: 'Entrauschungs-Filmstreifen',
        timelineLabel: 'Schritt',
        phaseEarly: 'Komposition',
        phaseMid: 'Semantik',
        phaseLate: 'Detail',
        phaseEarlyDesc: 'Globale Struktur und Farbverteilung entstehen',
        phaseMidDesc: 'Objekte und Formen werden erkennbar',
        phaseLateDesc: 'Texturen und feine Details werden gesch\u00e4rft',
        finalImageLabel: 'Finales Bild (volle Aufl\u00f6sung)',
        download: 'Bild herunterladen'
      },
      textLab: {
        headerTitle: 'Latent Text Lab \u2014 Wissenschaftliche LLM-Dekonstruktion',
        headerSubtitle: 'Representation Engineering, vergleichende Modell-Arch\u00e4ologie und systematische Bias-Analyse: Drei forschungsbasierte Werkzeuge zur Untersuchung von Sprachmodellen.',
        explanationToggle: 'Wissenschaftliche Grundlagen anzeigen',
        explainWhatTitle: 'Was zeigt dieses Tool?',
        explainWhatText: 'Dieses Tool implementiert drei aktuelle Forschungsans\u00e4tze zur LLM-Interpretierbarkeit: Representation Engineering (Zou et al. 2023) findet Konzept-Richtungen im Aktivierungsraum und manipuliert Modellverhalten gezielt. Vergleichende Modell-Arch\u00e4ologie (Belinkov 2022, Olsson 2022) zeigt, wie verschiedene Modelle denselben Text intern repr\u00e4sentieren. Bias-Arch\u00e4ologie (Zou 2023, Bricken 2023) deckt systematische Verzerrungen durch kontrollierte Token-Manipulation auf.',
        explainHowTitle: 'Wie benutze ich das Tool?',
        explainHowText: 'Lade zuerst ein Modell \u00fcber das Modell-Panel (klein f\u00fcr schnelle Tests, ab 3B f\u00fcr saubere Konzept-Richtungen). Dann stehen drei Tabs zur Verf\u00fcgung: Tab 1 (RepEng) definiert Kontrastpaare und findet Konzept-Richtungen. Tab 2 (Vergleich) l\u00e4dt zwei Modelle und vergleicht ihre inneren Repr\u00e4sentationen. Tab 3 (Bias) f\u00fchrt systematische Bias-Experimente durch.',
        modelPanel: {
          title: 'Modell-Verwaltung',
          presetLabel: 'Preset',
          presetNone: 'Kein Preset (eigene ID)',
          customModelLabel: 'HuggingFace Modell-ID',
          customModelPlaceholder: 'z.B. meta-llama/Llama-3.2-1B',
          quantizationLabel: 'Quantisierung',
          quantAuto: 'Auto',
          loadBtn: 'Laden',
          unloadBtn: 'Entladen',
          loading: 'Modell wird geladen...',
          statusLoaded: 'Geladen',
          statusNone: 'Kein Modell geladen',
          vramLabel: 'VRAM',
          noModelWarning: 'Lade zuerst ein Modell, um die Werkzeuge zu verwenden.'
        },
        tabs: {
          repeng: 'Representation Engineering',
          compare: 'Modell-Vergleich',
          bias: 'Bias-Arch\u00e4ologie'
        },
        repeng: {
          title: 'Representation Engineering',
          subtitle: 'Finde Konzept-Richtungen im Aktivierungsraum und steuere die Generierung',
          guide: 'Dieses Experiment extrahiert eine \"Wahrheits-Richtung\" aus dem Modell. Die vorausgef\u00fcllten Kontrastpaare enthalten jeweils einen wahren und einen falschen Satz. Aus den Unterschieden in den Aktivierungen berechnet das System eine Richtung im hochdimensionalen Raum. Wenn diese Richtung invertiert wird (\u03b1 = -1), sollte das Modell bei einem Fakten-Prompt eine falsche Antwort generieren \u2014 obwohl es die richtige \"wei\u00df\". Das zeigt: Das Modell kodiert nicht nur Wissen, sondern auch die Tendenz, korrekt vs. inkorrekt zu antworten.',
          languageHint: 'Empfehlung: Englische Prompts funktionieren deutlich besser, da die meisten Open-Source-LLMs (LLaMA, Mistral) prim\u00e4r auf englischen Daten trainiert wurden. Die vorausgef\u00fcllten Beispiele sind deshalb auf Englisch.',
          expectedResults: 'Erwartete Ergebnisse: Bei \u03b1 = 0 (Baseline) generiert das Modell die korrekte Antwort. Bei \u03b1 = -1 (Invertierung) sollte eine falsche Antwort erscheinen \u2014 das ist der Kern des Experiments. Bei \u03b1 = +1 \u00e4ndert sich wenig, weil das Modell bereits korrekt antwortet. Ab |\u03b1| > 2 dominieren Artefakte (Wiederholungen, Unsinn). Der \"Sweet Spot\" liegt laut Zou et al. bei |\u03b1| zwischen 0.5 und 2.0. Erkl\u00e4rte Varianz > 50% bedeutet eine saubere Trennung \u2014 darunter sind die Kontrastpaare zu \u00e4hnlich oder zu wenige (mindestens 3 empfohlen).',
          science: 'Basiert auf Zou et al. (2023) "Representation Engineering" und Li et al. (2024) "Inference-Time Intervention". Kernidee: LLMs kodieren abstrakte Konzepte (Wahrheit, Stimmung, Ethik) als Richtungen im hochdimensionalen Aktivierungsraum. Durch Kontrastpaare (wahrer vs. falscher Satz) l\u00e4sst sich die Richtung extrahieren, die ein Konzept kodiert. Addiert man diese Richtung zur Laufzeit, \u00e4ndert sich das Modellverhalten gezielt \u2014 ohne Retraining.',
          pairsTitle: 'Kontrastpaare',
          pairsSubtitle: 'Mindestens 3 Paare empfohlen. Jedes Paar muss sich nur im Zielkonzept unterscheiden (wahr vs. falsch). Die Beispiele sind editierbar.',
          positiveLabel: 'Positiv (wahr)',
          negativeLabel: 'Negativ (falsch)',
          positivePlaceholder: 'z.B. The capital of France is Paris',
          negativePlaceholder: 'z.B. The capital of France is Berlin',
          addPair: 'Paar hinzuf\u00fcgen',
          removePair: 'Entfernen',
          targetLayerLabel: 'Ziel-Schicht',
          targetLayerAuto: 'Letzte Schicht',
          findDirection: 'Richtung finden',
          finding: 'Konzept-Richtung wird berechnet...',
          directionFound: 'Konzept-Richtung gefunden',
          varianceLabel: 'Erkl\u00e4rte Varianz',
          dimLabel: 'Dimensionen',
          projectionsTitle: 'Projektionen der Kontrastpaare',
          testTitle: 'Test + Manipulation',
          testSubtitle: 'Gib einen Satz ein und steuere die Generierung entlang der Konzept-Richtung',
          testPromptLabel: 'Test-Prompt',
          testPromptPlaceholder: 'z.B. The capital of Germany is',
          alphaLabel: 'Manipulationsst\u00e4rke (\u03b1)',
          temperatureLabel: 'Temperatur',
          maxTokensLabel: 'Max. Tokens',
          seedLabel: 'Seed (-1 = zuf\u00e4llig)',
          generateBtn: 'Generieren mit Manipulation',
          generating: 'Manipulierte Generierung l\u00e4uft...',
          baselineLabel: 'Baseline (ohne Manipulation)',
          manipulatedLabel: 'Manipuliert (\u03b1 = {alpha})',
          projectionLabel: 'Projektion auf Konzept-Richtung',
          interpretationTitle: 'Erl\u00e4uterung',
          interpreting: 'Ergebnisse werden analysiert...',
          interpretationError: 'Erl\u00e4uterung konnte nicht generiert werden'
        },
        compare: {
          title: 'Vergleichende Modell-Arch\u00e4ologie',
          subtitle: 'Lade zwei Modelle und vergleiche ihre inneren Repr\u00e4sentationen systematisch',
          science: 'Basiert auf Belinkov (2022) "Probing Classifiers" und Olsson et al. (2022) "In-Context Learning and Induction Heads". Die Heatmap zeigt CKA (Centered Kernel Alignment) zwischen Schichten beider Modelle. Hohe \u00c4hnlichkeit bedeutet: Diese Schichten repr\u00e4sentieren Information auf \u00e4hnliche Weise. Fr\u00fche Schichten (Syntax) sind oft \u00e4hnlich \u2014 sp\u00e4te Schichten (Semantik) divergieren st\u00e4rker, besonders bei verschiedenen Modellgr\u00f6\u00dfen.',
          modelATitle: 'Modell A (oben geladen)',
          modelAHint: 'Wechsel \u00fcber das Modell-Panel oben',
          modelBTitle: 'Modell B (zweites Modell)',
          modelBPresetLabel: 'Preset',
          modelBCustomLabel: 'HuggingFace Modell-ID',
          modelBCustomPlaceholder: 'z.B. TinyLlama/TinyLlama-1.1B-Chat-v1.0',
          modelBLoadBtn: 'Modell B laden',
          modelBLoaded: 'Modell B geladen',
          modelBNone: 'Modell B nicht geladen',
          promptLabel: 'Prompt',
          promptPlaceholder: 'z.B. Die Katze sa\u00df auf der Matte und beobachtete die V\u00f6gel',
          seedLabel: 'Seed',
          temperatureLabel: 'Temperatur',
          maxTokensLabel: 'Max. Tokens',
          compareBtn: 'Vergleichen',
          comparing: 'Modelle werden verglichen...',
          heatmapTitle: 'Schicht-Alignment (CKA)',
          heatmapAxisA: 'Modell A \u2014 Schichten',
          heatmapAxisB: 'Modell B \u2014 Schichten',
          heatmapExplain: 'Helle Zellen = hohe Repr\u00e4sentations\u00e4hnlichkeit. Diagonale Muster zeigen, dass die Modelle Information in \u00e4hnlicher Reihenfolge verarbeiten.',
          attentionTitle: 'Attention-Vergleich (letzte Schicht)',
          modelALabel: 'Modell A',
          modelBLabel: 'Modell B',
          generationTitle: 'Generierungs-Vergleich (gleicher Seed)',
          layerStatsTitle: 'Schicht-Statistiken',
          interpretationTitle: 'Erl\u00e4uterung',
          interpreting: 'Ergebnisse werden analysiert...',
          interpretationError: 'Erl\u00e4uterung konnte nicht generiert werden'
        },
        bias: {
          title: 'Bias-Arch\u00e4ologie',
          subtitle: 'Systematische Bias-Experimente durch kontrollierte Token-Manipulation',
          science: 'Basiert auf Zou et al. (2023) "Representation Engineering" und Bricken et al. (2023) "Towards Monosemanticity". Statt freiem Manipulieren untersucht dieses Tool systematische Verzerrungen: Was passiert, wenn alle m\u00e4nnlichen Pronomen unterdr\u00fcckt werden? Welches Geschlecht w\u00e4hlt das Modell als Default? Wie stark \u00e4ndert sich der Text, wenn positive vs. negative W\u00f6rter verst\u00e4rkt werden? Die Antworten zeigen, welche Biases im Modell kodiert sind.',
          presetLabel: 'Experiment-Typ',
          presetGender: 'Geschlecht \u2014 Unterdr\u00fccke gendered Pronomen',
          presetSentiment: 'Sentiment \u2014 Verst\u00e4rke positiv/negativ',
          presetDomain: 'Dom\u00e4ne \u2014 Verst\u00e4rke wissenschaftlich/poetisch',
          presetCustom: 'Eigenes Experiment',
          promptLabel: 'Prompt',
          promptPlaceholder: 'z.B. The doctor said to the patient',
          customBoostLabel: 'Verst\u00e4rkte Tokens (kommagetrennt)',
          customBoostPlaceholder: 'z.B. dark,shadow,night',
          customSuppressLabel: 'Unterdr\u00fcckte Tokens (kommagetrennt)',
          customSuppressPlaceholder: 'z.B. light,sun,bright',
          numSamplesLabel: 'Stichproben pro Bedingung',
          temperatureLabel: 'Temperatur',
          maxTokensLabel: 'Max. Tokens',
          seedLabel: 'Basis-Seed',
          runBtn: 'Experiment starten',
          running: 'Bias-Experiment l\u00e4uft...',
          baselineTitle: 'Baseline (keine Manipulation)',
          groupTitle: 'Gruppe: {name}',
          modeSuppress: 'unterdr\u00fcckt',
          modeBoost: 'verst\u00e4rkt',
          tokensLabel: 'Tokens',
          sampleSeedLabel: 'Seed',
          genderDesc: 'Unterdr\u00fcckt alle geschlechtsspezifischen Pronomen und beobachtet, welche Defaults das Modell w\u00e4hlt.',
          sentimentDesc: 'Verst\u00e4rkt positive oder negative W\u00f6rter und misst, wie stark der gesamte Textfluss beeinflusst wird.',
          domainDesc: 'Verst\u00e4rkt wissenschaftliches oder poetisches Vokabular und beobachtet Register-Verschiebungen.',
          interpretationTitle: 'Erl\u00e4uterung',
          interpreting: 'Ergebnisse werden analysiert...',
          interpretationError: 'Erl\u00e4uterung konnte nicht generiert werden'
        },
        error: {
          gpuUnreachable: 'GPU-Service nicht erreichbar. Ist er gestartet?',
          loadFailed: 'Modell konnte nicht geladen werden.',
          operationFailed: 'Operation fehlgeschlagen.'
        }
      },
      crossmodal: {
        headerTitle: 'Crossmodal Lab',
        headerSubtitle: 'Klang aus latenten Raeumen: T5-Embedding-Manipulation, bildgesteuerte Audiogenerierung, crossmodaler Transfer',
        generate: 'Generieren',
        generating: 'Generiere...',
        result: 'Ergebnis',
        seed: 'Seed',
        generationTime: 'Generierungszeit',
        tabs: {
          synth: {
            label: 'Latent Audio Synth',
            short: 'T5-Embedding-Manipulation',
            title: 'Latent Audio Synth',
            description: 'Direkte Manipulation des T5-Conditioning-Raums (768d) fuer Stable Audio. Interpoliere zwischen Prompts, extrapoliere ueber den Prompt hinaus, skaliere Embeddings und injiziere Rauschen. Ultra-kurze Loops, nahezu Echtzeit.'
          },
          mmaudio: {
            label: 'MMAudio',
            short: 'Bild/Text zu Audio (CVPR 2025)',
            title: 'MMAudio — Video/Image to Audio',
            description: 'MMAudio (CVPR 2025) ist ein dediziertes crossmodales Modell, das auf beiden Modalitaeten trainiert wurde. 157M Parameter, generiert 8s Audio in ~1.2s. Genuiner crossmodaler Transfer statt naive Feature-Projektion.'
          },
          guidance: {
            label: 'ImageBind Guidance',
            short: 'Gradientenbasierte Bildsteuerung',
            title: 'ImageBind Gradient Guidance',
            description: 'Gradient-basierte Steuerung waehrend des Stable Audio Denoising-Prozesses. ImageBind liefert einen gemeinsamen 1024d-Raum fuer Bild und Audio — der Gradient der Cosine-Similarity lenkt die Audiogenerierung in Richtung des Bild-Embeddings.'
          }
        },
        synth: {
          promptA: 'Prompt A (Basis)',
          promptAPlaceholder: 'z.B. Meereswellen',
          promptB: 'Prompt B (Optional, fuer Interpolation)',
          promptBPlaceholder: 'z.B. Klaviermelodie',
          alpha: 'Alpha (Interpolation)',
          alphaHint: '0 = nur A, 1 = nur B, dazwischen = Mischung, >1 oder <0 = Extrapolation',
          magnitude: 'Magnitude (Skalierung)',
          magnitudeHint: 'Globale Skalierung des Embeddings (1.0 = unveraendert)',
          noise: 'Rauschen',
          noiseHint: 'Gauss-Rauschen auf dem Embedding (0 = kein Rauschen)',
          duration: 'Dauer (s)',
          steps: 'Schritte',
          cfg: 'CFG',
          loop: 'Loop-Wiedergabe',
          loopOn: 'Loop An',
          loopOff: 'Loop Aus',
          stop: 'Stop',
          looping: 'Loopt',
          playing: 'Spielt',
          stopped: 'Gestoppt',
          transpose: 'Transposition (Halbtoene)',
          midiSection: 'MIDI-Steuerung',
          midiUnsupported: 'Web MIDI wird von diesem Browser nicht unterstuetzt.',
          midiInput: 'MIDI-Eingang',
          midiNone: '(keiner)',
          midiMappings: 'CC-Zuordnungen',
          midiNoteC3: 'Note (C3 = Ref)',
          midiGenerate: 'Generieren + Transposition',
          midiPitch: 'Tonhoehe rel. C3',
          loopInterval: 'Loop-Intervall',
          loopOptimize: 'Auto-Optimierung',
          loopPingPong: 'Ping-Pong',
          loopIntervalHint: 'Start/Ende des Loop-Bereichs — verkuerze das Ende, um Stable Audios Fade-Out abzuschneiden',
          modeLoop: 'Loop',
          modePingPong: 'Ping-Pong',
          modeWavetable: 'Wavetable',
          modeRate: 'Tempo (schnell)',
          modePitch: 'Tonhoehe (OLA)',
          wavetableScan: 'Scan-Position',
          wavetableScanHint: 'Morpht zwischen Frames (0 = Anfang, 1 = Ende)',
          wavetableFrames: '{count} Frames',
          midiScan: 'Scan-Position',
          adsrTitle: 'ADSR-Huellkurve',
          adsrAttack: 'A',
          adsrDecay: 'D',
          adsrSustain: 'S',
          adsrRelease: 'R',
          adsrHint: 'Huellkurve fuer MIDI-Noten (Attack/Decay/Sustain/Release)',
          play: 'Abspielen',
          normalize: 'Lautheit normalisieren',
          peak: 'Peak',
          crossfade: 'Crossfade',
          saveRaw: 'Raw speichern',
          saveLoop: 'Loop speichern',
          embeddingStats: 'Embedding-Statistiken',
          dimensions: {
            section: 'Dimensions-Explorer',
            hint: 'Auf Balken ziehen = Offset setzen. Horizontal malen = mehrere Dimensionen.',
            resetAll: 'Alle zuruecksetzen',
            hoverActivation: 'Aktivierung',
            hoverOffset: 'Offset',
            rightClickReset: 'Rechtsklick = zuruecksetzen',
            sortDiff: 'Sortiert nach Prompt-Differenz',
            sortMagnitude: 'Sortiert nach Aktivierung',
            activeOffsets: '{count} Offsets aktiv',
            applyAndGenerate: 'Anwenden und neu generieren',
            undo: 'Rueckgaengig',
            redo: 'Wiederholen'
          }
        },
        mmaudio: {
          imageUpload: 'Bild hochladen',
          prompt: 'Text-Prompt',
          promptPlaceholder: 'z.B. Knisterndes Lagerfeuer',
          negativePrompt: 'Negativ-Prompt',
          duration: 'Dauer (s)',
          maxDuration: 'Max 8s (Modell-Limit)',
          cfg: 'CFG',
          steps: 'Schritte',
          compareHint: 'Vergleiche: Nur Text vs. Bild + Text'
        },
        guidance: {
          imageUpload: 'Bild hochladen',
          prompt: 'Basis-Prompt (optional)',
          promptPlaceholder: 'z.B. Umgebungsklanglandschaft',
          lambda: 'Guidance-Staerke',
          lambdaHint: 'Wie stark das Bild die Audiogenerierung steuert',
          warmupSteps: 'Warmup-Schritte',
          warmupHint: 'Gradient-Guidance nur in den ersten N Schritten',
          totalSteps: 'Gesamt-Schritte',
          duration: 'Dauer (s)',
          cfg: 'CFG',
          cosineSimilarity: 'Cosine-Similarity (Bild-Audio-Naehe)'
        }
      }
    },
    edutainment: {
      ui: {
        didYouKnow: '🤔 Wusstest du?',
        learnMore: '📚 Mehr erfahren',
        currentlyHappening: '⚡ Gerade passiert:',
        energyUsed: 'Verbrauchte Energie',
        co2Produced: 'CO₂ produziert'
      },
      energy: {
        kids_1: '💡 KI-Bilder brauchen Strom – so viel wie dein Handy 3 Stunden laden!',
        kids_2: '🔌 Die GPU ist wie ein Super-Taschenrechner der sehr viel Strom frisst!',
        kids_3: '⚡ Jedes Bild braucht so viel Energie wie eine LED-Lampe 10 Minuten an!',
        youth_1: '⚡ Eine GPU braucht beim Generieren {watts}W – wie ein kleiner Heizlüfter!',
        youth_2: '🔋 Ein Bild verbraucht etwa 0.01-0.02 kWh – klingt wenig, summiert sich aber!',
        youth_3: '🌡️ Die GPU wird gerade {temp}°C heiß – deshalb braucht sie Kühlung!',
        expert_1: '📊 Echtzeit: {watts}W bei {util}% Auslastung = {kwh} kWh bisher',
        expert_2: '🔥 TDP-Limit: {tdp}W | Aktuell: {watts}W ({percent}% des Limits)',
        expert_3: '💾 VRAM: {used}/{total} GB ({percent}%) – Modell + Aktivierungen'
      },
      data: {
        kids_1: '🧮 Die GPU rechnet gerade 10 Milliarden mal – schneller als du zählen kannst!',
        kids_2: '🎨 Das Bild entsteht in 50 kleinen Schritten – wie ein Puzzle das sich selbst löst!',
        kids_3: '🧩 Millionen von Zahlen fliegen gerade durch die GPU!',
        youth_1: '🔄 Jedes Bild durchläuft ~50 "Denoising Steps" – 50 Runden Rauschen entfernen!',
        youth_2: '📐 8 Milliarden Parameter werden gerade abgefragt – pro Bild!',
        youth_3: '🧠 Die KI "denkt" in Vektoren mit tausenden Dimensionen – wie Koordinaten in einem Raum.',
        expert_1: '🔬 MMDiT: Multimodal Diffusion Transformer – Text + Bild in gemeinsamen Attention-Layern',
        expert_2: '📈 Self-Attention: O(n²) Komplexität – jedes Token "sieht" alle anderen',
        expert_3: '⚙️ Classifier-Free Guidance: Prompt-Einfluss vs. Kreativität-Balance'
      },
      model: {
        kids_1: '🎓 Das KI-Modell hat sich Millionen Bilder angeschaut um malen zu lernen!',
        kids_2: '🤖 Die KI ist wie ein Künstler der nie vergisst was er gesehen hat!',
        kids_3: '✨ 8 Milliarden Verbindungen im Modell – mehr als Sterne am Himmel die du sehen kannst!',
        youth_1: '🧠 SD3.5 Large hat 8 Milliarden Parameter – wie 8 Milliarden Entscheidungsknoten.',
        youth_2: '📚 3 Text-Encoder arbeiten zusammen: CLIP-L, CLIP-G und T5-XXL',
        youth_3: '🔢 Das Modell braucht {vram} GB VRAM nur um geladen zu werden!',
        expert_1: '🏗️ Architektur: Rectified Flow + MMDiT mit 38 Transformer-Blöcken',
        expert_2: '📊 FP16/FP8 Quantisierung: Präzision vs. VRAM-Trade-off',
        expert_3: '🔗 LoRA: Low-Rank Adaptation – nur 0.1% der Parameter neu trainiert'
      },
      ethics: {
        kids_1: '🌍 KI lernt von Bildern im Internet – deshalb ist es wichtig, fair mit Kunst anderer zu sein!',
        kids_2: '⚖️ Nicht alle Künstler wurden gefragt ob die KI von ihnen lernen darf.',
        kids_3: '🤝 Gute KI respektiert die Arbeit von Menschen!',
        youth_1: '📜 Trainingsdaten stammen oft aus dem Internet. Künstler diskutieren: Fair Use oder Kopieren?',
        youth_2: '🏛️ Der EU AI Act fordert Transparenz: Woher kommen die Trainingsdaten?',
        youth_3: '💭 Frage: Wem gehört ein KI-generiertes Bild eigentlich?',
        expert_1: '⚠️ LAION-5B wurde teils ohne Urheber-Zustimmung erstellt – rechtliche Grauzone.',
        expert_2: '📋 EU AI Act Art. 52: Kennzeichnungspflicht für KI-generierte Inhalte',
        expert_3: '🔍 Model Cards & Datasheets: Best Practice für ML-Transparenz'
      },
      environment: {
        kids_1: '☁️ Jedes KI-Bild produziert ein bisschen CO₂ – wie Autofahren, nur weniger!',
        kids_2: '🌱 Überlege: Ist dieses Bild den Strom wert?',
        kids_3: '🌞 Die Energie für KI kommt oft aus Kraftwerken – manche sauber, manche nicht.',
        youth_1: '🏭 Deutscher Strommix: ~400g CO₂ pro kWh – das addiert sich!',
        youth_2: '📈 {co2}g CO₂ für dieses Bild – bei 1000 Bildern wären das {totalKg} kg!',
        youth_3: '💡 Tipp: Weniger Bilder generieren, dafür bewusster – spart Energie und CO₂.',
        expert_1: '📊 Berechnung: {watts}W × {seconds}s ÷ 3600 × 400g/kWh = {co2}g CO₂',
        expert_2: '🔬 Scope 2 Emissionen: Standort des Rechenzentrums entscheidend',
        expert_3: '⚡ PUE (Power Usage Effectiveness): Zusätzlicher Energie-Overhead für Kühlung'
      },
      iceberg: {
        drawPrompt: 'KI-Generierung verbraucht viel Energie. Zeichne Eisberge und schau was geschieht...',
        redraw: 'Neu zeichnen',
        startMelting: 'Schmelzen starten',
        melting: 'Eisberg schmilzt...',
        melted: 'Geschmolzen!',
        meltedMessage: '{co2}g CO₂ produziert',
        comparison: 'Diese CO₂-Menge lässt etwa {volume} cm³ Arktis-Eis schmelzen.',
        comparisonInfo: '(Jede Tonne CO₂ = ca. 6m³ Meereis-Verlust)',
        gpuPower: 'Stromverbrauch der Grafikkarte',
        gpuTemp: 'Temperatur der Grafikkarte',
        co2Info: 'CO₂-Emissionen durch Stromverbrauch (basierend auf deutschem Strommix)',
        drawAgain: 'Zeichne weitere Eisberge...'
      },
      pixel: {
        grafikkarte: 'Grafikkarte',
        energieverbrauch: 'Energieverbrauch',
        co2Menge: 'CO₂-Menge',
        smartphoneComparison: 'Du müsstest Dein Handy {minutes} Minuten ausgeschaltet lassen, um den CO₂-Verbrauch wieder auszugleichen!',
        clickToProcess: 'Klicke auf die Daten-Pixel um ein Minibild zu generieren!'
      },
      forest: {
        trees: 'Bäume',
        clickToPlant: 'Klicke um Bäume zu pflanzen! Wo Du einen Baum pflanzt, wird die Fabrik verschwinden.',
        gameOver: 'Der Wald ist verloren!',
        treesPlanted: 'Du hast {count} Bäume gepflanzt.',
        complete: 'Generation abgeschlossen',
        comparison: 'Ein durchschnittlicher Baum braucht {minutes} Minuten, um diese CO₂-Menge zu absorbieren.'
      },
      rareearth: {
        clickToClean: 'Klicke auf den See um Giftschlamm zu entfernen!',
        sludgeRemoved: 'Schlamm entfernt',
        environmentHealth: 'Umwelt',
        gameOverInactive: 'Du hast aufgegeben... der Abbau geht weiter',
        infoBanner: 'Seltene Erden für GPU-Chips: Der Abbau hinterlässt Giftschlamm und zerstört Ökosysteme. Deine Aufräum-Arbeit kann die Geschwindigkeit des Abbaus nicht aufhalten.',
        instructionsCooldown: '⏳ {seconds}s',
        statsGpu: 'GPU',
        statsHealth: 'Umwelt',
        statsSludge: 'Schlamm entfernt'
      }
    }
  },
  en: {
    app: {
      title: 'UCDCAE AI LAB',
      subtitle: 'Creative AI Transformations'
    },
    form: {
      inputLabel: 'Your Text',
      inputPlaceholder: 'e.g. A flower in the meadow',
      schemaLabel: 'Transformation Style',
      executeModeLabel: 'Execution Mode',
      safetyLabel: 'Safety Level',
      generateButton: 'Generate'
    },
    schemas: {
      dada: 'Dada (Random & Absurd)',
      bauhaus: 'Bauhaus (Geometric)',
      stillepost: 'Stille Post (Iterative)'
    },
    executionModes: {
      eco: 'Eco (Fast)',
      fast: 'Fast (Balanced)',
      best: 'Best (Quality)'
    },
    safetyLevels: {
      kids: 'Kids',
      youth: 'Youth',
      adult: 'Adults',
      research: 'Research'
    },
    stages: {
      pipeline_starting: 'Pipeline Starting',
      translation_and_safety: 'Translation & Safety',
      interception: 'Transformation',
      pre_output_safety: 'Output Safety',
      media_generation: 'Image Generation',
      completed: 'Completed'
    },
    status: {
      idle: 'Ready',
      executing: 'Pipeline running...',
      connectionSlow: 'Connection slow, retrying...',
      completed: 'Pipeline completed!',
      error: 'Error occurred'
    },
    entities: {
      input: 'Input',
      translation: 'Translation',
      safety: 'Safety Check',
      interception: 'Transformation',
      safety_pre_output: 'Output Safety',
      media: 'Generated Image'
    },
    properties: {
      chill: 'chill',
      chaotic: 'wild',
      narrative: 'tell stories',
      algorithmic: 'follow rules',
      historical: 'history',
      contemporary: 'present',
      explore: 'test AI',
      create: 'make art',
      playful: 'playful',
      serious: 'serious'
    },
    phase2: {
      title: 'Prompt Input',
      userInput: 'Your Input',
      yourInput: 'Your Input',
      yourIdea: 'Your Idea: WHAT should this be about?',
      rules: 'Your Rules: HOW should your idea be implemented?',
      yourInstructions: 'Your Instructions',
      what: 'WHAT',
      how: 'HOW',
      userInputPlaceholder: 'e.g. A flower in the meadow',
      inputPlaceholder: 'Your text appears here...',
      metaPrompt: 'Artistic Instruction',
      instruction: 'Instruction',
      transformation: 'Artistic Transformation',
      metaPromptPlaceholder: 'Describe the transformation...',
      result: 'Result',
      expectedResult: 'Expected Result',
      execute: 'Execute Pipeline',
      executing: 'Running...',
      transforming: 'LLM transforming...',
      startTransformation: 'Start Transformation',
      letsGo: 'Ok, let\'s go!',
      modified: 'Modified',
      reset: 'Reset',
      loadingConfig: 'Loading configuration...',
      loadingMetaPrompt: 'Loading meta-prompt...',
      errorLoadingConfig: 'Error loading configuration',
      errorLoadingMetaPrompt: 'Error loading meta-prompt',
      threeForces: '3 Forces Working Together',
      twoForces: 'WHAT + HOW → LLM → Result',
      yourPrompt: 'Your Prompt:',
      writeYourText: 'Write your text...',
      examples: 'Examples',
      estimatedTime: '~12 seconds',
      stage12Time: '~5-10 seconds',
      willAppearAfterExecution: 'Will appear after execution...',
      back: 'Back',
      retry: 'Retry',
      transformedPrompt: 'Transformed Prompt',
      notYetTransformed: 'Not yet transformed...',
      transform: 'Transform',
      reTransform: 'Try again differently',
      startAI: 'AI, process my input',
      aiWorking: 'AI is working...',
      continueToMedia: 'Continue to Image Generation',
      readyForMedia: 'Ready for Image Generation',
      stage1: 'Stage 1: Translation + Safety...',
      stage2: 'Stage 2: Transformation...',
      selectMedia: 'Choose your medium:',
      mediaImage: 'Image',
      mediaAudio: 'Audio',
      mediaVideo: 'Video',
      media3D: '3D',
      comingSoon: 'Coming soon',
      generateMedia: 'Start!'
    },
    phase3: {
      generating: 'Image is being generated...',
      generatingHint: '~30 seconds'
    },
    common: {
      back: 'Back',
      loading: 'Loading...',
      error: 'Error',
      retry: 'Retry',
      cancel: 'Cancel'
    },
    gallery: {
      title: 'Favorites',  // Session 145: "My" redundant with switch
      empty: 'No favorites yet',
      favorite: 'Add to favorites',
      unfavorite: 'Remove from favorites',
      continue: 'Continue editing',
      restore: 'Restore session',
      viewMine: 'My favorites',  // Session 145
      viewAll: 'All favorites'  // Session 145
    },
    settings: {
      authRequired: 'Authentication Required',
      authPrompt: 'Please enter the password to access settings:',
      passwordPlaceholder: 'Enter password...',
      authenticate: 'Sign In',
      authenticating: 'Authenticating...'
    },
    pipeline: {
      yourInput: 'Your input',
      result: 'Result',
      generatedMedia: 'Generated image'
    },
    landing: {
      subtitlePrefix: 'Pedagogical-artistic experimentation platform of the',
      subtitleSuffix: 'for the explorative use of generative AI in cultural-aesthetic media education',
      research: '',
      features: {
        textTransformation: {
          title: 'Text Transformation',
          description: 'Perspective shift through AI — your prompt is transformed through artistic-pedagogical lenses into image, video, and sound.'
        },
        imageTransformation: {
          title: 'Image Transformation',
          description: 'Transform images through different models and perspectives into new images and videos.'
        },
        multiImage: {
          title: 'Image Fusion',
          description: 'Combine multiple images and merge them into new image compositions through AI models.'
        },
        canvas: {
          title: 'Canvas Workflow',
          description: 'Visual workflow composition — connect modules via drag & drop into custom AI pipelines.'
        },
        music: {
          title: 'Music Generation',
          description: 'AI-powered music creation with lyrics, tags, and stylistic control.'
        },
        latentLab: {
          title: 'Latent Lab',
          description: 'Vector space research — surrealization, dimension elimination, embedding interpolation.'
        }
      }
    },
    research: {
      locked: 'Only available in research mode',
      lockedHint: 'Requires safety level "Adult" or "Research" (config.py)',
      complianceTitle: 'Research Mode Notice',
      complianceWarning: 'In research mode, no safety filters are active for prompts or generated images. Unexpected or inappropriate results may occur.',
      complianceAge: 'This mode is not recommended for persons under 16 years of age.',
      complianceConfirm: 'I confirm that I have understood the notices',
      complianceCancel: 'Cancel',
      complianceProceed: 'Proceed'
    },
    presetOverlay: {
      title: 'Choose Perspective',
      close: 'Close'
    },
    imageUpload: {
      clickHere: 'Click here',
      orDragImage: 'or drag an image here',
      formatHint: 'PNG, JPG, WEBP (max 10MB)',
      invalidFormat: 'Invalid file format. Only PNG, JPG, and WEBP allowed.',
      fileTooLarge: 'File too large. Maximum: {max}MB',
      uploadFailed: 'Upload failed',
      infoOriginal: 'Original:',
      infoSize: 'Size:'
    },
    mediaInput: {
      choosePreset: 'Choose Perspective',
      translateToEnglish: 'Translate to English',
      copy: 'Copy',
      paste: 'Paste',
      delete: 'Delete',
      loading: 'Loading...',
      contentBlocked: 'Content blocked'
    },
    nav: {
      about: 'About',
      impressum: 'Imprint',
      privacy: 'Privacy',
      docs: 'Documentation',
      language: 'Switch language',
      settings: 'Settings',
      canvas: 'Canvas Workflow'
    },
    canvas: {
      title: 'Canvas Workflow',
      newWorkflow: 'New Workflow',
      importWorkflow: 'Import',
      exportWorkflow: 'Export',
      execute: 'Execute',
      ready: 'Ready',
      errors: 'errors',
      discardWorkflow: 'Discard current workflow?',
      importError: 'Failed to import file',
      selectTransformation: 'Select Transformation',
      selectOutput: 'Select Output Model',
      search: 'Search...',
      noResults: 'No results found',
      dragHint: 'Click or drag modules onto the canvas',
      editNameHint: '(double-click to edit)',
      modules: 'Modules',
      toggleSidebar: 'Toggle sidebar',
      dsgvoTooltip: 'Canvas workflows may use external LLM APIs. GDPR compliance is the responsibility of the user.',
      batchExecute: 'Batch Execute',
      batchExecution: 'Batch Execution',
      batchAbort: 'Abort Batch',
      abort: 'Abort',
      cancel: 'Cancel',
      loading: 'Loading...',
      executingWorkflow: 'Executing Workflow...',
      starting: 'Starting...',
      nodes: 'nodes',
      batchRunCount: 'Number of Runs',
      batchUseSeed: 'Use Base Seed',
      batchBaseSeed: 'Base Seed',
      batchSeedHint: 'Each run: seed + index',
      batchStart: 'Start Batch',
      stage: {
        configSelectPlaceholder: 'Select...',
        evaluationCriteriaFallback: 'Evaluation criteria...',
        branchFalseDefault: 'False',
        branchTrueDefault: 'True',
        feedbackInputTitle: 'Feedback Input',
        deleteTitle: 'Delete',
        selectLlmPlaceholder: 'Select LLM...',
        resizeTitle: 'Resize',
        input: {
          promptPlaceholder: 'Your prompt...'
        },
        imageInput: {
          uploadLabel: 'Upload Image'
        },
        interception: {
          contextPromptLabel: 'Context Prompt',
          contextPromptPlaceholder: 'Transformation instructions...'
        },
        translation: {
          translationPromptLabel: 'Translation Prompt',
          translationPromptPlaceholder: 'Translation instructions...'
        },
        modelAdaption: {
          targetModelLabel: 'Target Model',
          noAdaptionOption: 'No Adaption',
          videoModelsOption: 'Video Models',
          audioModelsOption: 'Audio Models'
        },
        comparisonEvaluator: {
          criteriaLabel: 'Comparison Criteria',
          criteriaPlaceholder: 'e.g. Compare by originality, clarity, detail...',
          infoText: 'Connect up to 3 text outputs'
        },
        seed: {
          modeLabel: 'Mode',
          modeFixed: 'Fixed',
          modeRandom: 'Random',
          valueLabel: 'Value',
          baseLabel: 'Base'
        },
        resolution: {
          customOption: 'Custom',
          widthLabel: 'Width',
          heightLabel: 'Height'
        },
        collector: {
          emptyText: 'Waiting for execution...'
        },
        evaluation: {
          typeLabel: 'Evaluation Type',
          typeCreativity: 'Creativity',
          typeQuality: 'Quality',
          typeCustom: 'Custom',
          criteriaLabel: 'Evaluation Criteria',
          outputTypeLabel: 'Output Type',
          outputCommentary: 'Commentary + Binary',
          outputScore: 'Commentary + Score + Binary',
          outputAll: 'All',
          enableBranching: 'Enable Branching',
          branchConditionLabel: 'Branch Condition',
          branchThresholdOption: 'Threshold (Score)',
          thresholdLabel: 'Threshold (0-10)',
          trueLabelFieldLabel: 'True Path Label',
          trueLabelDefault: 'Approved',
          trueLabelPlaceholder: 'e.g. Approved',
          falseLabelFieldLabel: 'False Path Label',
          falseLabelDefault: 'Needs Revision',
          falseLabelPlaceholder: 'e.g. Needs Revision',
          connectorPassthrough: 'Passthrough (OK - unchanged)',
          connectorCommented: 'Commented (FAIL - with feedback)',
          connectorCommentary: 'Commentary only (for display)'
        },
        imageEvaluation: {
          visionModelPlaceholder: 'Select Vision Model...',
          frameworkLabel: 'Analysis Framework',
          frameworkPanofsky: 'Art Historical (Panofsky)',
          frameworkEducational: 'Educational Theory',
          frameworkEthical: 'Ethical',
          frameworkCritical: 'Critical/Decolonial',
          frameworkCustom: 'Custom',
          customPromptLabel: 'Analysis Prompt',
          customPromptPlaceholder: 'Describe how the image should be analyzed...'
        },
        display: {
          imageAlt: 'Preview',
          emptyText: 'Preview (after execution)'
        }
      }
    },
    about: {
      title: 'About the UCDCAE AI LAB',
      intro: 'The UCDCAE AI LAB is a pedagogical-artistic experimentation platform of the UNESCO Chair in Digital Culture and Arts in Education for the explorative use of generative artificial intelligence in cultural-aesthetic media education. It was developed within the AI4ArtsEd and COMeARTS projects.',
      project: {
        title: 'The Project',
        description: 'AI is transforming society and the world of work; it is increasingly becoming a subject of education. The project explores opportunities, conditions, and limits of the pedagogical use of artificial intelligence (AI) in culturally diversity-sensitive settings of cultural education.',
        paragraph2: 'In three sub-projects – General Pedagogy (TPap), Computer Science (TPinf), and Art Education (TPkp) – creativity-oriented pedagogical AI practice research and computer science AI conception and programming interlock in close cooperation. From the outset, the project systematically involves artistic-pedagogical practitioners in the design process; it acts as a bridge between professional (quality-related, aesthetic, ethical, and value-based) pedagogical-practical implementation on the one hand and the implementation and training process of the computer science sub-project on the other.',
        paragraph3: 'A participatory design process spanning approximately two years aims to produce an open-source AI technology that explores the extent to which AI systems can already incorporate artistic-pedagogical principles at their structural level under favorable real-world conditions.',
        paragraph4: 'The focus is on a) the future applicability and added value of highly innovative technologies for cultural education, b) the scope and limits of AI literacy among teachers and learners, and c) the overarching question of the assessability and evaluation of the transformation of pedagogical settings by complex non-human actors in terms of pedagogical ethics and technology assessment.',
        moreInfo: 'More information:'
      },
      subproject: {
        title: 'Sub-project "General Pedagogy"',
        description: 'The sub-project "General Pedagogy" researches possibilities and limits of an artistic-pedagogical AI design process based on participatory practice research within the framework of the joint research question of the collaborative project. For this purpose, it conducts a series of research, analyses, expert workshops, and open spaces in the first project year. The subsequent project phase, designed as a feedback loop in several cycles, explores the use of a prototype with pedagogical practitioners and artist-educators, particularly in non-formal cultural education, as a relational and collective transformative educational process.'
      },
      team: {
        title: 'Team',
        projectLead: 'Project Lead',
        leadName: 'Prof. Dr. Benjamin Jörissen',
        leadInstitute: 'Institute of Education',
        leadChair: 'Chair of Education with Focus on Culture and Aesthetic Education',
        leadUnesco: 'UNESCO Chair in Digital Culture and Arts in Education',
        researcher: 'Research Associate',
        researcherName: 'Vanessa Baumann',
        researcherInstitute: 'Institute of Education',
        researcherChair: 'Chair of Education with Focus on Culture and Aesthetic Education',
        researcherUnesco: 'UNESCO Chair in Digital Culture and Arts in Education'
      },
      funding: {
        title: 'Funded by'
      }
    },
    legal: {
      impressum: {
        title: 'Imprint',
        publisher: 'Publisher',
        represented: 'Represented by the President',
        responsible: 'Responsible for content',
        authority: 'Supervisory Authority',
        moreInfo: 'Additional Information',
        moreInfoText: 'Complete imprint of FAU:',
        funding: 'Funded by'
      },
      privacy: {
        title: 'Privacy Policy',
        notice: 'Notice: Generated content is stored on the server for research purposes. No user or IP data is collected. Uploaded images are not stored.',
        usage: 'Use of this platform is exclusively permitted for registered cooperation partners of the UCDCAE AI LAB. The data protection agreements made in this context apply. If you have any questions, please contact vanessa.baumann@fau.de.'
      }
    },
    docs: {
      title: 'Documentation & Guide',
      intro: {
        title: 'Welcome',
        content: 'Creative experiments with AI transformations.'
      },
      gettingStarted: {
        title: 'Getting Started',
        step1: 'Select properties from quadrants',
        step2: 'Enter text or image',
        step3: 'Start transformation'
      },
      modes: {
        title: 'Modes',
        mode1: { name: 'Direct', desc: 'Quick experiments' },
        mode2: { name: 'Text', desc: 'Text-based transformations' },
        mode3: { name: 'Image', desc: 'Image-based procedures' }
      },
      support: {
        title: 'Support',
        content: 'For questions:'
      },
      wikipedia: {
        title: 'Wikipedia Research',
        subtitle: 'Knowledge about the world as part of artistic processes',
        feature: 'Artistic processes require not only aesthetic knowledge, but also knowledge about facts in the world. The AI researches Wikipedia during transformation to find factual information.',
        languages: 'Over 70 languages are supported',
        languagesDesc: 'The AI automatically chooses the appropriate language Wikipedia for each topic:',
        examples: {
          nigeria: 'Topic about Nigeria → Hausa, Yoruba, Igbo, or English',
          india: 'Topic about India → Hindi, Tamil, Bengali, or other regional languages',
          indigenous: 'Indigenous cultures → Quechua, Māori, Inuktitut, etc.'
        },
        why: 'Transparency: What does the AI know?',
        whyDesc: 'The system shows all research attempts: Both found articles (as clickable links) and terms for which nothing was found. This makes visible what the AI thinks it knows – and what it does not.',
        culturalRespect: 'Invitation to research yourself',
        culturalRespectDesc: 'The displayed Wikipedia links are an invitation to learn more yourself. Click on the links to check the sources and expand your own knowledge.',
        limitations: 'AI research is an aid, not a substitute for your own engagement with the topic.'
      }
    },
    multiImage: {
      image1Label: 'Image 1',
      image2Label: 'Image 2 (optional)',
      image3Label: 'Image 3 (optional)',
      contextLabel: 'Describe what you want to do with the images',
      contextPlaceholder: 'e.g. Insert the house from image 2 and the horse from image 3 into image 1. Preserve colors and style from image 1.',
      modeTitle: 'Multiple Images → Image',
      selectConfig: 'Choose your model:',
      generating: 'Images are being fused...'
    },
    imageTransform: {
      imageLabel: 'Your Image',
      contextLabel: 'Describe what you want to change in the image',
      contextPlaceholder: 'e.g. Transform it into an oil painting... Make it more colorful... Add a sunset...'
    },
    textTransform: {
      inputLabel: 'Your Idea = WHAT?',
      inputTooltip: 'Enter what your creation should be about.',
      inputPlaceholder: 'e.g. A festival in my street: ...',
      contextLabel: 'Your Rules = HOW?',
      contextTooltip: 'Enter how your idea should be presented, or click the circle icon!',
      contextPlaceholder: 'e.g. Describe everything as the birds in the trees perceive it!',
      resultLabel: 'Idea + Rules = Prompt',
      resultPlaceholder: 'Prompt will appear after clicking start (or enter your own text)',
      optimizedLabel: 'Model-Optimized Prompt',
      optimizedPlaceholder: 'The optimized prompt will appear after model selection.'
    },
    training: {
      info: {
        title: 'About LoRA Training',
        studioDescription: 'Train custom LoRA models for Stable Diffusion 3.5 Large with your own images.',
        description: 'This built-in training is designed for quick tests.',
        limitations: 'Limitations',
        limitationDuration: 'Training takes 1-3 hours',
        limitationBlocking: 'Blocks image generation during training',
        limitationConfig: 'Limited configuration options',
        showMore: 'Learn more',
        showLess: 'Show less'
      },
      placeholders: {
        projectName: 'e.g. Our School Building',
        triggerWords: 'e.g. our_school_building, schoolyard, classroom'
      },
      labels: {
        projectName: 'Project Name',
        triggerWords: 'Trigger Words',
        triggerHelp: 'Comma-separated tags. First = primary trigger, rest = additional tags per image.',
        images: 'Training Images (10–50 recommended)',
        dropZone: 'Click or drop images here',
        imagesSelected: '{count} images selected',
        logs: 'Training Logs',
        waiting: 'Waiting for training to start...'
      },
      buttons: {
        start: 'Start Training',
        stop: 'Stop',
        inProgress: 'Training in Progress...',
        delete: 'Delete Project Files (GDPR)',
        cancel: 'Cancel'
      },
      vram: {
        title: 'GPU VRAM Check',
        checking: 'Checking VRAM...',
        used: 'used',
        free: 'free',
        notEnough: 'Not enough free VRAM for training (need {gb} GB).',
        clearQuestion: 'Clear VRAM to continue?',
        enough: 'Enough VRAM available for training.',
        clearing: 'Clearing VRAM...',
        newFree: 'New free',
        clearBtn: 'Clear ComfyUI + Ollama VRAM'
      }
    },
    safetyBadges: {
      '§86a': '§86a',
      '86a_filter': '§86a',
      age_filter: 'Age Filter',
      dsgvo_ner: 'GDPR',
      dsgvo_llm: 'GDPR',
      translation: '\u2192 EN',
      fast_filter: 'Content',
      llm_context_check: 'Content (LLM)',
      llm_safety_check: 'Youth Protection',
      llm_check_failed: 'Check Failed',
      disabled: '\u2014'
    },
    safetyBlocked: {
      vlm: 'Your prompt was fine, but the generated image was flagged as unsuitable by an image analysis AI. This can happen \u2014 image generation is not always predictable. Just try again, every generation is different!',
      para86a: 'Your prompt was blocked because it contains symbols or terms that are prohibited under German law (\u00A786a StGB). This rule protects us all from hate and violence. Try a different topic!',
      dsgvo: 'Your prompt was blocked because it contains something that looks like a person\'s name. This is protected by the General Data Protection Regulation (GDPR). Use descriptions like \"a girl\" or \"an old man\" instead of names.',
      kids: 'Your prompt was blocked by the child safety filter. Some terms are not suitable for children because they can be scary or disturbing. Try describing your idea with friendlier words!',
      youth: 'Your prompt was blocked by the youth protection filter. Some content is not suitable for teenagers either. Try rephrasing your idea!',
      generic: 'Your prompt was blocked by the safety system. The system protects you from unsuitable content. Try a different wording!',
      inputImage: 'The uploaded image was flagged as unsuitable by an image analysis AI. Please use a different image.',
      vlmSaw: 'The image AI saw',
      systemUnavailable: 'The safety system (Ollama) is not responding, so no further processing is possible. Please contact the system administrator.',
      suggestionLoading: 'Hang on, I have an idea...',
      suggestionError: 'I couldn\'t generate a suggestion right now. Just try again with different words!'
    },
    splitCombine: {
      infoTitle: 'Split & Combine - Semantic Vector Fusion',
      infoDescription: 'This workflow fuses two prompts at the semantic vector level. The result is not a simple blend, but a deeper mathematical connection of meaning spaces.',
      purposeTitle: 'Pedagogical Purpose',
      purposeText: 'Explore how AI models represent meaning as numerical spaces. What happens when we mathematically merge different concepts?',
      techTitle: 'Technical Details',
      techText: 'Model: SD3.5 Large | Encoder: DualCLIP (CLIP-G + T5-XXL)'
    },
    partialElimination: {
      infoTitle: 'Partial Elimination - Vector Deconstruction',
      infoDescription: 'This workflow specifically manipulates parts of the semantic vector. By eliminating certain dimensions, we can observe which aspects of meaning are lost.',
      purposeTitle: 'Pedagogical Purpose',
      purposeText: 'Understand how meaning is encoded across different dimensions of the vector space. What remains when we "switch off" parts?',
      techTitle: 'Technical Details',
      techText: 'Model: SD3.5 Large | Encoder: TripleCLIP (CLIP-L + CLIP-G + T5-XXL)',
      encoderLabel: 'Text Encoder',
      modeLabel: 'Elimination Mode',
      dimensionRange: 'Dimension Range',
      selected: 'Selected',
      dimensions: 'Dimensions',
      emptyTitle: 'Waiting for generation...',
      emptySubtitle: 'Results will appear here',
      referenceLabel: 'Reference Image',
      referenceDesc: 'Unmanipulated output (original)',
      innerLabel: 'Inner range eliminated',
      outerLabel: 'Outer range eliminated'
    },
    surrealizer: {
      infoTitle: 'Hallucinator — Extrapolation Beyond the Known',
      infoDescription: 'Two AI "brains" read your text: CLIP-L understands language through images, T5 understands it purely linguistically. The slider doesn\'t simply blend between them — it pushes the image far beyond what T5 alone would produce. The AI must then interpret vectors it has never encountered during training. The result: AI hallucinations — images that no prompt could directly produce.',
      purposeTitle: 'The Slider',
      purposeText: 'α < 0: CLIP-L amplified, T5 negated — the upper 3328 dimensions (where CLIP-L is zero-padded) receive inverted T5 vectors. Cross-attention patterns in the transformer flip: visually driven hallucinations. ◆ α = 0: pure CLIP-L — normal image. ◆ α = 1: pure T5-XXL — still normal, but different quality. ◆ α > 1: extrapolation past T5. At α = 20 the formula pushes the embedding 19× beyond T5 into unexplored vector space — linguistically driven hallucinations. ◆ Sweet spot: α = 15–35.',
      techTitle: 'How It Works',
      techText: 'Your prompt is sent through two encoders separately: CLIP-L (visually trained, 77 tokens, 768 dims → padded to 4096) and T5-XXL (linguistically trained, 512 tokens, 4096 dims). The first 77 token positions are fused: (1-α)·CLIP-L + α·T5. The remaining T5 tokens (78–512) stay unchanged as a semantic anchor — they keep the image tied to your text no matter how extreme α gets. At α > 1 this is not blending but extrapolation: vectors no training ever produced. At α < 0, T5 is negated and CLIP-L amplified — qualitatively different hallucinations because cross-attention patterns in the transformer are inverted.',
      sliderLabel: 'Extrapolation (α)',
      sliderNormal: 'normal',
      sliderWeird: 'weird',
      sliderCrazy: 'crazy',
      sliderExtremeWeird: 'super weird',
      sliderExtremeCrazy: 'super crazy',
      sliderHint: "α<0: past CLIP {'|'} α=0: pure CLIP {'|'} α=1: pure T5 {'|'} α>1: past T5",
      expandLabel: 'Expand prompt for T5',
      expandSuggest: 'Short prompt detected — T5 expansion significantly improves results with few words.',
      expandHint: 'Your prompt has few words (~{count} CLIP tokens). For optimal hallucinations, the AI can narratively expand the T5 context.',
      expandActive: 'Expanding prompt...',
      expandResultLabel: 'T5 expansion (T5 encoder only)',
      advancedLabel: 'Advanced Settings',
      negativeLabel: 'Negative Prompt',
      negativeHint: 'Extrapolated with the same α. Determines what the image extrapolates AWAY from — different negatives produce fundamentally different aesthetics.',
      cfgLabel: 'CFG Scale',
      cfgHint: 'Classifier-Free Guidance: strength of prompt influence. Higher = stronger effect, less variation.'
    },
    musicGeneration: {
      infoTitle: 'Music Generation',
      infoDescription: 'Create music from text and style tags. The AI generates melodies, rhythms, and harmonies based on your lyrics and genre specifications.',
      purposeTitle: 'Pedagogical Purpose',
      purposeText: 'Explore how AI interprets musical concepts. How does word choice in lyrics affect the melody?',
      lyricsLabel: 'Lyrics (Text)',
      lyricsPlaceholder: '[Verse]\nYour lyrics here...\n\n[Chorus]\nChorus...',
      tagsLabel: 'Style Tags',
      tagsPlaceholder: 'pop, piano, upbeat, female vocal, 120bpm',
      selectModel: 'Choose a music model:',
      generate: 'Generate Music',
      generating: 'Generating music...'
    },
    musicGen: {
      simpleMode: 'Simple',
      advancedMode: 'Advanced',
      lyricsLabel: 'Lyrics',
      lyricsPlaceholder: 'Write your song lyrics with structure markers like [Verse], [Chorus], [Bridge]...\n\nExample:\n[Verse]\nde doo doo doo\nde blaa blaa blaa\n\n[Chorus]\nis all I want to sing to you',
      tagsLabel: 'Style Tags',
      tagsPlaceholder: 'Genre, mood, instruments...\n\nExample: ska, aggressive, upbeat, high definition, bass and sax trio',
      refineButton: 'Refine Lyrics & Tags',
      refinedLyricsLabel: 'Refined Lyrics',
      refinedLyricsPlaceholder: 'Your refined lyrics will appear here...',
      refiningLyricsMessage: 'AI is refining your lyrics...',
      refinedTagsLabel: 'Refined Tags',
      refinedTagsPlaceholder: 'Refined style tags will appear here...',
      refiningTagsMessage: 'AI is generating matching style tags...',
      selectModel: 'Choose a Music Model',
      generateButton: 'Generate Music',
      quality: 'Quality'
    },
    musicGenV2: {
      lyricsWorkshop: 'Lyrics Workshop',
      lyricsInput: 'Your Text',
      lyricsPlaceholder: 'Write lyrics, a theme, keywords, or a mood...',
      themeToLyrics: 'Keywords to Song Lyrics',
      refineLyrics: 'Structure Song Lyrics',
      resultLabel: 'Result',
      resultPlaceholder: 'Your lyrics will appear here...',
      expandingTheme: 'AI is writing song lyrics from your keywords...',
      refiningLyrics: 'AI is structuring your song lyrics...',
      soundExplorer: 'Sound Explorer',
      suggestFromLyrics: 'Suggest from Lyrics',
      suggestingTags: 'AI is analyzing your lyrics...',
      mostImportant: 'most important',
      dimGenre: 'Genre',
      dimTimbre: 'Timbre',
      dimGender: 'Voice',
      dimMood: 'Mood',
      dimInstrument: 'Instruments',
      dimScene: 'Scene',
      dimRegion: 'Region (UNESCO)',
      dimTopic: 'Topic',
      audioLength: 'Audio Length',
      generateButton: 'Generate Music',
      selectModel: 'Model',
      customTags: 'Custom Tags',
      customTagsPlaceholder: 'e.g. acoustic,dreamy,summer_vibes'
    },
    latentLab: {
      tabs: {
        attention: 'Attention Cartography',
        probing: 'Feature Probing',
        algebra: 'Concept Algebra',
        fusion: 'Encoder Fusion',
        archaeology: 'Denoising Archaeology',
        textlab: 'Latent Text Lab',
        crossmodal: 'Crossmodal Lab'
      },
      comingSoon: 'This tool will be implemented in a future version.',
      attention: {
        headerTitle: 'Attention Cartography — Which word steers which image region?',
        headerSubtitle: 'For each word in the prompt, a heatmap overlay on the generated image shows WHERE in the image that word had the most influence. This reveals how the model spatially distributes semantic concepts.',
        explanationToggle: 'Show detailed explanation',
        explainWhatTitle: 'What does this tool show?',
        explainWhatText: 'When a diffusion model generates an image, it does not read the prompt word by word like a set of instructions. Instead, a mechanism called "attention" distributes the influence of each word across different image regions. The word "house" mainly influences the region where the house appears — but also neighboring areas, because the model understands the context of the entire scene. This tool makes that distribution visible: click on a word and see which image regions light up.',
        explainHowTitle: 'How do I read the heatmap?',
        explainHowText: 'Bright, intense color = strong influence of the word on that region. Dark or absent color = little influence. If you select multiple words, they appear in different colors. Note: the maps are NOT perfectly sharp-edged — this is not a bug, but shows that the model processes concepts contextually, not in isolation. A "house" in a farm scene also has some influence on animals and fields, because the model understands the scene as a whole.',
        explainReadTitle: 'What do the two sliders reveal?',
        explainReadText: 'The denoising step slider shows WHEN in the 25-step generation process you are viewing attention. Early steps show rough layout planning, late steps show detail assignment. The network depth selector shows WHERE in the transformer attention is measured: shallow layers (near input) show global composition planning, middle layers semantic assignment, deep layers fine-tuning. Both axes are independent — it is worth systematically exploring different combinations.',
        techTitle: 'Technical details',
        techText: 'SD3.5 uses an MMDiT (Multimodal Diffusion Transformer) with joint attention: image and text tokens attend to each other across 24 transformer blocks. We replace the default SDPA processor with a manual softmax(QK^T/√d) processor at 3 selected blocks to extract the text→image attention submatrix. Maps are 64x64 resolution (patch grid), upscaled to image resolution via bilinear interpolation. SD3.5 uses two text encoders: CLIP-L (BPE, 77 tokens) and T5-XXL (SentencePiece, 512 tokens). Both can be toggled here to see how different tokenization strategies affect attention.',
        promptLabel: 'Prompt',
        promptPlaceholder: 'e.g. A house stands in a landscape, surrounded by farmland, nature and animals. Some people can be seen.',
        generate: 'Generate + Analyze',
        generating: 'Generating image and extracting attention...',
        emptyHint: 'Enter a prompt and click Generate to visualize the model\'s attention maps.',
        advancedLabel: 'Advanced Settings',
        negativeLabel: 'Negative Prompt',
        stepsLabel: 'Steps',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        tokensLabel: 'Tokens',
        tokensHint: 'Click one or more words. Subword tokens (e.g. "Ku"+"gel") are automatically combined. Multiple words appear in different colors.',
        timestepLabel: 'Denoising step',
        timestepHint: 'Diffusion models generate images in 25 steps from noise to image. Early steps establish rough structure, late steps refine details. This slider shows what the model attends to at each step.',
        step: 'Step',
        layerLabel: 'Network depth',
        layerHint: 'At every denoising step, the signal passes through all 24 transformer layers. Shallow layers (near input) capture global composition, middle layers semantic assignment, deep layers (near output) fine details. Both controls are independent: step = when in the process, depth = where in the network.',
        layerEarly: 'Shallow (Composition)',
        layerMid: 'Middle (Semantics)',
        layerLate: 'Deep (Detail)',
        opacityLabel: 'Heatmap',
        opacityHint: 'Strength of the colored overlay on the image.',
        baseImageLabel: 'Base image',
        baseColor: 'Color',
        baseBW: 'B/W',
        baseOff: 'Off',
        baseImageHint: 'Color shows the original image. B/W desaturates it so heatmap colors stand out. Off hides the image entirely and shows only the attention map.',
        encoderLabel: 'Text Encoder',
        encoderClipL: 'CLIP-L (77 Tokens)',
        encoderT5: 'T5-XXL (512 Tokens)',
        encoderHint: 'SD3.5 uses two text encoders with different tokenization. CLIP-L uses BPE (Byte-Pair Encoding), T5-XXL uses SentencePiece. Compare how both encoders process the same prompt and which image regions each one steers.',
        download: 'Download Image'
      },
      probing: {
        headerTitle: 'Feature Probing — Which dimensions encode what?',
        headerSubtitle: 'Compare two prompts and discover which embedding dimensions encode the semantic difference. Selectively transfer individual dimensions to see how they affect the image.',
        explanationToggle: 'Show detailed explanation',
        explainWhatTitle: 'What does this tool show?',
        explainWhatText: 'Every word is converted by the text encoder into a high-dimensional vector (e.g. 4096 dimensions for T5). When you change a word in the prompt — e.g. "red" to "blue" — certain dimensions change more than others. This tool shows you WHICH dimensions change most and lets you selectively transfer individual dimensions from prompt B into prompt A.',
        explainHowTitle: 'How does the transfer work?',
        explainHowText: 'The bar chart shows all dimensions sorted by difference magnitude. Use the rank range controls (From/To) to select a window — e.g. just the top 100 or specifically ranks 880–920. Clicking "Transfer" regenerates the image with the same settings (same seed!) — but with selected dimensions from prompt B. This lets you see exactly what those dimensions "encode".',
        explainReadTitle: 'How do I read the bar chart?',
        explainReadText: 'Each bar represents one embedding dimension. The length shows how much that dimension differs between prompt A and B. Dimensions with large differences are the most likely carriers of the semantic change. But note: embeddings are distributed — often multiple dimensions together are needed to produce a visible change.',
        techTitle: 'Technical details',
        techText: 'SD3.5 uses three text encoders: CLIP-L (768d), CLIP-G (1280d) and T5-XXL (4096d). You can probe each individually. The difference is computed as mean absolute deviation across all token positions: mean(abs(B-A), dim=tokens). The transfer replaces selected dimensions across all token positions simultaneously.',
        promptALabel: 'Prompt A (Original)',
        promptBLabel: 'Prompt B (Comparison)',
        promptAPlaceholder: 'e.g. A red house by the lake',
        promptBPlaceholder: 'e.g. A blue house by the lake',
        encoderLabel: 'Encoder',
        encoderAll: 'All (recommended)',
        encoderClipL: 'CLIP-L (768d)',
        encoderClipG: 'CLIP-G (1280d)',
        encoderT5: 'T5-XXL (4096d)',
        analyzeBtn: 'Analyze',
        analyzing: 'Encoding and comparing prompts...',
        transferBtn: 'Transfer selected vector dimensions from Prompt B into the generated image',
        transferring: 'Generating image with modified embedding...',
        rankFromLabel: 'From rank',
        rankToLabel: 'To rank',
        sliderLabel: 'Select dimensions from Prompt B',
        range1Label: 'Range 1',
        range2Label: 'Range 2',
        addRange: 'Add range',
        selectionDesc: '{count} dimensions from Prompt B selected (rank {ranges} of {total})',
        listTitle: 'The {count} dimensions from Prompt B with the largest difference to Prompt A',
        sortAsc: 'Ascending',
        sortDesc: 'Descending',
        originalLabel: 'Original (Prompt A)',
        modifiedLabel: 'Modified (Transfer from Prompt B)',
        modifiedHint: 'Select a rank range below and click "Transfer" — this will show prompt A with the transferred dimensions from B (same seed).',
        noDifference: 'The embeddings are identical — change prompt B.',
        advancedLabel: 'Advanced Settings',
        negativeLabel: 'Negative Prompt',
        stepsLabel: 'Steps',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        selectAll: 'All',
        selectNone: 'None',
        downloadOriginal: 'Download Original',
        downloadModified: 'Download Modified'
      },
      algebra: {
        headerTitle: 'Concept Algebra \u2014 Vector arithmetic on image embeddings',
        headerSubtitle: 'Apply the famous word2vec analogy to image generation: King \u2212 Man + Woman \u2248 Queen. Three prompts are encoded and algebraically combined.',
        explanationToggle: 'Show detailed explanation',
        explainWhatTitle: 'What does this tool show?',
        explainWhatText: 'In 2013, Mikolov showed that word embeddings encode semantic relationships as linear directions: the vector for "King" minus "Man" plus "Woman" yields a vector close to "Queen". This tool applies that idea to SD3.5\'s text encoders: instead of single words, you manipulate entire prompt embeddings. The result is an image that contains concept A but with B replaced by C.',
        explainHowTitle: 'How does the algebra work \u2014 and why not just use a negative prompt?',
        explainHowText: 'You enter three prompts: A (base), B (subtract), and C (add). The formula is: Result = A \u2212 Scale\u2081\u00d7B + Scale\u2082\u00d7C. The scale sliders control intensity: at 1.0, B is fully subtracted and C fully added. At 0.5, only half. Values above 1.0 amplify the effect. \u2014 Why not just use "A + C" as the prompt and "B" as the negative prompt? Because that does something fundamentally different: A negative prompt steers the denoising process away from B at EVERY one of the 25 steps \u2014 the model decides step by step how to interpret "not B". Concept Algebra instead computes a new vector BEFORE image generation: the subtraction happens in embedding space, not in the diffusion process. The result is a single vector that directly encodes "A without B-ness plus C-ness". The negative prompt says "don\'t do this". The algebra says "take this concept out and put that one in" \u2014 a surgical operation in meaning-space rather than a step-by-step avoidance strategy.',
        explainReadTitle: 'What do the results mean?',
        explainReadText: 'On the left you see the reference image (prompt A only, same seed). On the right, the algebra result. If the analogy works, the right image should show concept A but with the semantic change B\u2192C. Example: "Sunset at the beach" \u2212 "Beach" + "Mountains" \u2248 "Sunset over mountains". The L2 distance shows how far the result has moved from the original. \u2014 Is the operation commutative? No. Subtraction of B and addition of C happen relative to vector A. The direction B\u2192C only makes sense in the context of A: "King \u2212 Man" removes the "male" directions from the King vector, "+ Woman" adds the "female" directions \u2014 the result lands near "Queen". C is not surgically placed where B was removed; it is simply added. That this still works shows that semantic relationships are encoded as consistent linear directions in the vector space.',
        techTitle: 'Technical details',
        techText: 'The algebra is performed on the selected encoder embeddings: CLIP-L (768d), CLIP-G (1280d), T5-XXL (4096d), or all combined (589 tokens \u00d7 4096d). The same operation is also applied to pooled embeddings (2048d). Both images use the same seed for fair comparison.',
        promptALabel: 'Prompt A (Base)',
        promptAPlaceholder: 'e.g. Sunset at the beach with palm trees',
        promptBLabel: 'Prompt B (Subtract)',
        promptBPlaceholder: 'e.g. Beach with palm trees',
        promptCLabel: 'Prompt C (Add)',
        promptCPlaceholder: 'e.g. Snow-covered mountains',
        formulaLabel: 'A \u2212 B + C = ?',
        encoderLabel: 'Encoder',
        encoderAll: 'All (recommended)',
        encoderClipL: 'CLIP-L (768d)',
        encoderClipG: 'CLIP-G (1280d)',
        encoderT5: 'T5-XXL (4096d)',
        generateBtn: 'Compute',
        generating: 'Computing embeddings and generating images...',
        referenceLabel: 'Reference (Prompt A)',
        resultLabel: 'Result (A \u2212 B + C)',
        l2Label: 'L2 distance from original',
        advancedLabel: 'Advanced Settings',
        negativeLabel: 'Negative Prompt',
        stepsLabel: 'Steps',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        scaleSubLabel: 'Subtraction scale',
        scaleAddLabel: 'Addition scale',
        downloadReference: 'Download Reference',
        downloadResult: 'Download Result',
        resultHint: 'Enter three prompts and click Compute \u2014 the result of the vector arithmetic will appear here.'
      },
      archaeology: {
        headerTitle: 'Denoising Archaeology \u2014 How does noise become an image?',
        headerSubtitle: 'Observe every single denoising step. Diffusion models don\'t draw left-to-right \u2014 they work everywhere simultaneously, from rough shapes to fine detail.',
        explanationToggle: 'Show detailed explanation',
        explainWhatTitle: 'What does this tool show?',
        explainWhatText: 'A diffusion model creates an image by progressively removing noise. Unlike drawing from left to right, the model works on ALL image regions simultaneously. In the first steps, rough structures emerge: Where is up, where is down? Where is the horizon? In the middle steps, semantic content appears: objects, shapes, colors. The final steps refine textures and details. This tool makes every single step visible.',
        explainHowTitle: 'How do I use this tool?',
        explainHowText: 'Enter a prompt and click Generate. The model produces 25 intermediate images (one per denoising step). These appear as a filmstrip below. Click a thumbnail or use the timeline slider to view each step at full size. Compare early and late steps: When does the model "know" what it is drawing?',
        explainReadTitle: 'What do the three phases reveal?',
        explainReadText: 'Early steps (1\u20138): Global composition \u2014 basic structure, color distribution, layout planning. Middle steps (9\u201317): Semantic emergence \u2014 objects become recognizable, shapes crystallize. Late steps (18\u201325): Detail refinement \u2014 textures, edges, fine patterns. The transitions are gradual, but the phases clearly show: the model first "plans" globally, then refines locally. Particularly revealing: The very first step does not show fine-grained pixels, but colorful patches. This is because the noise is generated in latent space (128\u00d7128 at 16 channels), not in pixel space. The VAE translates each latent pixel into an ~8\u00d78 pixel patch \u2014 even pure Gaussian noise becomes coherent color clusters. The model never "thinks" in individual pixels, but always in this compressed space.',
        techTitle: 'Technical details',
        techText: 'SD3.5 Large uses Rectified Flow as scheduler with 25 default steps. At each step, the current latent vectors are decoded through the VAE (1024\u00d71024 JPEG). The VAE (Variational Autoencoder) translates the mathematical latent space into pixels. The latent representation is 128\u00d7128 at 16 channels \u2014 each latent pixel corresponds to an ~8\u00d78 pixel patch in the image. This is why even the first step shows colorful clusters instead of fine pixel noise: the VAE interprets random 16-dimensional vectors as coherent color patches.',
        promptLabel: 'Prompt',
        promptPlaceholder: 'e.g. A marketplace in a medieval town with people, buildings and a fountain',
        generate: 'Generate',
        generating: 'Generating image \u2014 recording every step...',
        emptyHint: 'Enter a prompt and click Generate to visualize the denoising process.',
        advancedLabel: 'Advanced Settings',
        negativeLabel: 'Negative Prompt',
        stepsLabel: 'Steps',
        cfgLabel: 'CFG',
        seedLabel: 'Seed',
        filmstripLabel: 'Denoising Filmstrip',
        timelineLabel: 'Step',
        phaseEarly: 'Composition',
        phaseMid: 'Semantics',
        phaseLate: 'Detail',
        phaseEarlyDesc: 'Global structure and color distribution emerge',
        phaseMidDesc: 'Objects and shapes become recognizable',
        phaseLateDesc: 'Textures and fine details are sharpened',
        finalImageLabel: 'Final image (full resolution)',
        download: 'Download Image'
      },
      textLab: {
        headerTitle: 'Latent Text Lab \u2014 Scientific LLM Deconstruction',
        headerSubtitle: 'Representation Engineering, comparative model archaeology, and systematic bias analysis: Three research-based tools for investigating language models.',
        explanationToggle: 'Show scientific foundations',
        explainWhatTitle: 'What does this tool show?',
        explainWhatText: 'This tool implements three current research approaches to LLM interpretability: Representation Engineering (Zou et al. 2023) finds concept directions in activation space and manipulates model behavior in a targeted way. Comparative Model Archaeology (Belinkov 2022, Olsson 2022) shows how different models internally represent the same text. Bias Archaeology (Zou 2023, Bricken 2023) uncovers systematic biases through controlled token manipulation.',
        explainHowTitle: 'How do I use this tool?',
        explainHowText: 'First load a model via the model panel (small for quick tests, 3B+ for clean concept directions). Three tabs are available: Tab 1 (RepEng) defines contrast pairs and finds concept directions. Tab 2 (Compare) loads two models and compares their internal representations. Tab 3 (Bias) runs systematic bias experiments.',
        modelPanel: {
          title: 'Model Management',
          presetLabel: 'Preset',
          presetNone: 'No preset (custom ID)',
          customModelLabel: 'HuggingFace Model ID',
          customModelPlaceholder: 'e.g. meta-llama/Llama-3.2-1B',
          quantizationLabel: 'Quantization',
          quantAuto: 'Auto',
          loadBtn: 'Load',
          unloadBtn: 'Unload',
          loading: 'Loading model...',
          statusLoaded: 'Loaded',
          statusNone: 'No model loaded',
          vramLabel: 'VRAM',
          noModelWarning: 'Load a model first to use the tools.'
        },
        tabs: {
          repeng: 'Representation Engineering',
          compare: 'Model Comparison',
          bias: 'Bias Archaeology'
        },
        repeng: {
          title: 'Representation Engineering',
          subtitle: 'Find concept directions in activation space and steer generation',
          guide: 'This experiment extracts a "truth direction" from the model. The pre-filled contrast pairs each contain a true and a false statement. From the differences in activations, the system computes a direction in high-dimensional space. When this direction is inverted (\u03b1 = -1), the model should generate a wrong answer for a factual prompt \u2014 even though it "knows" the correct one. This reveals: the model encodes not just knowledge, but also the tendency to answer correctly vs. incorrectly.',
          languageHint: 'Recommendation: English prompts work significantly better since most open-source LLMs (LLaMA, Mistral) were primarily trained on English data. The pre-filled examples are in English for this reason.',
          expectedResults: 'Expected results: At \u03b1 = 0 (baseline), the model generates the correct answer. At \u03b1 = -1 (inversion), a wrong answer should appear \u2014 this is the core of the experiment. At \u03b1 = +1, little changes because the model already answers correctly. Beyond |\u03b1| > 2, artifacts dominate (repetitions, nonsense). The "sweet spot" according to Zou et al. is |\u03b1| between 0.5 and 2.0. Explained variance > 50% indicates clean separation \u2014 below that, contrast pairs are too similar or too few (at least 3 recommended).',
          science: 'Based on Zou et al. (2023) "Representation Engineering" and Li et al. (2024) "Inference-Time Intervention". Core idea: LLMs encode abstract concepts (truth, sentiment, ethics) as directions in high-dimensional activation space. Through contrast pairs (true vs. false sentence), the direction encoding a concept can be extracted. Adding this direction at runtime changes model behavior in a targeted way \u2014 without retraining.',
          pairsTitle: 'Contrast Pairs',
          pairsSubtitle: 'At least 3 pairs recommended. Each pair should differ only in the target concept (true vs. false). The examples are editable.',
          positiveLabel: 'Positive (true)',
          negativeLabel: 'Negative (false)',
          positivePlaceholder: 'e.g. The capital of France is Paris',
          negativePlaceholder: 'e.g. The capital of France is Berlin',
          addPair: 'Add pair',
          removePair: 'Remove',
          targetLayerLabel: 'Target layer',
          targetLayerAuto: 'Last layer',
          findDirection: 'Find direction',
          finding: 'Computing concept direction...',
          directionFound: 'Concept direction found',
          varianceLabel: 'Explained variance',
          dimLabel: 'Dimensions',
          projectionsTitle: 'Contrast pair projections',
          testTitle: 'Test + Manipulation',
          testSubtitle: 'Enter a sentence and steer generation along the concept direction',
          testPromptLabel: 'Test prompt',
          testPromptPlaceholder: 'e.g. The capital of Germany is',
          alphaLabel: 'Manipulation strength (\u03b1)',
          temperatureLabel: 'Temperature',
          maxTokensLabel: 'Max tokens',
          seedLabel: 'Seed (-1 = random)',
          generateBtn: 'Generate with manipulation',
          generating: 'Running manipulated generation...',
          baselineLabel: 'Baseline (no manipulation)',
          manipulatedLabel: 'Manipulated (\u03b1 = {alpha})',
          projectionLabel: 'Projection onto concept direction',
          interpretationTitle: 'Interpretation',
          interpreting: 'Analyzing results...',
          interpretationError: 'Could not generate interpretation'
        },
        compare: {
          title: 'Comparative Model Archaeology',
          subtitle: 'Load two models and systematically compare their internal representations',
          science: 'Based on Belinkov (2022) "Probing Classifiers" and Olsson et al. (2022) "In-Context Learning and Induction Heads". The heatmap shows CKA (Centered Kernel Alignment) between layers of both models. High similarity means: these layers represent information in a similar way. Early layers (syntax) are often similar \u2014 late layers (semantics) diverge more strongly, especially with different model sizes.',
          modelATitle: 'Model A (loaded above)',
          modelAHint: 'Change via the model panel above',
          modelBTitle: 'Model B (second model)',
          modelBPresetLabel: 'Preset',
          modelBCustomLabel: 'HuggingFace Model ID',
          modelBCustomPlaceholder: 'e.g. TinyLlama/TinyLlama-1.1B-Chat-v1.0',
          modelBLoadBtn: 'Load Model B',
          modelBLoaded: 'Model B loaded',
          modelBNone: 'Model B not loaded',
          promptLabel: 'Prompt',
          promptPlaceholder: 'e.g. The cat sat on the mat and watched the birds',
          seedLabel: 'Seed',
          temperatureLabel: 'Temperature',
          maxTokensLabel: 'Max tokens',
          compareBtn: 'Compare',
          comparing: 'Comparing models...',
          heatmapTitle: 'Layer Alignment (CKA)',
          heatmapAxisA: 'Model A \u2014 Layers',
          heatmapAxisB: 'Model B \u2014 Layers',
          heatmapExplain: 'Bright cells = high representation similarity. Diagonal patterns show the models process information in a similar order.',
          attentionTitle: 'Attention Comparison (last layer)',
          modelALabel: 'Model A',
          modelBLabel: 'Model B',
          generationTitle: 'Generation Comparison (same seed)',
          layerStatsTitle: 'Layer Statistics',
          interpretationTitle: 'Interpretation',
          interpreting: 'Analyzing results...',
          interpretationError: 'Could not generate interpretation'
        },
        bias: {
          title: 'Bias Archaeology',
          subtitle: 'Systematic bias experiments through controlled token manipulation',
          science: 'Based on Zou et al. (2023) "Representation Engineering" and Bricken et al. (2023) "Towards Monosemanticity". Instead of free manipulation, this tool investigates systematic biases: What happens when all masculine pronouns are suppressed? Which gender does the model choose as default? How strongly does the text change when positive vs. negative words are boosted? The answers reveal which biases are encoded in the model.',
          presetLabel: 'Experiment type',
          presetGender: 'Gender \u2014 Suppress gendered pronouns',
          presetSentiment: 'Sentiment \u2014 Boost positive/negative',
          presetDomain: 'Domain \u2014 Boost scientific/poetic',
          presetCustom: 'Custom experiment',
          promptLabel: 'Prompt',
          promptPlaceholder: 'e.g. The doctor said to the patient',
          customBoostLabel: 'Boost tokens (comma-separated)',
          customBoostPlaceholder: 'e.g. dark,shadow,night',
          customSuppressLabel: 'Suppress tokens (comma-separated)',
          customSuppressPlaceholder: 'e.g. light,sun,bright',
          numSamplesLabel: 'Samples per condition',
          temperatureLabel: 'Temperature',
          maxTokensLabel: 'Max tokens',
          seedLabel: 'Base seed',
          runBtn: 'Run experiment',
          running: 'Running bias experiment...',
          baselineTitle: 'Baseline (no manipulation)',
          groupTitle: 'Group: {name}',
          modeSuppress: 'suppressed',
          modeBoost: 'boosted',
          tokensLabel: 'Tokens',
          sampleSeedLabel: 'Seed',
          genderDesc: 'Suppresses all gendered pronouns and observes which defaults the model chooses.',
          sentimentDesc: 'Boosts positive or negative words and measures how strongly the entire text flow is affected.',
          domainDesc: 'Boosts scientific or poetic vocabulary and observes register shifts.',
          interpretationTitle: 'Interpretation',
          interpreting: 'Analyzing results...',
          interpretationError: 'Could not generate interpretation'
        },
        error: {
          gpuUnreachable: 'GPU service unreachable. Is it running?',
          loadFailed: 'Failed to load model.',
          operationFailed: 'Operation failed.'
        }
      },
      crossmodal: {
        headerTitle: 'Crossmodal Lab',
        headerSubtitle: 'Sound from latent spaces: T5 embedding manipulation, image-guided audio generation, crossmodal transfer',
        generate: 'Generate',
        generating: 'Generating...',
        result: 'Result',
        seed: 'Seed',
        generationTime: 'Generation time',
        tabs: {
          synth: {
            label: 'Latent Audio Synth',
            short: 'T5 embedding manipulation',
            title: 'Latent Audio Synth',
            description: 'Direct manipulation of Stable Audio\'s T5 conditioning space (768d). Interpolate between prompts, extrapolate beyond the prompt, scale embeddings and inject noise. Ultra-short loops, near real-time.'
          },
          mmaudio: {
            label: 'MMAudio',
            short: 'Image/text to audio (CVPR 2025)',
            title: 'MMAudio — Video/Image to Audio',
            description: 'MMAudio (CVPR 2025) is a dedicated crossmodal model trained on both modalities. 157M parameters, generates 8s audio in ~1.2s. Genuine crossmodal transfer instead of naive feature projection.'
          },
          guidance: {
            label: 'ImageBind Guidance',
            short: 'Gradient-based image guidance',
            title: 'ImageBind Gradient Guidance',
            description: 'Gradient-based steering during the Stable Audio denoising process. ImageBind provides a shared 1024d space for image and audio — the gradient of the cosine similarity guides audio generation towards the image embedding.'
          }
        },
        synth: {
          promptA: 'Prompt A (Base)',
          promptAPlaceholder: 'e.g. ocean waves',
          promptB: 'Prompt B (Optional, for interpolation)',
          promptBPlaceholder: 'e.g. piano melody',
          alpha: 'Alpha (Interpolation)',
          alphaHint: '0 = A only, 1 = B only, between = blend, >1 or <0 = extrapolation',
          magnitude: 'Magnitude (Scaling)',
          magnitudeHint: 'Global embedding scaling (1.0 = unchanged)',
          noise: 'Noise',
          noiseHint: 'Gaussian noise on embedding (0 = no noise)',
          duration: 'Duration (s)',
          steps: 'Steps',
          cfg: 'CFG',
          loop: 'Loop playback',
          loopOn: 'Loop On',
          loopOff: 'Loop Off',
          stop: 'Stop',
          looping: 'Looping',
          playing: 'Playing',
          stopped: 'Stopped',
          transpose: 'Transpose (semitones)',
          midiSection: 'MIDI Control',
          midiUnsupported: 'Web MIDI is not supported by this browser.',
          midiInput: 'MIDI Input',
          midiNone: '(none)',
          midiMappings: 'CC Mappings',
          midiNoteC3: 'Note (C3 = Ref)',
          midiGenerate: 'Generate + Transpose',
          midiPitch: 'Pitch rel. C3',
          loopInterval: 'Loop interval',
          loopOptimize: 'Auto-optimize',
          loopPingPong: 'Ping-pong',
          loopIntervalHint: 'Start/end of loop region — shorten end to trim Stable Audio fade-out',
          modeLoop: 'Loop',
          modePingPong: 'Ping-Pong',
          modeWavetable: 'Wavetable',
          modeRate: 'Tempo (fast)',
          modePitch: 'Pitch (OLA)',
          wavetableScan: 'Scan Position',
          wavetableScanHint: 'Morph between frames (0 = start, 1 = end)',
          wavetableFrames: '{count} frames',
          midiScan: 'Scan Position',
          adsrTitle: 'ADSR Envelope',
          adsrAttack: 'A',
          adsrDecay: 'D',
          adsrSustain: 'S',
          adsrRelease: 'R',
          adsrHint: 'Envelope for MIDI notes (Attack/Decay/Sustain/Release)',
          play: 'Play',
          normalize: 'Normalize loudness',
          peak: 'Peak',
          crossfade: 'Crossfade',
          saveRaw: 'Save raw',
          saveLoop: 'Save loop',
          embeddingStats: 'Embedding statistics',
          dimensions: {
            section: 'Dimension Explorer',
            hint: 'Drag on bars = set offset. Paint horizontally = multiple dimensions.',
            resetAll: 'Reset all',
            hoverActivation: 'Activation',
            hoverOffset: 'Offset',
            rightClickReset: 'Right-click = reset',
            sortDiff: 'Sorted by prompt difference',
            sortMagnitude: 'Sorted by activation',
            activeOffsets: '{count} offsets active',
            applyAndGenerate: 'Apply and regenerate',
            undo: 'Undo',
            redo: 'Redo'
          }
        },
        mmaudio: {
          imageUpload: 'Upload image',
          prompt: 'Text prompt',
          promptPlaceholder: 'e.g. crackling campfire',
          negativePrompt: 'Negative prompt',
          duration: 'Duration (s)',
          maxDuration: 'Max 8s (model limit)',
          cfg: 'CFG',
          steps: 'Steps',
          compareHint: 'Compare: Text only vs. Image + Text'
        },
        guidance: {
          imageUpload: 'Upload image',
          prompt: 'Base prompt (optional)',
          promptPlaceholder: 'e.g. ambient soundscape',
          lambda: 'Guidance strength',
          lambdaHint: 'How strongly the image steers audio generation',
          warmupSteps: 'Warmup steps',
          warmupHint: 'Gradient guidance only during first N steps',
          totalSteps: 'Total steps',
          duration: 'Duration (s)',
          cfg: 'CFG',
          cosineSimilarity: 'Cosine similarity (image-audio proximity)'
        }
      }
    },
    edutainment: {
      ui: {
        didYouKnow: '🤔 Did you know?',
        learnMore: '📚 Learn more',
        currentlyHappening: '⚡ Currently happening:',
        energyUsed: 'Energy used',
        co2Produced: 'CO₂ produced'
      },
      energy: {
        kids_1: '💡 AI images need electricity – as much as charging your phone for 3 hours!',
        kids_2: '🔌 The GPU is like a super calculator that eats lots of power!',
        kids_3: '⚡ Each image needs as much energy as running an LED light for 10 minutes!',
        youth_1: '⚡ A GPU uses {watts}W while generating – like a small space heater!',
        youth_2: '🔋 One image uses about 0.01-0.02 kWh – sounds little, but adds up!',
        youth_3: '🌡️ The GPU is getting {temp}°C hot right now – that\'s why it needs cooling!',
        expert_1: '📊 Realtime: {watts}W at {util}% utilization = {kwh} kWh so far',
        expert_2: '🔥 TDP limit: {tdp}W | Current: {watts}W ({percent}% of limit)',
        expert_3: '💾 VRAM: {used}/{total} GB ({percent}%) – model + activations'
      },
      data: {
        kids_1: '🧮 The GPU is calculating 10 billion times right now – faster than you can count!',
        kids_2: '🎨 The image is created in 50 small steps – like a puzzle solving itself!',
        kids_3: '🧩 Millions of numbers are flying through the GPU right now!',
        youth_1: '🔄 Each image goes through ~50 "denoising steps" – 50 rounds of removing noise!',
        youth_2: '📐 8 billion parameters are being queried – per image!',
        youth_3: '🧠 The AI "thinks" in vectors with thousands of dimensions – like coordinates in a space.',
        expert_1: '🔬 MMDiT: Multimodal Diffusion Transformer – text + image in joint attention layers',
        expert_2: '📈 Self-Attention: O(n²) complexity – every token "sees" all others',
        expert_3: '⚙️ Classifier-Free Guidance: prompt influence vs. creativity balance'
      },
      model: {
        kids_1: '🎓 The AI model looked at millions of images to learn how to paint!',
        kids_2: '🤖 The AI is like an artist who never forgets what they\'ve seen!',
        kids_3: '✨ 8 billion connections in the model – more than stars you can see in the sky!',
        youth_1: '🧠 SD3.5 Large has 8 billion parameters – like 8 billion decision nodes.',
        youth_2: '📚 3 text encoders work together: CLIP-L, CLIP-G, and T5-XXL',
        youth_3: '🔢 The model needs {vram} GB VRAM just to be loaded!',
        expert_1: '🏗️ Architecture: Rectified Flow + MMDiT with 38 transformer blocks',
        expert_2: '📊 FP16/FP8 quantization: precision vs. VRAM trade-off',
        expert_3: '🔗 LoRA: Low-Rank Adaptation – only 0.1% of parameters retrained'
      },
      ethics: {
        kids_1: '🌍 AI learns from images on the internet – that\'s why it\'s important to be fair with other people\'s art!',
        kids_2: '⚖️ Not all artists were asked if the AI could learn from them.',
        kids_3: '🤝 Good AI respects people\'s work!',
        youth_1: '📜 Training data often comes from the internet. Artists debate: Fair Use or copying?',
        youth_2: '🏛️ The EU AI Act demands transparency: Where does the training data come from?',
        youth_3: '💭 Question: Who actually owns an AI-generated image?',
        expert_1: '⚠️ LAION-5B was partly created without creator consent – legal gray area.',
        expert_2: '📋 EU AI Act Art. 52: Labeling requirement for AI-generated content',
        expert_3: '🔍 Model Cards & Datasheets: Best practice for ML transparency'
      },
      environment: {
        kids_1: '☁️ Each AI image produces a bit of CO₂ – like driving a car, but less!',
        kids_2: '🌱 Think: Is this image worth the electricity?',
        kids_3: '🌞 Energy for AI often comes from power plants – some clean, some not.',
        youth_1: '🏭 German power grid: ~400g CO₂ per kWh – that adds up!',
        youth_2: '📈 {co2}g CO₂ for this image – with 1000 images that would be {totalKg} kg!',
        youth_3: '💡 Tip: Generate fewer images, but more thoughtfully – saves energy and CO₂.',
        expert_1: '📊 Calculation: {watts}W × {seconds}s ÷ 3600 × 400g/kWh = {co2}g CO₂',
        expert_2: '🔬 Scope 2 emissions: data center location is decisive',
        expert_3: '⚡ PUE (Power Usage Effectiveness): Additional energy overhead for cooling'
      },
      iceberg: {
        drawPrompt: 'AI generation uses a lot of energy. Draw icebergs and see what happens...',
        redraw: 'Redraw',
        startMelting: 'Start melting',
        melting: 'Iceberg melting...',
        melted: 'Melted!',
        meltedMessage: '{co2}g CO₂ produced',
        comparison: 'This CO₂ amount melts about {volume} cm³ of Arctic ice.',
        comparisonInfo: '(Each ton of CO₂ = approx. 6m³ sea ice loss)',
        gpuPower: 'Graphics card power consumption',
        gpuTemp: 'Graphics card temperature',
        co2Info: 'CO₂ emissions from power consumption (based on German energy mix)',
        drawAgain: 'Draw more icebergs...'
      },
      pixel: {
        grafikkarte: 'Graphics Card',
        energieverbrauch: 'Energy Usage',
        co2Menge: 'CO₂ Amount',
        smartphoneComparison: 'You would need to keep your phone off for {minutes} minutes to offset this CO₂ usage!',
        clickToProcess: 'Click on the data pixels to generate a mini image!'
      },
      forest: {
        trees: 'Trees',
        clickToPlant: 'Click to plant trees! Where you plant a tree, the factory will disappear.',
        gameOver: 'The forest is lost!',
        treesPlanted: 'You planted {count} trees.',
        complete: 'Generation complete',
        comparison: 'An average tree needs {minutes} minutes to absorb this amount of CO₂.'
      },
      rareearth: {
        clickToClean: 'Click the lake to remove toxic sludge!',
        sludgeRemoved: 'Sludge removed',
        environmentHealth: 'Environment',
        gameOverInactive: 'You gave up... mining continues',
        infoBanner: 'Rare earth mining for GPU chips leaves toxic sludge and destroys ecosystems. Your cleanup efforts cannot match the speed of extraction.',
        instructionsCooldown: '⏳ {seconds}s',
        statsGpu: 'GPU',
        statsHealth: 'Environment',
        statsSludge: 'Sludge removed'
      }
    }
  },
  tr: {
    app: {
      title: 'UCDCAE AI LAB',
      subtitle: 'Yaratıcı Yapay Zeka Dönüşümleri'
    },
    form: {
      inputLabel: 'Metniniz',
      inputPlaceholder: 'örn. Çayırdaki bir çiçek',
      schemaLabel: 'Dönüşüm Stili',
      executeModeLabel: 'Çalışma Modu',
      safetyLabel: 'Güvenlik Seviyesi',
      generateButton: 'Oluştur'
    },
    schemas: {
      dada: 'Dada (Rastgele & Absürt)',
      bauhaus: 'Bauhaus (Geometrik)',
      stillepost: 'Stille Post (Yinelemeli)'
    },
    executionModes: {
      eco: 'Eko (Hızlı)',
      fast: 'Hızlı (Dengeli)',
      best: 'En İyi (Kalite)'
    },
    safetyLevels: {
      kids: 'Çocuklar',
      youth: 'Gençler',
      adult: 'Yetişkinler',
      research: 'Araştırma'
    },
    stages: {
      pipeline_starting: 'Pipeline Başlatılıyor',
      translation_and_safety: 'Çeviri & Güvenlik',
      interception: 'Dönüşüm',
      pre_output_safety: 'Çıktı Güvenliği',
      media_generation: 'Görsel Oluşturma',
      completed: 'Tamamlandı'
    },
    status: {
      idle: 'Hazır',
      executing: 'Pipeline çalışıyor...',
      connectionSlow: 'Bağlantı yavaş, yeniden deneniyor...',
      completed: 'Pipeline tamamlandı!',
      error: 'Hata oluştu'
    },
    entities: {
      input: 'Girdi',
      translation: 'Çeviri',
      safety: 'Güvenlik Kontrolü',
      interception: 'Dönüşüm',
      safety_pre_output: 'Çıktı Güvenliği',
      media: 'Oluşturulan Görsel'
    },
    properties: {
      chill: 'rahat',
      chaotic: 'çılgın',
      narrative: 'hikaye anlat',
      algorithmic: 'kurallara uy',
      historical: 'tarih',
      contemporary: 'güncel',
      explore: 'yapay zekayı dene',
      create: 'sanat yap',
      playful: 'eğlenceli',
      serious: 'ciddi'
    },
    phase2: {
      title: 'Prompt Girişi',
      userInput: 'Girdiniz',
      yourInput: 'Girdiniz',
      yourIdea: 'Fikriniz: Bu NE hakkında olmalı?',
      rules: 'Kurallarınız: Fikriniz NASIL hayata geçirilmeli?',
      yourInstructions: 'Talimatlarınız',
      what: 'NE',
      how: 'NASIL',
      userInputPlaceholder: 'örn. Çayırdaki bir çiçek',
      inputPlaceholder: 'Metniniz burada görünecek...',
      metaPrompt: 'Sanatsal Talimat',
      instruction: 'Talimat',
      transformation: 'Sanatsal Dönüşüm',
      metaPromptPlaceholder: 'Dönüşümü açıklayın...',
      result: 'Sonuç',
      expectedResult: 'Beklenen Sonuç',
      execute: 'Pipeline\'ı Çalıştır',
      executing: 'Çalışıyor...',
      transforming: 'LLM dönüştürüyor...',
      startTransformation: 'Dönüşümü Başlat',
      letsGo: 'Tamam, başlayalım!',
      modified: 'Değiştirildi',
      reset: 'Sıfırla',
      loadingConfig: 'Yapılandırma yükleniyor...',
      loadingMetaPrompt: 'Meta-prompt yükleniyor...',
      errorLoadingConfig: 'Yapılandırma yüklenirken hata oluştu',
      errorLoadingMetaPrompt: 'Meta-prompt yüklenirken hata oluştu',
      threeForces: 'Birlikte Çalışan 3 Kuvvet',
      twoForces: 'NE + NASIL → LLM → Sonuç',
      yourPrompt: 'Promptunuz:',
      writeYourText: 'Metninizi yazın...',
      examples: 'Örnekler',
      estimatedTime: '~12 saniye',
      stage12Time: '~5-10 saniye',
      willAppearAfterExecution: 'Çalıştırıldıktan sonra görünecek...',
      back: 'Geri',
      retry: 'Tekrar Dene',
      transformedPrompt: 'Dönüştürülmüş Prompt',
      notYetTransformed: 'Henüz dönüştürülmedi...',
      transform: 'Dönüştür',
      reTransform: 'Farklı şekilde tekrar dene',
      startAI: 'Yapay zeka, girdimi işle',
      aiWorking: 'Yapay zeka çalışıyor...',
      continueToMedia: 'Görsel Oluşturmaya Geç',
      readyForMedia: 'Görsel Oluşturma için Hazır',
      stage1: 'Aşama 1: Çeviri + Güvenlik...',
      stage2: 'Aşama 2: Dönüşüm...',
      selectMedia: 'Medyanızı seçin:',
      mediaImage: 'Görsel',
      mediaAudio: 'Ses',
      mediaVideo: 'Video',
      media3D: '3D',
      comingSoon: 'Yakında',
      generateMedia: 'Başla!'
    },
    phase3: {
      generating: 'Görsel oluşturuluyor...',
      generatingHint: '~30 saniye'
    },
    common: {
      back: 'Geri',
      loading: 'Yükleniyor...',
      error: 'Hata',
      retry: 'Tekrar Dene',
      cancel: 'İptal'
    },
    gallery: {
      title: 'Favoriler',
      empty: 'Henüz favori yok',
      favorite: 'Favorilere ekle',
      unfavorite: 'Favorilerden kaldır',
      continue: 'Düzenlemeye devam et',
      restore: 'Oturumu geri yükle',
      viewMine: 'Favorilerim',
      viewAll: 'Tüm favoriler'
    },
    settings: {
      authRequired: 'Kimlik Doğrulama Gerekli',
      authPrompt: 'Ayarlara erişmek için lütfen şifreyi girin:',
      passwordPlaceholder: 'Şifre girin...',
      authenticate: 'Giriş Yap',
      authenticating: 'Kimlik doğrulanıyor...'
    },
    pipeline: {
      yourInput: 'Girdiniz',
      result: 'Sonuç',
      generatedMedia: 'Oluşturulan görsel'
    },
    landing: {
      subtitlePrefix: 'UNESCO Dijital Kültür ve Sanatta Eğitim Kürsüsü\'nün pedagojik-sanatsal deney platformu',
      subtitleSuffix: 'kültürel-estetik medya eğitiminde üretken yapay zekanın keşifsel kullanımı için',
      research: '',
      features: {
        textTransformation: {
          title: 'Metin Dönüşümü',
          description: 'Yapay zeka aracılığıyla bakış açısı değişimi — promptunuz sanatsal-pedagojik lensler aracılığıyla görsel, video ve sese dönüştürülür.'
        },
        imageTransformation: {
          title: 'Görsel Dönüşümü',
          description: 'Görselleri farklı modeller ve bakış açıları aracılığıyla yeni görsellere ve videolara dönüştürün.'
        },
        multiImage: {
          title: 'Görsel Füzyonu',
          description: 'Birden fazla görseli birleştirin ve yapay zeka modelleri aracılığıyla yeni görsel kompozisyonlarla birleştirin.'
        },
        canvas: {
          title: 'Kanvas İş Akışı',
          description: 'Görsel iş akışı kompozisyonu — modülleri sürükle & bırak ile özel yapay zeka pipeline\'larına bağlayın.'
        },
        music: {
          title: 'Müzik Oluşturma',
          description: 'Sözler, etiketler ve stilistik kontrol ile yapay zeka destekli müzik yaratımı.'
        },
        latentLab: {
          title: 'Latent Lab',
          description: 'Vektör uzayı araştırması — gerçeküstüleştirme, boyut eleme, gömme interpolasyonu.'
        }
      }
    },
    research: {
      locked: 'Yalnızca araştırma modunda kullanılabilir',
      lockedHint: '"Yetişkin" veya "Araştırma" güvenlik seviyesi gerektirir (config.py)',
      complianceTitle: 'Araştırma Modu Bildirimi',
      complianceWarning: 'Araştırma modunda, promptlar veya oluşturulan görseller için hiçbir güvenlik filtresi etkin değildir. Beklenmedik veya uygunsuz sonuçlar oluşabilir.',
      complianceAge: 'Bu mod, 16 yaşın altındaki kişilere tavsiye edilmez.',
      complianceConfirm: 'Bildirimleri anladığımı onaylıyorum',
      complianceCancel: 'İptal',
      complianceProceed: 'Devam Et'
    },
    presetOverlay: {
      title: 'Bakış Açısı Seçin',
      close: 'Kapat'
    },
    imageUpload: {
      clickHere: 'Buraya tıklayın',
      orDragImage: 'veya bir görsel sürükleyin',
      formatHint: 'PNG, JPG, WEBP (maks. 10MB)',
      invalidFormat: 'Geçersiz dosya formatı. Yalnızca PNG, JPG ve WEBP kabul edilir.',
      fileTooLarge: 'Dosya çok büyük. Maksimum: {max}MB',
      uploadFailed: 'Yükleme başarısız',
      infoOriginal: 'Orijinal:',
      infoSize: 'Boyut:'
    },
    mediaInput: {
      choosePreset: 'Bakış Açısı Seçin',
      translateToEnglish: 'İngilizceye çevir',
      copy: 'Kopyala',
      paste: 'Yapıştır',
      delete: 'Sil',
      loading: 'Yükleniyor...',
      contentBlocked: 'İçerik engellendi'
    },
    nav: {
      about: 'Hakkında',
      impressum: 'Künye',
      privacy: 'Gizlilik',
      docs: 'Dokümantasyon',
      language: 'Dil değiştir',
      settings: 'Ayarlar',
      canvas: 'Kanvas İş Akışı'
    },
    canvas: {
      title: 'Kanvas İş Akışı',
      newWorkflow: 'Yeni İş Akışı',
      importWorkflow: 'İçe Aktar',
      exportWorkflow: 'Dışa Aktar',
      execute: 'Çalıştır',
      ready: 'Hazır',
      errors: 'hatalar',
      discardWorkflow: 'Mevcut iş akışı iptal edilsin mi?',
      importError: 'Dosya içe aktarılamadı',
      selectTransformation: 'Dönüşüm Seçin',
      selectOutput: 'Çıktı Modeli Seçin',
      search: 'Ara...',
      noResults: 'Sonuç bulunamadı',
      dragHint: 'Modülleri kanvasa tıklayın veya sürükleyin',
      editNameHint: '(düzenlemek için çift tıklayın)',
      modules: 'Modüller',
      toggleSidebar: 'Kenar çubuğunu aç/kapat',
      dsgvoTooltip: 'Kanvas iş akışları harici LLM API\'larını kullanabilir. KVKK uyumluluğu kullanıcının sorumluluğundadır.',
      batchExecute: 'Toplu Çalıştırma',
      batchExecution: 'Toplu Çalıştırma',
      batchAbort: 'Toplu işlemi iptal et',
      abort: 'İptal',
      cancel: 'İptal',
      loading: 'Yükleniyor...',
      executingWorkflow: 'İş akışı çalıştırılıyor...',
      starting: 'Başlatılıyor...',
      nodes: 'düğüm',
      batchRunCount: 'Çalıştırma Sayısı',
      batchUseSeed: 'Temel Seed Kullan',
      batchBaseSeed: 'Temel Seed',
      batchSeedHint: 'Her çalıştırma: seed + index',
      batchStart: 'Toplu Başlat',
      stage: {
        configSelectPlaceholder: 'Seçin...',
        evaluationCriteriaFallback: 'Değerlendirme kriterleri...',
        branchFalseDefault: 'Yanlış',
        branchTrueDefault: 'Doğru',
        feedbackInputTitle: 'Geri Bildirim Girişi',
        deleteTitle: 'Sil',
        selectLlmPlaceholder: 'LLM Seçin...',
        resizeTitle: 'Boyutlandır',
        input: {
          promptPlaceholder: 'Promptunuz...'
        },
        imageInput: {
          uploadLabel: 'Görsel Yükle'
        },
        interception: {
          contextPromptLabel: 'Bağlam Promptu',
          contextPromptPlaceholder: 'Dönüşüm talimatları...'
        },
        translation: {
          translationPromptLabel: 'Çeviri Promptu',
          translationPromptPlaceholder: 'Çeviri talimatları...'
        },
        modelAdaption: {
          targetModelLabel: 'Hedef Model',
          noAdaptionOption: 'Adaptasyon Yok',
          videoModelsOption: 'Video Modelleri',
          audioModelsOption: 'Ses Modelleri'
        },
        comparisonEvaluator: {
          criteriaLabel: 'Karşılaştırma Kriterleri',
          criteriaPlaceholder: 'örn. Orijinallik, netlik, ayrıntıya göre karşılaştırın...',
          infoText: 'En fazla 3 metin çıkışı bağlayın'
        },
        seed: {
          modeLabel: 'Mod',
          modeFixed: 'Sabit',
          modeRandom: 'Rastgele',
          valueLabel: 'Değer',
          baseLabel: 'Temel'
        },
        resolution: {
          customOption: 'Özel',
          widthLabel: 'Genişlik',
          heightLabel: 'Yükseklik'
        },
        collector: {
          emptyText: 'Çalıştırma bekleniyor...'
        },
        evaluation: {
          typeLabel: 'Değerlendirme Türü',
          typeCreativity: 'Yaratıcılık',
          typeQuality: 'Kalite',
          typeCustom: 'Özel',
          criteriaLabel: 'Değerlendirme Kriterleri',
          outputTypeLabel: 'Çıkış Türü',
          outputCommentary: 'Yorum + Binary',
          outputScore: 'Yorum + Skor + Binary',
          outputAll: 'Tümü',
          enableBranching: 'Dallanmayı Etkinleştir',
          branchConditionLabel: 'Dallanma Koşulu',
          branchThresholdOption: 'Eşik Değeri (Skor)',
          thresholdLabel: 'Eşik Değeri (0-10)',
          trueLabelFieldLabel: 'Doğru Yol Etiketi',
          trueLabelDefault: 'Onaylandı',
          trueLabelPlaceholder: 'örn. Onaylandı',
          falseLabelFieldLabel: 'Yanlış Yol Etiketi',
          falseLabelDefault: 'Revizyon Gerekli',
          falseLabelPlaceholder: 'örn. Revizyon Gerekli',
          connectorPassthrough: 'Geçiş (OK - değiştirilmedi)',
          connectorCommented: 'Yorumlandı (FAIL - geri bildirimli)',
          connectorCommentary: 'Yalnızca Yorum (görüntüleme için)'
        },
        imageEvaluation: {
          visionModelPlaceholder: 'Görüntü Modeli Seçin...',
          frameworkLabel: 'Analiz Çerçevesi',
          frameworkPanofsky: 'Sanat Tarihsel (Panofsky)',
          frameworkEducational: 'Eğitim Teorisi',
          frameworkEthical: 'Etik',
          frameworkCritical: 'Eleştirel/Dekolonyal',
          frameworkCustom: 'Özel Talimat',
          customPromptLabel: 'Analiz Promptu',
          customPromptPlaceholder: 'Görselin nasıl analiz edilmesi gerektiğini açıklayın...'
        },
        display: {
          imageAlt: 'Önizleme',
          emptyText: 'Önizleme (çalıştırmadan sonra)'
        }
      }
    },
    about: {
      title: 'UCDCAE AI LAB Hakkında',
      intro: 'UCDCAE AI LAB, UNESCO Dijital Kültür ve Sanatta Eğitim Kürsüsü\'nün kültürel-estetik medya eğitiminde üretken yapay zekanın keşifsel kullanımına yönelik pedagojik-sanatsal bir deney platformudur. AI4ArtsEd ve COMeARTS projeleri kapsamında geliştirilmiştir.',
      project: {
        title: 'Proje',
        description: 'Yapay zeka toplumu ve iş dünyasını dönüştürmektedir; giderek eğitimin bir konusu haline gelmektedir. Proje, kültürel çeşitliliğe duyarlı kültürel eğitim ortamlarında yapay zekanın (YZ) pedagojik kullanımının fırsatlarını, koşullarını ve sınırlarını araştırmaktadır.',
        paragraph2: 'Üç alt proje — Genel Pedagoji (TPap), Bilgisayar Bilimi (TPinf) ve Sanat Eğitimi (TPkp) — yaratıcılık odaklı pedagojik YZ pratik araştırması ile bilgisayar bilimi YZ kavramlaştırması ve programlamanın yakın iş birliği içinde kesiştiği bir yapı oluşturmaktadır. Proje başından itibaren tasarım sürecine sanatsal-pedagojik uygulayıcıları sistematik olarak dahil etmektedir; bir yanda mesleki (kalite, estetik, etik ve değer odaklı) pedagojik-pratik uygulama ile diğer yanda bilgisayar bilimi alt projesinin uygulama ve eğitim süreci arasında köprü görevi görmektedir.',
        paragraph3: 'Yaklaşık iki yıl süren katılımcı bir tasarım süreci, elverişli gerçek dünya koşulları altında yapay zeka sistemlerinin yapısal düzeyde sanatsal-pedagojik ilkeleri ne ölçüde içerebileceğini araştıran açık kaynaklı bir yapay zeka teknolojisi üretmeyi amaçlamaktadır.',
        paragraph4: 'Odak noktaları: a) kültürel eğitim için son derece yenilikçi teknolojilerin gelecekteki uygulanabilirliği ve katma değeri, b) öğretmenler ve öğrenciler arasında YZ okuryazarlığının kapsamı ve sınırları ve c) pedagojik etik ve teknoloji değerlendirmesi açısından karmaşık insan dışı aktörler tarafından pedagojik ortamların dönüşümünün değerlendirilebilirliği ve değerlendirmesi üzerine kapsamlı soru.',
        moreInfo: 'Daha fazla bilgi:'
      },
      subproject: {
        title: '"Genel Pedagoji" Alt Projesi',
        description: '"Genel Pedagoji" alt projesi, ortak araştırma projesinin ortak araştırma sorusu çerçevesinde katılımcı pratik araştırmaya dayalı sanatsal-pedagojik bir YZ tasarım sürecinin olanaklarını ve sınırlarını araştırmaktadır. Bu amaçla ilk proje yılında bir dizi araştırma, analiz, uzman çalıştayları ve açık alanlar yürütmektedir. Birden fazla döngüde geri bildirim döngüsü olarak tasarlanan sonraki proje aşaması, özellikle biçimsel olmayan kültürel eğitimde pedagojik uygulayıcılar ve sanatçı-eğitimcilerle bir prototip kullanımını ilişkisel ve kolektif dönüştürücü bir eğitim süreci olarak araştırmaktadır.'
      },
      team: {
        title: 'Ekip',
        projectLead: 'Proje Lideri',
        leadName: 'Prof. Dr. Benjamin Jörissen',
        leadInstitute: 'Eğitim Enstitüsü',
        leadChair: 'Kültür ve Estetik Eğitime Odaklanan Eğitim Kürsüsü',
        leadUnesco: 'UNESCO Dijital Kültür ve Sanatta Eğitim Kürsüsü',
        researcher: 'Araştırma Görevlisi',
        researcherName: 'Vanessa Baumann',
        researcherInstitute: 'Eğitim Enstitüsü',
        researcherChair: 'Kültür ve Estetik Eğitime Odaklanan Eğitim Kürsüsü',
        researcherUnesco: 'UNESCO Dijital Kültür ve Sanatta Eğitim Kürsüsü'
      },
      funding: {
        title: 'Destekleyen'
      }
    },
    legal: {
      impressum: {
        title: 'Künye',
        publisher: 'Yayıncı',
        represented: 'Rektör tarafından temsil edilmektedir',
        responsible: 'İçerikten sorumlu',
        authority: 'Denetleme Makamı',
        moreInfo: 'Ek Bilgi',
        moreInfoText: 'FAU\'nun tam künyesi:',
        funding: 'Destekleyen'
      },
      privacy: {
        title: 'Gizlilik Politikası',
        notice: 'Bildirim: Oluşturulan içerik araştırma amaçlı sunucuda saklanmaktadır. Kullanıcı veya IP verisi toplanmamaktadır. Yüklenen görseller saklanmamaktadır.',
        usage: 'Bu platformun kullanımı yalnızca UCDCAE AI LAB\'ın kayıtlı iş birliği ortakları için izin verilmektedir. Bu bağlamda yapılan veri koruma anlaşmaları geçerlidir. Sorularınız için lütfen vanessa.baumann@fau.de adresiyle iletişime geçin.'
      }
    },
    docs: {
      title: 'Dokümantasyon & Rehber',
      intro: {
        title: 'Hoş Geldiniz',
        content: 'Yapay zeka dönüşümleriyle yaratıcı deneyler.'
      },
      gettingStarted: {
        title: 'Başlarken',
        step1: 'Kadranlardan özellikler seçin',
        step2: 'Metin veya görsel girin',
        step3: 'Dönüşümü başlatın'
      },
      modes: {
        title: 'Modlar',
        mode1: { name: 'Doğrudan', desc: 'Hızlı deneyler' },
        mode2: { name: 'Metin', desc: 'Metin tabanlı dönüşümler' },
        mode3: { name: 'Görsel', desc: 'Görsel tabanlı işlemler' }
      },
      support: {
        title: 'Destek',
        content: 'Sorular için:'
      },
      wikipedia: {
        title: 'Wikipedia Araştırması',
        subtitle: 'Sanatsal süreçlerin bir parçası olarak dünya hakkında bilgi',
        feature: 'Sanatsal süreçler yalnızca estetik bilgi değil, dünya hakkında olgusal bilgi de gerektirir. Yapay zeka, dönüşüm sırasında olgusal bilgi bulmak için Wikipedia\'yı araştırır.',
        languages: '70\'ten fazla dil desteklenmektedir',
        languagesDesc: 'Yapay zeka her konu için uygun Wikipedia dilini otomatik olarak seçer:',
        examples: {
          nigeria: 'Nijerya hakkında konu → Hausa, Yoruba, Igbo veya İngilizce',
          india: 'Hindistan hakkında konu → Hintçe, Tamilce, Bengalce veya diğer bölgesel diller',
          indigenous: 'Yerli kültürler → Keçuva, Maori, İnuktitut, vb.'
        },
        why: 'Şeffaflık: Yapay zeka ne biliyor?',
        whyDesc: 'Sistem tüm araştırma girişimlerini gösterir: Hem bulunan makaleleri (tıklanabilir bağlantılar olarak) hem de hiçbir şey bulunamayan terimleri. Bu, yapay zekanın ne bildiğini düşündüğünü — ve ne bilmediğini — görünür kılar.',
        culturalRespect: 'Kendiniz araştırmaya davet',
        culturalRespectDesc: 'Gösterilen Wikipedia bağlantıları, daha fazlasını öğrenmek için bir davettir. Kaynakları kontrol etmek ve kendi bilginizi genişletmek için bağlantılara tıklayın.',
        limitations: 'Yapay zeka araştırması bir yardımcıdır, konuyla kendi ilgilenmenizin yerini alamaz.'
      }
    },
    multiImage: {
      image1Label: 'Görsel 1',
      image2Label: 'Görsel 2 (isteğe bağlı)',
      image3Label: 'Görsel 3 (isteğe bağlı)',
      contextLabel: 'Görsellerle ne yapmak istediğinizi açıklayın',
      contextPlaceholder: 'örn. Görsel 2\'deki evi ve Görsel 3\'teki atı Görsel 1\'e ekleyin. Görsel 1\'deki renkleri ve stili koruyun.',
      modeTitle: 'Birden Fazla Görsel → Görsel',
      selectConfig: 'Modelinizi seçin:',
      generating: 'Görseller birleştiriliyor...'
    },
    imageTransform: {
      imageLabel: 'Görseliniz',
      contextLabel: 'Görselde ne değiştirmek istediğinizi açıklayın',
      contextPlaceholder: 'örn. Yağlı boya tabloya dönüştür... Daha renkli yap... Gün batımı ekle...'
    },
    textTransform: {
      inputLabel: 'Fikriniz = NE?',
      inputTooltip: 'Kreasyonunuzun ne hakkında olacağını girin.',
      inputPlaceholder: 'örn. Sokağımda bir festival: ...',
      contextLabel: 'Kurallarınız = NASIL?',
      contextTooltip: 'Fikrinizin nasıl sunulacağını girin veya daire simgesine tıklayın!',
      contextPlaceholder: 'örn. Her şeyi ağaçlardaki kuşların algıladığı gibi tanımlayın!',
      resultLabel: 'Fikir + Kurallar = Prompt',
      resultPlaceholder: 'Prompt, başlat butonuna tıklandıktan sonra görünecek (veya kendi metninizi girin)',
      optimizedLabel: 'Model-Optimize Edilmiş Prompt',
      optimizedPlaceholder: 'Optimize edilmiş prompt, model seçiminden sonra görünecek.'
    },
    training: {
      info: {
        title: 'LoRA Eğitimi Hakkında',
        studioDescription: 'Stable Diffusion 3.5 Large için kendi görsellerinizle özel LoRA modelleri eğitin.',
        description: 'Bu yerleşik eğitim hızlı testler için tasarlanmıştır.',
        limitations: 'Sınırlamalar',
        limitationDuration: 'Eğitim 1-3 saat sürer',
        limitationBlocking: 'Eğitim sırasında görsel oluşturmayı engeller',
        limitationConfig: 'Sınırlı yapılandırma seçenekleri',
        showMore: 'Daha fazla bilgi',
        showLess: 'Daha az göster'
      },
      placeholders: {
        projectName: 'örn. Okul Binamız',
        triggerWords: 'örn. okul_binamiz, okul_bahcesi, sinif'
      },
      labels: {
        projectName: 'Proje Adı',
        triggerWords: 'Tetikleyici Kelimeler',
        triggerHelp: 'Virgülle ayrılmış etiketler. İlki = birincil tetikleyici, geri kalanları = görsel başına ek etiketler.',
        images: 'Eğitim Görselleri (10–50 önerilir)',
        dropZone: 'Görselleri buraya tıklayın veya bırakın',
        imagesSelected: '{count} görsel seçildi',
        logs: 'Eğitim Günlükleri',
        waiting: 'Eğitimin başlaması bekleniyor...'
      },
      buttons: {
        start: 'Eğitimi Başlat',
        stop: 'Durdur',
        inProgress: 'Eğitim Devam Ediyor...',
        delete: 'Proje Dosyalarını Sil (KVKK)',
        cancel: 'İptal'
      },
      vram: {
        title: 'GPU VRAM Kontrolü',
        checking: 'VRAM kontrol ediliyor...',
        used: 'kullanılan',
        free: 'serbest',
        notEnough: 'Eğitim için yeterli serbest VRAM yok ({gb} GB gerekli).',
        clearQuestion: 'Devam etmek için VRAM temizlensin mi?',
        enough: 'Eğitim için yeterli VRAM mevcut.',
        clearing: 'VRAM temizleniyor...',
        newFree: 'Yeni serbest',
        clearBtn: 'ComfyUI + Ollama VRAM\'ı Temizle'
      }
    },
    safetyBadges: {
      '§86a': '§86a',
      '86a_filter': '§86a',
      age_filter: 'Yaş Filtresi',
      dsgvo_ner: 'KVKK',
      dsgvo_llm: 'KVKK',
      translation: '\u2192 EN',
      fast_filter: 'İçerik',
      llm_context_check: 'İçerik (LLM)',
      llm_safety_check: 'Gençlik Koruması',
      llm_check_failed: 'Kontrol Başarısız',
      disabled: '\u2014'
    },
    safetyBlocked: {
      vlm: 'Promptunuz sorunsuzdu, ancak oluşturulan görsel bir görsel analiz yapay zekası tarafından uygunsuz olarak işaretlendi. Bu olabilir \u2014 görsel oluşturma her zaman öngörülebilir değildir. Tekrar deneyin, her oluşturma farklıdır!',
      para86a: 'Promptunuz, Alman hukuku kapsamında yasak olan semboller veya terimler içerdiği için engellendi (\u00A786a StGB). Bu kural hepimizi nefret ve şiddetten korur. Farklı bir konu deneyin!',
      dsgvo: 'Promptunuz, bir kişi adına benzeyen bir şey içerdiği için engellendi. Bu, Genel Veri Koruma Yönetmeliği (GDPR/KVKK) kapsamında koruma altındadır. İsimler yerine "bir kız" veya "yaşlı bir adam" gibi tanımlamalar kullanın.',
      kids: 'Promptunuz çocuk güvenlik filtresi tarafından engellendi. Bazı terimler korkutucu veya rahatsız edici olabileceğinden çocuklar için uygun değildir. Fikrinizi daha dostane kelimelerle açıklamayı deneyin!',
      youth: 'Promptunuz gençlik koruması filtresi tarafından engellendi. Bazı içerikler gençler için de uygun değildir. Fikrinizi farklı şekilde ifade etmeyi deneyin!',
      generic: 'Promptunuz güvenlik sistemi tarafından engellendi. Sistem sizi uygunsuz içerikten korur. Farklı bir ifade deneyin!',
      inputImage: 'Yüklenen görsel bir görsel analiz yapay zekası tarafından uygunsuz olarak işaretlendi. Lütfen farklı bir görsel kullanın.',
      vlmSaw: 'Görsel yapay zekası gördü',
      systemUnavailable: 'Güvenlik sistemi (Ollama) yanıt vermiyor, bu nedenle daha fazla işlem yapılamıyor. Lütfen sistem yöneticisiyle iletişime geçin.',
      suggestionLoading: 'Bekleyin, bir fikrim var...',
      suggestionError: 'Şu an bir öneri oluşturamadım. Farklı kelimelerle tekrar deneyin!'
    },
    splitCombine: {
      infoTitle: 'Böl & Birleştir - Anlamsal Vektör Füzyonu',
      infoDescription: 'Bu iş akışı iki promptu anlamsal vektör düzeyinde birleştirir. Sonuç basit bir karışım değil, anlam uzaylarının daha derin matematiksel bir bağlantısıdır.',
      purposeTitle: 'Pedagojik Amaç',
      purposeText: 'Yapay zeka modellerinin anlamı sayısal uzaylar olarak nasıl temsil ettiğini keşfedin. Farklı kavramları matematiksel olarak birleştirdiğimizde ne olur?',
      techTitle: 'Teknik Detaylar',
      techText: 'Model: SD3.5 Large | Kodlayıcı: DualCLIP (CLIP-G + T5-XXL)'
    },
    partialElimination: {
      infoTitle: 'Kısmi Eleme - Vektör Dekonstruksiyonu',
      infoDescription: 'Bu iş akışı anlamsal vektörün belirli bölümlerini özellikle manipüle eder. Belirli boyutları ortadan kaldırarak, anlam özelliklerinin hangi yönlerinin kaybolduğunu gözlemleyebiliriz.',
      purposeTitle: 'Pedagojik Amaç',
      purposeText: 'Anlamın vektör uzayının farklı boyutlarında nasıl kodlandığını anlayın. Bazı parçaları "kapattığımızda" ne kalır?',
      techTitle: 'Teknik Detaylar',
      techText: 'Model: SD3.5 Large | Kodlayıcı: TripleCLIP (CLIP-L + CLIP-G + T5-XXL)',
      encoderLabel: 'Metin Kodlayıcı',
      modeLabel: 'Eleme Modu',
      dimensionRange: 'Boyut Aralığı',
      selected: 'Seçili',
      dimensions: 'Boyutlar',
      emptyTitle: 'Oluşturma bekleniyor...',
      emptySubtitle: 'Sonuçlar burada görünecek',
      referenceLabel: 'Referans Görsel',
      referenceDesc: 'Manipüle edilmemiş çıktı (orijinal)',
      innerLabel: 'İç aralık elendi',
      outerLabel: 'Dış aralık elendi'
    },
    surrealizer: {
      infoTitle: 'Halüsinatör — Bilinenden Ötesine Ekstrapolasyon',
      infoDescription: 'İki yapay zeka "beyni" metninizi okur: CLIP-L dili görseller aracılığıyla, T5 ise onu saf dilbilimsel olarak anlar. Kaydırıcı sadece aralarında karıştırmaz — görseli T5\'in tek başına üretebileceğinin çok ötesine iter. Yapay zekanın ardından eğitim sırasında hiç karşılaşmadığı vektörleri yorumlaması gerekir. Sonuç: Yapay zeka halüsinasyonları — hiçbir promptun doğrudan üretemeyeceği görseller.',
      purposeTitle: 'Kaydırıcı',
      purposeText: 'α < 0: CLIP-L yükseltildi, T5 olumsuzlandı — üst 3328 boyut (CLIP-L\'nin sıfırla doldurulduğu) ters çevrilmiş T5 vektörleri alır. Transformatördeki çapraz dikkat örüntüleri tersine döner: görsel odaklı halüsinasyonlar. ◆ α = 0: saf CLIP-L — normal görsel. ◆ α = 1: saf T5-XXL — hâlâ normal, ancak farklı kalite. ◆ α > 1: T5\'in ötesine ekstrapolasyon. α = 20\'de formül gömmeyi T5\'in 19 katı ötesinde keşfedilmemiş vektör uzayına iter — dilbilimsel odaklı halüsinasyonlar. ◆ En iyi nokta: α = 15–35.',
      techTitle: 'Nasıl Çalışır',
      techText: 'Promptunuz iki kodlayıcı aracılığıyla ayrı ayrı gönderilir: CLIP-L (görsel olarak eğitilmiş, 77 token, 768 boyut → 4096\'ya doldurulmuş) ve T5-XXL (dilbilimsel olarak eğitilmiş, 512 token, 4096 boyut). İlk 77 token konumu birleştirilir: (1-α)·CLIP-L + α·T5. Geri kalan T5 tokenları (78–512) anlamsal çapa olarak değişmeden kalır — α ne kadar uç olursa olsun görseli metninize bağlarlar. α > 1\'de bu karışım değil ekstrapoloasyondur: hiçbir eğitimin üretmediği vektörler. α < 0\'da T5 olumsuzlanır ve CLIP-L yükseltilir — transformatördeki çapraz dikkat örüntüleri tersine çevrildiğinden niteliksel olarak farklı halüsinasyonlar.',
      sliderLabel: 'Ekstrapolasyon (α)',
      sliderNormal: 'normal',
      sliderWeird: 'tuhaf',
      sliderCrazy: 'çılgın',
      sliderExtremeWeird: 'süper tuhaf',
      sliderExtremeCrazy: 'süper çılgın',
      sliderHint: "α<0: CLIP ötesi {'|'} α=0: saf CLIP {'|'} α=1: saf T5 {'|'} α>1: T5 ötesi",
      expandLabel: 'T5 için promptu genişlet',
      expandSuggest: 'Kısa prompt algılandı — T5 genişletmesi az kelimeyle sonuçları önemli ölçüde iyileştirir.',
      expandHint: 'Promptunuzda az kelime var (~{count} CLIP token). Optimal halüsinasyonlar için yapay zeka T5 bağlamını anlatısal olarak genişletebilir.',
      expandActive: 'Prompt genişletiliyor...',
      expandResultLabel: 'T5 genişletmesi (yalnızca T5 kodlayıcı)',
      advancedLabel: 'Gelişmiş Ayarlar',
      negativeLabel: 'Negatif Prompt',
      negativeHint: 'Aynı α ile ekstrapolasyon yapılır. Görselin NEREDEN uzaklaştığını belirler — farklı negatifler temelden farklı estetikler üretir.',
      cfgLabel: 'CFG Ölçeği',
      cfgHint: 'Classifier-Free Guidance: prompt etkisinin gücü. Yüksek = daha güçlü etki, daha az varyasyon.'
    },
    musicGeneration: {
      infoTitle: 'Müzik Oluşturma',
      infoDescription: 'Metin ve stil etiketlerinden müzik oluşturun. Yapay zeka sözlerinize ve tür belirtimlerinize göre melodiler, ritimler ve armoniler üretir.',
      purposeTitle: 'Pedagojik Amaç',
      purposeText: 'Yapay zekanın müzikal kavramları nasıl yorumladığını keşfedin. Sözlerdeki kelime seçimi melodiyi nasıl etkiler?',
      lyricsLabel: 'Sözler (Metin)',
      lyricsPlaceholder: '[Kıta]\nSözleriniz burada...\n\n[Nakarat]\nNakarat...',
      tagsLabel: 'Stil Etiketleri',
      tagsPlaceholder: 'pop, piyano, neşeli, kadın vokal, 120bpm',
      selectModel: 'Bir müzik modeli seçin:',
      generate: 'Müzik Oluştur',
      generating: 'Müzik oluşturuluyor...'
    },
    musicGen: {
      simpleMode: 'Basit',
      advancedMode: 'Gelişmiş',
      lyricsLabel: 'Sözler',
      lyricsPlaceholder: 'Şarkı sözlerinizi [Kıta], [Nakarat], [Köprü] gibi yapı işaretçileriyle yazın...\n\nÖrnek:\n[Kıta]\nde doo doo doo\nde blaa blaa blaa\n\n[Nakarat]\nis all I want to sing to you',
      tagsLabel: 'Stil Etiketleri',
      tagsPlaceholder: 'Tür, ruh hali, enstrümanlar...\n\nÖrnek: ska, agresif, neşeli, yüksek kalite, bas ve saksofon üçlüsü',
      refineButton: 'Sözleri & Etiketleri İyileştir',
      refinedLyricsLabel: 'İyileştirilmiş Sözler',
      refinedLyricsPlaceholder: 'İyileştirilmiş sözleriniz burada görünecek...',
      refiningLyricsMessage: 'Yapay zeka sözlerinizi iyileştiriyor...',
      refinedTagsLabel: 'İyileştirilmiş Etiketler',
      refinedTagsPlaceholder: 'İyileştirilmiş stil etiketleri burada görünecek...',
      refiningTagsMessage: 'Yapay zeka eşleşen stil etiketleri oluşturuyor...',
      selectModel: 'Bir Müzik Modeli Seçin',
      generateButton: 'Müzik Oluştur',
      quality: 'Kalite'
    },
    musicGenV2: {
      lyricsWorkshop: 'Söz Atölyesi',
      lyricsInput: 'Metniniz',
      lyricsPlaceholder: 'Sözler, tema, anahtar kelimeler veya ruh hali yazın...',
      themeToLyrics: 'Anahtar Kelimelerden Şarkı Sözlerine',
      refineLyrics: 'Şarkı Sözlerini Yapılandır',
      resultLabel: 'Sonuç',
      resultPlaceholder: 'Sözleriniz burada görünecek...',
      expandingTheme: 'Yapay zeka anahtar kelimelerinizden şarkı sözleri yazıyor...',
      refiningLyrics: 'Yapay zeka şarkı sözlerinizi yapılandırıyor...',
      soundExplorer: 'Ses Kaşifi',
      suggestFromLyrics: 'Sözlerden Öner',
      suggestingTags: 'Yapay zeka sözlerinizi analiz ediyor...',
      mostImportant: 'en önemli',
      dimGenre: 'Tür',
      dimTimbre: 'Tını',
      dimGender: 'Ses',
      dimMood: 'Ruh Hali',
      dimInstrument: 'Enstrümanlar',
      dimScene: 'Sahne',
      dimRegion: 'Bölge (UNESCO)',
      dimTopic: 'Konu',
      audioLength: 'Ses Uzunluğu',
      generateButton: 'Müzik Oluştur',
      selectModel: 'Model',
      customTags: 'Özel Etiketler',
      customTagsPlaceholder: 'örn. acoustic,dreamy,summer_vibes'
    },
    latentLab: {
      tabs: {
        attention: 'Dikkat Kartografisi',
        probing: 'Özellik Araştırması',
        algebra: 'Kavram Cebiri',
        fusion: 'Kodlayıcı Füzyonu',
        archaeology: 'Gürültü Giderme Arkeolojisi',
        textlab: 'Latent Metin Laboratuvarı',
        crossmodal: 'Çapraz Modal Laboratuvar'
      },
      comingSoon: 'Bu araç gelecek bir sürümde hayata geçirilecek.',
      attention: {
        headerTitle: 'Dikkat Kartografisi — Hangi kelime hangi görsel bölgeyi yönlendirir?',
        headerSubtitle: 'Prompttaki her kelime için, oluşturulan görsel üzerinde bir ısı haritası katmanı, o kelimenin görüntünün hangi bölgesinde en fazla etkisi olduğunu gösterir. Bu, modelin anlamsal kavramları uzamsal olarak nasıl dağıttığını ortaya koyar.',
        explanationToggle: 'Ayrıntılı açıklamayı göster',
        explainWhatTitle: 'Bu araç ne gösteriyor?',
        explainWhatText: 'Bir difüzyon modeli görsel oluştururken promptu kelime kelime talimat seti gibi okumaz. Bunun yerine "dikkat" adı verilen bir mekanizma, her kelimenin etkisini farklı görsel bölgelere dağıtır. "Ev" kelimesi ağırlıklı olarak evin göründüğü bölgeyi etkiler — ancak aynı zamanda komşu alanları da, çünkü model tüm sahnenin bağlamını anlar. Bu araç bu dağılımı görünür kılar: bir kelimeye tıklayın ve hangi görsel bölgelerin aydınlandığını görün.',
        explainHowTitle: 'Isı haritasını nasıl okuyabilirim?',
        explainHowText: 'Parlak, yoğun renk = kelimenin o bölge üzerindeki güçlü etkisi. Koyu veya yok renk = az etki. Birden fazla kelime seçerseniz, farklı renklerde görünürler. Not: haritalar MÜKEMMEL keskin kenarlı değil — bu bir hata değil, modelin kavramları bağlamsal olarak, izolasyonda değil işlediğini gösterir. Çiftlik sahnesindeki bir "ev" aynı zamanda hayvanlar ve alanlar üzerinde biraz etkiye sahiptir, çünkü model sahneyi bir bütün olarak anlar.',
        explainReadTitle: 'İki kaydırıcı ne ortaya koyuyor?',
        explainReadText: 'Gürültü giderme adımı kaydırıcısı, 25 adımlık oluşturma sürecinde dikkatı HANGI noktada görüntülediğinizi gösterir. Erken adımlar kabataslak düzen planlamasını, geç adımlar detay atamayı gösterir. Ağ derinliği seçici, transformatörde dikkatın ölçüldüğü YERİ gösterir: sığ katmanlar (girdiye yakın) küresel kompozisyon planlamasını, orta katmanlar anlamsal atamayı, derin katmanlar ince ayarı gösterir. Her iki eksen bağımsızdır — farklı kombinasyonları sistematik olarak keşfetmeye değer.',
        techTitle: 'Teknik detaylar',
        techText: 'SD3.5, ortak dikkat ile MMDiT (Çok Modlu Difüzyon Transformatörü) kullanır: görsel ve metin tokenları 24 transformatör bloğu boyunca birbirlerine dikkat eder. Metin→görsel dikkat alt matrisini çıkarmak için 3 seçili blokta varsayılan SDPA işlemcisini manuel softmax(QK^T/√d) işlemcisiyle değiştiriyoruz. Haritalar 64x64 çözünürlüktür (yama ızgarası), iki doğrusal interpolasyon aracılığıyla görsel çözünürlüğe büyütülür. SD3.5 iki metin kodlayıcı kullanır: CLIP-L (BPE, 77 token) ve T5-XXL (SentencePiece, 512 token). Her ikisi de farklı tokenleştirme stratejilerinin dikkati nasıl etkilediğini görmek için burada değiştirilebilir.',
        promptLabel: 'Prompt',
        promptPlaceholder: 'örn. Çiftlik arazisiyle, doğa ve hayvanlarla çevrili bir manzarada bir ev duruyor. Bazı insanlar görülebilir.',
        generate: 'Oluştur + Analiz Et',
        generating: 'Görsel oluşturuluyor ve dikkat çıkarılıyor...',
        emptyHint: 'Modelin dikkat haritalarını görselleştirmek için bir prompt girin ve Oluştur\'a tıklayın.',
        advancedLabel: 'Gelişmiş Ayarlar',
        negativeLabel: 'Negatif Prompt',
        stepsLabel: 'Adımlar',
        cfgLabel: 'CFG',
        seedLabel: 'Tohum',
        tokensLabel: 'Tokenlar',
        tokensHint: 'Bir veya daha fazla kelimeye tıklayın. Alt kelime tokenları (örn. "Ku"+"gel") otomatik olarak birleştirilir. Birden fazla kelime farklı renklerde görünür.',
        timestepLabel: 'Gürültü giderme adımı',
        timestepHint: 'Difüzyon modelleri görüntüleri gürültüden görüntüye 25 adımda oluşturur. Erken adımlar kaba yapıyı oluşturur, geç adımlar detayları rafine eder. Bu kaydırıcı, modelin her adımda neye dikkat ettiğini gösterir.',
        step: 'Adım',
        layerLabel: 'Ağ derinliği',
        layerHint: 'Her gürültü giderme adımında sinyal tüm 24 transformatör katmanından geçer. Sığ katmanlar (girdiye yakın) küresel kompozisyonu, orta katmanlar anlamsal atamayı, derin katmanlar (çıktıya yakın) ince detayları yakalar. Her iki kontrol de bağımsızdır: adım = süreçte ne zaman, derinlik = ağda nerede.',
        layerEarly: 'Sığ (Kompozisyon)',
        layerMid: 'Orta (Anlambilim)',
        layerLate: 'Derin (Detay)',
        opacityLabel: 'Isı Haritası',
        opacityHint: 'Görsel üzerindeki renkli katmanın gücü.',
        baseImageLabel: 'Temel görsel',
        baseColor: 'Renkli',
        baseBW: 'S/B',
        baseOff: 'Kapalı',
        baseImageHint: 'Renkli orijinal görüntüyü gösterir. S/B ısı haritası renkleri öne çıksın diye görüntüyü renksizleştirir. Kapalı görüntüyü tamamen gizler ve yalnızca dikkat haritasını gösterir.',
        encoderLabel: 'Metin Kodlayıcı',
        encoderClipL: 'CLIP-L (77 Token)',
        encoderT5: 'T5-XXL (512 Token)',
        encoderHint: 'SD3.5 farklı tokenleştirmeli iki metin kodlayıcı kullanır. CLIP-L BPE (Byte-Pair Encoding) kullanır, T5-XXL SentencePiece kullanır. Her iki kodlayıcının aynı promptu nasıl işlediğini ve her birinin hangi görsel bölgelerini yönlendirdiğini karşılaştırın.',
        download: 'Görseli İndir'
      },
      probing: {
        headerTitle: 'Özellik Araştırması — Hangi boyutlar neyi kodlar?',
        headerSubtitle: 'İki promptu karşılaştırın ve hangi gömme boyutlarının anlamsal farkı kodladığını keşfedin. Görseli nasıl etkilediklerini görmek için tek tek boyutları seçici olarak aktarın.',
        explanationToggle: 'Ayrıntılı açıklamayı göster',
        explainWhatTitle: 'Bu araç ne gösteriyor?',
        explainWhatText: 'Her kelime, metin kodlayıcı tarafından yüksek boyutlu bir vektöre dönüştürülür (örn. T5 için 4096 boyut). Prompttaki bir kelimeyi değiştirdiğinizde — örn. "kırmızı"dan "mavi"ye — belirli boyutlar diğerlerinden daha fazla değişir. Bu araç size HANGİ boyutların en fazla değiştiğini gösterir ve Prompt B\'den bireysel boyutları Prompt A\'ya seçici olarak aktarmanıza olanak tanır.',
        explainHowTitle: 'Aktarım nasıl çalışır?',
        explainHowText: 'Çubuk grafik, fark büyüklüğüne göre sıralanmış tüm boyutları gösterir. Bir pencere seçmek için sıra aralığı kontrollerini (Başlangıç/Bitiş) kullanın — örn. yalnızca ilk 100 veya özellikle sıra 880–920. "Aktar"a tıklamak aynı ayarlarla (aynı tohum!) görüntüyü yeniden oluşturur — ancak Prompt B\'den seçili boyutlarla. Bu, o boyutların tam olarak ne "kodladığını" görmenizi sağlar.',
        explainReadTitle: 'Çubuk grafiği nasıl okuyabilirim?',
        explainReadText: 'Her çubuk bir gömme boyutunu temsil eder. Uzunluk, o boyutun Prompt A ve B arasında ne kadar farklılaştığını gösterir. Büyük farklılıklara sahip boyutlar, anlamsal değişimin en olası taşıyıcılarıdır. Ancak not: gömmeler dağıtılmıştır — görünür bir değişiklik üretmek için genellikle birden fazla boyutun birlikte gereklidir.',
        techTitle: 'Teknik detaylar',
        techText: 'SD3.5 üç metin kodlayıcı kullanır: CLIP-L (768d), CLIP-G (1280d) ve T5-XXL (4096d). Her birini ayrı ayrı araştırabilirsiniz. Fark, tüm token konumlarında ortalama mutlak sapma olarak hesaplanır: mean(abs(B-A), dim=tokens). Aktarım, seçili boyutları tüm token konumlarında aynı anda değiştirir.',
        promptALabel: 'Prompt A (Orijinal)',
        promptBLabel: 'Prompt B (Karşılaştırma)',
        promptAPlaceholder: 'örn. Gölün yanında kırmızı bir ev',
        promptBPlaceholder: 'örn. Gölün yanında mavi bir ev',
        encoderLabel: 'Kodlayıcı',
        encoderAll: 'Tümü (önerilen)',
        encoderClipL: 'CLIP-L (768d)',
        encoderClipG: 'CLIP-G (1280d)',
        encoderT5: 'T5-XXL (4096d)',
        analyzeBtn: 'Analiz Et',
        analyzing: 'Promptlar kodlanıyor ve karşılaştırılıyor...',
        transferBtn: 'Prompt B\'den seçili vektör boyutlarını oluşturulan görsele aktar',
        transferring: 'Değiştirilmiş gömme ile görsel oluşturuluyor...',
        rankFromLabel: 'Sıradan başla',
        rankToLabel: 'Sıraya kadar',
        sliderLabel: 'Prompt B\'den boyutları seçin',
        range1Label: 'Aralık 1',
        range2Label: 'Aralık 2',
        addRange: 'Aralık ekle',
        selectionDesc: 'Prompt B\'den {count} boyut seçildi (sıra {ranges} / {total})',
        listTitle: 'Prompt A\'ya göre en büyük farklılığa sahip Prompt B\'nin {count} boyutu',
        sortAsc: 'Artan',
        sortDesc: 'Azalan',
        originalLabel: 'Orijinal (Prompt A)',
        modifiedLabel: 'Değiştirilmiş (Prompt B\'den Aktarım)',
        modifiedHint: 'Aşağıdan bir sıra aralığı seçin ve "Aktar"a tıklayın — bu, B\'den aktarılan boyutlarla Prompt A\'yı gösterecek (aynı tohum).',
        noDifference: 'Gömmeler aynıdır — Prompt B\'yi değiştirin.',
        advancedLabel: 'Gelişmiş Ayarlar',
        negativeLabel: 'Negatif Prompt',
        stepsLabel: 'Adımlar',
        cfgLabel: 'CFG',
        seedLabel: 'Tohum',
        selectAll: 'Tümü',
        selectNone: 'Hiçbiri',
        downloadOriginal: 'Orijinali İndir',
        downloadModified: 'Değiştirilmişi İndir'
      },
      algebra: {
        headerTitle: 'Kavram Cebiri \u2014 Görsel gömmeler üzerinde vektör aritmetiği',
        headerSubtitle: 'Ünlü word2vec anolojisini görsel üretimine uygulayın: Kral \u2212 Adam + Kadın \u2248 Kraliçe. Üç prompt kodlanır ve cebirsel olarak birleştirilir.',
        explanationToggle: 'Ayrıntılı açıklamayı göster',
        explainWhatTitle: 'Bu araç ne gösteriyor?',
        explainWhatText: '2013\'te Mikolov, kelime gömmelerinin anlamsal ilişkileri doğrusal yönler olarak kodladığını gösterdi: "Kral" vektörü eksi "Adam" artı "Kadın" = "Kraliçe"ye yakın bir vektör verir. Bu araç, bu fikri SD3.5\'in metin kodlayıcılarına uygular: tek kelimeleri manipüle etmek yerine, tüm prompt gömmelerini manipüle edersiniz. Sonuç, kavram A\'yı içeren ancak B\'nin C ile değiştirildiği bir görseldir.',
        explainHowTitle: 'Cebir nasıl çalışır \u2014 ve neden sadece negatif prompt kullanmak yeterli değil?',
        explainHowText: 'Üç prompt girersiniz: A (temel), B (çıkar) ve C (ekle). Formül şudur: Sonuç = A \u2212 Ölçek\u2081\u00d7B + Ölçek\u2082\u00d7C. Ölçek kaydırıcıları yoğunluğu kontrol eder: 1,0\'da B tam olarak çıkarılır ve C tam olarak eklenir. 0,5\'te yalnızca yarısı. 1,0\'ın üzerindeki değerler etkiyi güçlendirir. \u2014 Neden sadece "A + C" promptu ve negatif olarak "B" kullanmıyorsunuz? Çünkü bu temelden farklı bir şey yapar: Negatif prompt, gürültü giderme sürecini 25 adımın HER BİRİNDE B\'den uzaklaştırır \u2014 model adım adım "B değil"i nasıl yorumlayacağına karar verir. Kavram Cebiri ise görsel üretimden ÖNCE yeni bir vektör hesaplar: çıkarma, difüzyon sürecinde değil gömme uzayında gerçekleşir. Sonuç, doğrudan "B-liksizlik artı C-lik olan A"yı kodlayan tek bir vektördür. Negatif prompt "bunu yapma" der. Cebir "bu kavramı çıkar ve o diğerini koy" der \u2014 adım adım bir kaçınma stratejisi yerine anlam uzayında cerrahi bir operasyon.',
        explainReadTitle: 'Sonuçlar ne anlama gelir?',
        explainReadText: 'Solda referans görsel (yalnızca Prompt A, aynı tohum) görürsünüz. Sağda cebir sonucu. Analoji işe yararsa, sağdaki görüntü kavram A\'yı ancak B\u2192C anlamsal değişimiyle göstermelidir. Örnek: "Sahilde gün batımı" \u2212 "Sahil" + "Dağlar" \u2248 "Dağların üzerinde gün batımı". L2 mesafesi, sonucun orijinalden ne kadar uzaklaştığını gösterir. \u2014 İşlem değişmeli midir? Hayır. B\'nin çıkarılması ve C\'nin eklenmesi, A vektörüne göre gerçekleşir. B\u2192C yönü yalnızca A bağlamında anlam ifade eder: "Kral \u2212 Adam" Kral vektöründen "eril" yönleri çıkarır, "+ Kadın" "dişil" yönleri ekler \u2014 sonuç "Kraliçe"ye yakın düşer. C, B\'nin kaldırıldığı yere cerrahi olarak yerleştirilmez; sadece eklenir. Bunun hâlâ işe yaraması, anlamsal ilişkilerin vektör uzayında tutarlı doğrusal yönler olarak kodlandığını gösterir.',
        techTitle: 'Teknik detaylar',
        techText: 'Cebir, seçili kodlayıcı gömmelerinde gerçekleştirilir: CLIP-L (768d), CLIP-G (1280d), T5-XXL (4096d) veya tümü birleşik (589 token \u00d7 4096d). Aynı işlem havuzlanmış gömmelere de uygulanır (2048d). Her iki görsel de adil karşılaştırma için aynı tohumu kullanır.',
        promptALabel: 'Prompt A (Temel)',
        promptAPlaceholder: 'örn. Palmiye ağaçlarıyla sahilde gün batımı',
        promptBLabel: 'Prompt B (Çıkar)',
        promptBPlaceholder: 'örn. Palmiye ağaçlarıyla sahil',
        promptCLabel: 'Prompt C (Ekle)',
        promptCPlaceholder: 'örn. Karlı dağlar',
        formulaLabel: 'A \u2212 B + C = ?',
        encoderLabel: 'Kodlayıcı',
        encoderAll: 'Tümü (önerilen)',
        encoderClipL: 'CLIP-L (768d)',
        encoderClipG: 'CLIP-G (1280d)',
        encoderT5: 'T5-XXL (4096d)',
        generateBtn: 'Hesapla',
        generating: 'Gömmeler hesaplanıyor ve görseller oluşturuluyor...',
        referenceLabel: 'Referans (Prompt A)',
        resultLabel: 'Sonuç (A \u2212 B + C)',
        l2Label: 'Orijinalden L2 mesafesi',
        advancedLabel: 'Gelişmiş Ayarlar',
        negativeLabel: 'Negatif Prompt',
        stepsLabel: 'Adımlar',
        cfgLabel: 'CFG',
        seedLabel: 'Tohum',
        scaleSubLabel: 'Çıkarma ölçeği',
        scaleAddLabel: 'Ekleme ölçeği',
        downloadReference: 'Referansı İndir',
        downloadResult: 'Sonucu İndir',
        resultHint: 'Üç prompt girin ve Hesapla\'ya tıklayın \u2014 vektör aritmetiğinin sonucu burada görünecek.'
      },
      archaeology: {
        headerTitle: 'Gürültü Giderme Arkeolojisi \u2014 Gürültü nasıl görüntüye dönüşür?',
        headerSubtitle: 'Her gürültü giderme adımını gözlemleyin. Difüzyon modelleri soldan sağa çizmez \u2014 kabataslak şekillerden ince detaylara her yerde aynı anda çalışırlar.',
        explanationToggle: 'Ayrıntılı açıklamayı göster',
        explainWhatTitle: 'Bu araç ne gösteriyor?',
        explainWhatText: 'Difüzyon modeli bir görüntü oluştururken gürültüyü kademeli olarak giderir. Soldan sağa çizmenin aksine, model TÜM görüntü bölgelerinde aynı anda çalışır. İlk adımlarda kabataslak yapılar ortaya çıkar: Yukarı nerede, aşağı nerede? Ufuk nerede? Orta adımlarda anlamsal içerik görünür: nesneler, şekiller, renkler. Son adımlar dokuları ve detayları rafine eder. Bu araç her tek adımı görünür kılar.',
        explainHowTitle: 'Bu aracı nasıl kullanırım?',
        explainHowText: 'Bir prompt girin ve Oluştur\'a tıklayın. Model 25 ara görüntü üretir (gürültü giderme adımı başına bir). Bunlar aşağıda film şeridi olarak görünür. Her adımı tam boyutta görüntülemek için bir küçük resme tıklayın veya zaman çizelgesi kaydırıcısını kullanın. Erken ve geç adımları karşılaştırın: Model ne çizdiğini ne zaman "biliyor"?',
        explainReadTitle: 'Üç aşama ne ortaya koyuyor?',
        explainReadText: 'Erken adımlar (1\u20138): Küresel kompozisyon \u2014 temel yapı, renk dağılımı, düzen planlaması. Orta adımlar (9\u201317): Anlamsal ortaya çıkış \u2014 nesneler tanınabilir hale gelir, şekiller kristalleşir. Geç adımlar (18\u201325): Detay iyileştirme \u2014 dokular, kenarlar, ince desenler. Geçişler kademeliydi ancak aşamalar açıkça gösteriyor: model önce küresel olarak "planlar", sonra yerel olarak rafine eder. Özellikle ilginç: İlk adım ince taneli pikseller değil, renkli yamalar gösterir. Bunun nedeni gürültünün piksel uzayında değil latent uzayda (16 kanallı 128\u00d7128) oluşturulmasıdır. VAE her latent pikseli ~8\u00d78 piksel yamasına çevirir \u2014 saf Gaussian gürültüsü bile tutarlı renk kümeleri haline gelir. Model asla bireysel pikseller içinde "düşünmez", her zaman bu sıkıştırılmış uzayda çalışır.',
        techTitle: 'Teknik detaylar',
        techText: 'SD3.5 Large, 25 varsayılan adımla düzeltilmiş akış zamanlayıcısı kullanır. Her adımda mevcut latent vektörler VAE aracılığıyla decode edilir (1024\u00d71024 JPEG). VAE (Varyasyonel Otokodlayıcı) matematiksel latent uzayı piksellere çevirir. Latent temsil 16 kanallı 128\u00d7128\'dir \u2014 her latent piksel görselde ~8\u00d78 piksel yamasına karşılık gelir. Bu yüzden ilk adım bile ince piksel gürültüsü yerine renkli kümeler gösterir: VAE rastgele 16 boyutlu vektörleri tutarlı renk yamaları olarak yorumlar.',
        promptLabel: 'Prompt',
        promptPlaceholder: 'örn. İnsanlarla, binalarla ve çeşmeyle orta çağ kasabasında bir pazar yeri',
        generate: 'Oluştur',
        generating: 'Görsel oluşturuluyor \u2014 her adım kaydediliyor...',
        emptyHint: 'Gürültü giderme sürecini görselleştirmek için bir prompt girin ve Oluştur\'a tıklayın.',
        advancedLabel: 'Gelişmiş Ayarlar',
        negativeLabel: 'Negatif Prompt',
        stepsLabel: 'Adımlar',
        cfgLabel: 'CFG',
        seedLabel: 'Tohum',
        filmstripLabel: 'Gürültü Giderme Film Şeridi',
        timelineLabel: 'Adım',
        phaseEarly: 'Kompozisyon',
        phaseMid: 'Anlambilim',
        phaseLate: 'Detay',
        phaseEarlyDesc: 'Küresel yapı ve renk dağılımı ortaya çıkıyor',
        phaseMidDesc: 'Nesneler ve şekiller tanınabilir hale geliyor',
        phaseLateDesc: 'Dokular ve ince detaylar keskinleştiriliyor',
        finalImageLabel: 'Son görsel (tam çözünürlük)',
        download: 'Görseli İndir'
      },
      textLab: {
        headerTitle: 'Latent Metin Laboratuvarı \u2014 Bilimsel LLM Dekonstruksiyonu',
        headerSubtitle: 'Temsil Mühendisliği, karşılaştırmalı model arkeolojisi ve sistematik önyargı analizi: Dil modellerini araştırmak için araştırmaya dayalı üç araç.',
        explanationToggle: 'Bilimsel temelleri göster',
        explainWhatTitle: 'Bu araç ne gösteriyor?',
        explainWhatText: 'Bu araç, LLM yorumlanabilirliğine yönelik üç güncel araştırma yaklaşımını hayata geçirir: Temsil Mühendisliği (Zou vd. 2023), aktivasyon uzayında kavram yönleri bulur ve model davranışını hedefli olarak manipüle eder. Karşılaştırmalı Model Arkeolojisi (Belinkov 2022, Olsson 2022), farklı modellerin aynı metni nasıl temsil ettiğini gösterir. Önyargı Arkeolojisi (Zou 2023, Bricken 2023), kontrollü token manipülasyonu aracılığıyla sistematik önyargıları ortaya çıkarır.',
        explainHowTitle: 'Bu aracı nasıl kullanırım?',
        explainHowText: 'Önce model paneli aracılığıyla bir model yükleyin (hızlı testler için küçük, temiz kavram yönleri için 3B+). Üç sekme mevcuttur: Sekme 1 (RepEng) kontrast çiftleri tanımlar ve kavram yönleri bulur. Sekme 2 (Karşılaştır) iki model yükler ve iç temsillerini karşılaştırır. Sekme 3 (Önyargı) sistematik önyargı deneyleri yürütür.',
        modelPanel: {
          title: 'Model Yönetimi',
          presetLabel: 'Hazır Ayar',
          presetNone: 'Hazır ayar yok (özel kimlik)',
          customModelLabel: 'HuggingFace Model Kimliği',
          customModelPlaceholder: 'örn. meta-llama/Llama-3.2-1B',
          quantizationLabel: 'Nicemleme',
          quantAuto: 'Otomatik',
          loadBtn: 'Yükle',
          unloadBtn: 'Kaldır',
          loading: 'Model yükleniyor...',
          statusLoaded: 'Yüklendi',
          statusNone: 'Yüklü model yok',
          vramLabel: 'VRAM',
          noModelWarning: 'Araçları kullanmak için önce bir model yükleyin.'
        },
        tabs: {
          repeng: 'Temsil Mühendisliği',
          compare: 'Model Karşılaştırması',
          bias: 'Önyargı Arkeolojisi'
        },
        repeng: {
          title: 'Temsil Mühendisliği',
          subtitle: 'Aktivasyon uzayında kavram yönleri bulun ve üretimi yönlendirin',
          guide: 'Bu deney modelden bir "doğruluk yönü" çıkarır. Önceden doldurulmuş kontrast çiftlerin her biri gerçek ve yanlış bir ifade içerir. Aktivasyonlardaki farklılıklardan sistem yüksek boyutlu uzayda bir yön hesaplar. Bu yön ters çevrildiğinde (α = -1), model olgusal bir prompt için yanlış bir yanıt oluşturmalıdır \u2014 doğru olanı "bilmesine" rağmen. Bu şunu ortaya koyar: model yalnızca bilgiyi değil, doğru ya da yanlış yanıt verme eğilimini de kodlar.',
          languageHint: 'Öneri: Açık kaynaklı LLM\'lerin çoğu (LLaMA, Mistral) öncelikli olarak İngilizce verilerle eğitildiğinden İngilizce promptlar önemli ölçüde daha iyi çalışır. Bu nedenle önceden doldurulmuş örnekler İngilizce\'dir.',
          expectedResults: 'Beklenen sonuçlar: α = 0\'da (temel), model doğru yanıtı üretir. α = -1\'de (inversiyon), yanlış bir yanıt görünmelidir \u2014 deneyin özü budur. α = +1\'de model zaten doğru yanıt verdiği için az şey değişir. |α| > 2\'nin ötesinde, eserler (tekrarlar, saçmalık) baskın olur. Zou vd.\'ne göre "tatlı nokta" |α| 0,5 ile 2,0 arasındadır. Açıklanan varyans > %50, temiz ayrımı gösterir \u2014 bunun altında, kontrast çiftler çok benzer veya çok azdır (en az 3 önerilir).',
          science: 'Zou vd. (2023) "Representation Engineering" ve Li vd. (2024) "Inference-Time Intervention" çalışmalarına dayanır. Temel fikir: LLM\'ler soyut kavramları (doğruluk, duygu, etik) yüksek boyutlu aktivasyon uzayında yönler olarak kodlar. Kontrast çiftler (doğru ve yanlış cümle) aracılığıyla, bir kavramı kodlayan yön çıkarılabilir. Bu yönün çalışma zamanında eklenmesi, yeniden eğitim olmadan model davranışını hedefli olarak değiştirir.',
          pairsTitle: 'Kontrast Çiftleri',
          pairsSubtitle: 'En az 3 çift önerilir. Her çift yalnızca hedef kavramda farklı olmalıdır (doğru ve yanlış). Örnekler düzenlenebilir.',
          positiveLabel: 'Pozitif (doğru)',
          negativeLabel: 'Negatif (yanlış)',
          positivePlaceholder: 'örn. Fransa\'nın başkenti Paris\'tir',
          negativePlaceholder: 'örn. Fransa\'nın başkenti Berlin\'dir',
          addPair: 'Çift ekle',
          removePair: 'Kaldır',
          targetLayerLabel: 'Hedef katman',
          targetLayerAuto: 'Son katman',
          findDirection: 'Yön bul',
          finding: 'Kavram yönü hesaplanıyor...',
          directionFound: 'Kavram yönü bulundu',
          varianceLabel: 'Açıklanan varyans',
          dimLabel: 'Boyutlar',
          projectionsTitle: 'Kontrast çift projeksiyanları',
          testTitle: 'Test + Manipülasyon',
          testSubtitle: 'Bir cümle girin ve üretimi kavram yönü boyunca yönlendirin',
          testPromptLabel: 'Test promptu',
          testPromptPlaceholder: 'örn. Almanya\'nın başkenti',
          alphaLabel: 'Manipülasyon gücü (α)',
          temperatureLabel: 'Sıcaklık',
          maxTokensLabel: 'Maks token',
          seedLabel: 'Tohum (-1 = rastgele)',
          generateBtn: 'Manipülasyonla oluştur',
          generating: 'Manipüle edilmiş üretim çalıştırılıyor...',
          baselineLabel: 'Temel (manipülasyon yok)',
          manipulatedLabel: 'Manipüle edilmiş (α = {alpha})',
          projectionLabel: 'Kavram yönüne projeksiyon',
          interpretationTitle: 'Yorum',
          interpreting: 'Sonuçlar analiz ediliyor...',
          interpretationError: 'Yorum oluşturulamadı'
        },
        compare: {
          title: 'Karşılaştırmalı Model Arkeolojisi',
          subtitle: 'İki model yükleyin ve iç temsillerini sistematik olarak karşılaştırın',
          science: 'Belinkov (2022) "Probing Classifiers" ve Olsson vd. (2022) "In-Context Learning and Induction Heads" çalışmalarına dayanır. Isı haritası, her iki modelin katmanları arasındaki CKA (Merkezi Kernel Hizalaması) gösterir. Yüksek benzerlik: bu katmanlar bilgiyi benzer şekilde temsil eder. Erken katmanlar (sözdizimi) genellikle benzerdir \u2014 geç katmanlar (anlambilim), özellikle farklı model boyutlarında daha güçlü biçimde ayrışır.',
          modelATitle: 'Model A (yukarıda yüklü)',
          modelAHint: 'Yukarıdaki model paneli aracılığıyla değiştirin',
          modelBTitle: 'Model B (ikinci model)',
          modelBPresetLabel: 'Hazır Ayar',
          modelBCustomLabel: 'HuggingFace Model Kimliği',
          modelBCustomPlaceholder: 'örn. TinyLlama/TinyLlama-1.1B-Chat-v1.0',
          modelBLoadBtn: 'Model B\'yi Yükle',
          modelBLoaded: 'Model B yüklendi',
          modelBNone: 'Model B yüklü değil',
          promptLabel: 'Prompt',
          promptPlaceholder: 'örn. Kedi paspasın üzerinde oturdu ve kuşları izledi',
          seedLabel: 'Tohum',
          temperatureLabel: 'Sıcaklık',
          maxTokensLabel: 'Maks token',
          compareBtn: 'Karşılaştır',
          comparing: 'Modeller karşılaştırılıyor...',
          heatmapTitle: 'Katman Hizalaması (CKA)',
          heatmapAxisA: 'Model A \u2014 Katmanlar',
          heatmapAxisB: 'Model B \u2014 Katmanlar',
          heatmapExplain: 'Parlak hücreler = yüksek temsil benzerliği. Köşegen örüntüler, modellerin bilgiyi benzer bir sırayla işlediğini gösterir.',
          attentionTitle: 'Dikkat Karşılaştırması (son katman)',
          modelALabel: 'Model A',
          modelBLabel: 'Model B',
          generationTitle: 'Üretim Karşılaştırması (aynı tohum)',
          layerStatsTitle: 'Katman İstatistikleri',
          interpretationTitle: 'Yorum',
          interpreting: 'Sonuçlar analiz ediliyor...',
          interpretationError: 'Yorum oluşturulamadı'
        },
        bias: {
          title: 'Önyargı Arkeolojisi',
          subtitle: 'Kontrollü token manipülasyonu aracılığıyla sistematik önyargı deneyleri',
          science: 'Zou vd. (2023) "Representation Engineering" ve Bricken vd. (2023) "Towards Monosemanticity" çalışmalarına dayanır. Serbest manipülasyon yerine bu araç sistematik önyargıları araştırır: Tüm eril zamirler bastırıldığında ne olur? Model varsayılan olarak hangi cinsiyeti seçer? Pozitif ve negatif kelimeler güçlendirildiğinde metin ne kadar güçlü değişir? Cevaplar, hangi önyargıların modele kodlandığını ortaya koyar.',
          presetLabel: 'Deney türü',
          presetGender: 'Cinsiyet \u2014 Cinsiyetli zamirleri bastır',
          presetSentiment: 'Duygu \u2014 Pozitif/negatifi güçlendir',
          presetDomain: 'Alan \u2014 Bilimsel/şiirsel güçlendir',
          presetCustom: 'Özel deney',
          promptLabel: 'Prompt',
          promptPlaceholder: 'örn. Doktor hastaya dedi ki',
          customBoostLabel: 'Güçlendirme tokenları (virgülle ayrılmış)',
          customBoostPlaceholder: 'örn. karanlık,gölge,gece',
          customSuppressLabel: 'Bastırma tokenları (virgülle ayrılmış)',
          customSuppressPlaceholder: 'örn. ışık,güneş,parlak',
          numSamplesLabel: 'Koşul başına örnek',
          temperatureLabel: 'Sıcaklık',
          maxTokensLabel: 'Maks token',
          seedLabel: 'Temel tohum',
          runBtn: 'Deneyi çalıştır',
          running: 'Önyargı deneyi çalıştırılıyor...',
          baselineTitle: 'Temel (manipülasyon yok)',
          groupTitle: 'Grup: {name}',
          modeSuppress: 'bastırıldı',
          modeBoost: 'güçlendirildi',
          tokensLabel: 'Tokenlar',
          sampleSeedLabel: 'Tohum',
          genderDesc: 'Tüm cinsiyetli zamirleri bastırır ve modelin hangi varsayılanları seçtiğini gözlemler.',
          sentimentDesc: 'Pozitif veya negatif kelimeleri güçlendirir ve tüm metin akışının ne kadar güçlü etkilendiğini ölçer.',
          domainDesc: 'Bilimsel veya şiirsel kelime dağarcığını güçlendirir ve kayıt değişimlerini gözlemler.',
          interpretationTitle: 'Yorum',
          interpreting: 'Sonuçlar analiz ediliyor...',
          interpretationError: 'Yorum oluşturulamadı'
        },
        error: {
          gpuUnreachable: 'GPU hizmetine ulaşılamıyor. Çalışıyor mu?',
          loadFailed: 'Model yüklenemedi.',
          operationFailed: 'İşlem başarısız oldu.'
        }
      },
      crossmodal: {
        headerTitle: 'Çapraz Modal Laboratuvar',
        headerSubtitle: 'Latent uzaylardan ses: T5 gömme manipülasyonu, görsel rehberli ses üretimi, çapraz modal aktarım',
        generate: 'Oluştur',
        generating: 'Oluşturuluyor...',
        result: 'Sonuç',
        seed: 'Tohum',
        generationTime: 'Oluşturma süresi',
        tabs: {
          synth: {
            label: 'Latent Ses Sentezci',
            short: 'T5 gömme manipülasyonu',
            title: 'Latent Ses Sentezci',
            description: 'Stable Audio\'nun T5 koşullandırma uzayının (768d) doğrudan manipülasyonu. Promptlar arasında interpolasyon, promptun ötesinde ekstrapolasyon, gömmeleri ölçeklendirme ve gürültü enjeksiyonu. Ultra kısa döngüler, gerçek zamana yakın.'
          },
          mmaudio: {
            label: 'MMAudio',
            short: 'Görsel/metinden ses (CVPR 2025)',
            title: 'MMAudio — Video/Görselden Ses',
            description: 'MMAudio (CVPR 2025), her iki modalite üzerinde eğitilmiş özel bir çapraz modal modeldir. 157M parametre, ~1,2s\'de 8s ses üretir. Naif özellik projeksiyonu yerine gerçek çapraz modal aktarım.'
          },
          guidance: {
            label: 'ImageBind Rehberliği',
            short: 'Gradyan tabanlı görsel rehberlik',
            title: 'ImageBind Gradyan Rehberliği',
            description: 'Stable Audio gürültü giderme süreci sırasında gradyan tabanlı yönlendirme. ImageBind görsel ve ses için paylaşılan 1024d uzayı sağlar \u2014 kosinüs benzerliğinin gradyanı ses üretimini görsel gömmeye doğru yönlendirir.'
          }
        },
        synth: {
          promptA: 'Prompt A (Temel)',
          promptAPlaceholder: 'örn. okyanus dalgaları',
          promptB: 'Prompt B (İsteğe bağlı, interpolasyon için)',
          promptBPlaceholder: 'örn. piyano melodisi',
          alpha: 'Alpha (İnterpolasyon)',
          alphaHint: '0 = yalnızca A, 1 = yalnızca B, arası = karışım, >1 veya <0 = ekstrapolasyon',
          magnitude: 'Büyüklük (Ölçekleme)',
          magnitudeHint: 'Küresel gömme ölçekleme (1,0 = değişmez)',
          noise: 'Gürültü',
          noiseHint: 'Gömme üzerinde Gaussian gürültüsü (0 = gürültü yok)',
          duration: 'Süre (s)',
          steps: 'Adımlar',
          cfg: 'CFG',
          loop: 'Döngü oynatma',
          loopOn: 'Döngü Açık',
          loopOff: 'Döngü Kapalı',
          stop: 'Durdur',
          looping: 'Döngüde',
          playing: 'Oynatılıyor',
          stopped: 'Durduruldu',
          transpose: 'Transpozisyon (yarım tonlar)',
          midiSection: 'MIDI Kontrolü',
          midiUnsupported: 'Web MIDI bu tarayıcı tarafından desteklenmiyor.',
          midiInput: 'MIDI Girişi',
          midiNone: '(yok)',
          midiMappings: 'CC Eşlemeleri',
          midiNoteC3: 'Nota (C3 = Ref)',
          midiGenerate: 'Oluştur + Transpose',
          midiPitch: 'C3\'e göre perde',
          loopInterval: 'Döngü aralığı',
          loopOptimize: 'Otomatik optimize',
          loopPingPong: 'Ping-pong',
          loopIntervalHint: 'Döngü bölgesinin başlangıç/sonu — Stable Audio solmasını kesmek için sonu kısaltın',
          modeLoop: 'Döngü',
          modePingPong: 'Ping-Pong',
          modeWavetable: 'Dalga Tablosu',
          modeRate: 'Tempo (hızlı)',
          modePitch: 'Perde (OLA)',
          wavetableScan: 'Tarama Konumu',
          wavetableScanHint: 'Kareler arasında morph (0 = başlangıç, 1 = bitiş)',
          wavetableFrames: '{count} kare',
          midiScan: 'Tarama Konumu',
          adsrTitle: 'ADSR Zarfı',
          adsrAttack: 'A',
          adsrDecay: 'D',
          adsrSustain: 'S',
          adsrRelease: 'R',
          adsrHint: 'MIDI notaları için zarf (Atış/Azalma/Tutma/Salıverme)',
          play: 'Oynat',
          normalize: 'Ses seviyesini normalleştir',
          peak: 'Zirve',
          crossfade: 'Geçiş',
          saveRaw: 'Ham kaydet',
          saveLoop: 'Döngü kaydet',
          embeddingStats: 'Gömme istatistikleri',
          dimensions: {
            section: 'Boyut Gezgini',
            hint: 'Çubuklara sürükle = ofset ayarla. Yatay olarak boya = birden fazla boyut.',
            resetAll: 'Tümünü sıfırla',
            hoverActivation: 'Aktivasyon',
            hoverOffset: 'Ofset',
            rightClickReset: 'Sağ tıkla = sıfırla',
            sortDiff: 'Prompt farkına göre sıralandı',
            sortMagnitude: 'Aktivasyona göre sıralandı',
            activeOffsets: '{count} ofset aktif',
            applyAndGenerate: 'Uygula ve yeniden oluştur',
            undo: 'Geri Al',
            redo: 'Yinele'
          }
        },
        mmaudio: {
          imageUpload: 'Görsel yükle',
          prompt: 'Metin promptu',
          promptPlaceholder: 'örn. çatırdayan kamp ateşi',
          negativePrompt: 'Negatif prompt',
          duration: 'Süre (s)',
          maxDuration: 'Maks 8s (model sınırı)',
          cfg: 'CFG',
          steps: 'Adımlar',
          compareHint: 'Karşılaştır: Yalnızca metin - Görsel + Metin'
        },
        guidance: {
          imageUpload: 'Görsel yükle',
          prompt: 'Temel prompt (isteğe bağlı)',
          promptPlaceholder: 'örn. ortam ses manzarası',
          lambda: 'Rehberlik gücü',
          lambdaHint: 'Görselin ses üretimini ne kadar güçlü yönlendirdiği',
          warmupSteps: 'Isınma adımları',
          warmupHint: 'Gradyan rehberliği yalnızca ilk N adım boyunca',
          totalSteps: 'Toplam adımlar',
          duration: 'Süre (s)',
          cfg: 'CFG',
          cosineSimilarity: 'Kosinüs benzerliği (görsel-ses yakınlığı)'
        }
      }
    },
    edutainment: {
      ui: {
        didYouKnow: '🤔 Biliyor muydunuz?',
        learnMore: '📚 Daha fazla bilgi',
        currentlyHappening: '⚡ Şu anda oluyor:',
        energyUsed: 'Kullanılan enerji',
        co2Produced: 'Üretilen CO₂'
      },
      energy: {
        kids_1: '💡 Yapay zeka görselleri elektrik harcıyor – telefonunuzu 3 saat şarj etmek kadar!',
        kids_2: '🔌 GPU, çok güç tüketen bir süper hesap makinesi gibidir!',
        kids_3: '⚡ Her görsel, bir LED ışığı 10 dakika yakmak için yeterli enerji gerektiriyor!',
        youth_1: '⚡ GPU oluştururken {watts}W kullanıyor – küçük bir elektrik ısıtıcısı gibi!',
        youth_2: '🔋 Bir görsel yaklaşık 0,01-0,02 kWh kullanıyor – az gibi görünüyor ama birikir!',
        youth_3: '🌡️ GPU şu an {temp}°C sıcaklığa ulaşıyor – bu yüzden soğutma gerekiyor!',
        expert_1: '📊 Anlık: {watts}W, %{util} kullanımda = şimdiye kadar {kwh} kWh',
        expert_2: '🔥 TDP sınırı: {tdp}W | Şu anki: {watts}W (sınırın %{percent}\'i)',
        expert_3: '💾 VRAM: {used}/{total} GB (%{percent}) – model + aktivasyonlar'
      },
      data: {
        kids_1: '🧮 GPU şu an 10 milyar kez hesaplıyor – sizin sayabileceğinizden daha hızlı!',
        kids_2: '🎨 Görsel 50 küçük adımda oluşturuluyor – kendi kendini çözen bir bulmaca gibi!',
        kids_3: '🧩 Şu an milyonlarca sayı GPU\'dan geçiyor!',
        youth_1: '🔄 Her görsel ~50 "gürültü giderme adımı"ndan geçiyor – gürültü gidermenin 50 turu!',
        youth_2: '📐 8 milyar parametre sorgulanıyor – görsel başına!',
        youth_3: '🧠 Yapay zeka binlerce boyutlu vektörler içinde "düşünüyor" – bir uzaydaki koordinatlar gibi.',
        expert_1: '🔬 MMDiT: Çok Modlu Difüzyon Transformatörü – birleşik dikkat katmanlarında metin + görsel',
        expert_2: '📈 Self-Attention: O(n²) karmaşıklığı – her token diğerlerini "görüyor"',
        expert_3: '⚙️ Classifier-Free Guidance: prompt etkisi ve yaratıcılık dengesi'
      },
      model: {
        kids_1: '🎓 Yapay zeka modeli nasıl resim yapacağını öğrenmek için milyonlarca görsele baktı!',
        kids_2: '🤖 Yapay zeka, gördüklerini asla unutmayan bir sanatçı gibidir!',
        kids_3: '✨ Modelde 8 milyar bağlantı var – gökyüzünde görebileceğiniz yıldızlardan daha fazla!',
        youth_1: '🧠 SD3.5 Large\'ın 8 milyar parametresi var – 8 milyar karar düğümü gibi.',
        youth_2: '📚 3 metin kodlayıcı birlikte çalışıyor: CLIP-L, CLIP-G ve T5-XXL',
        youth_3: '🔢 Model yalnızca yüklenmek için {vram} GB VRAM\'a ihtiyaç duyuyor!',
        expert_1: '🏗️ Mimari: 38 transformatör bloğuyla Rectified Flow + MMDiT',
        expert_2: '📊 FP16/FP8 nicemleme: hassasiyet ile VRAM dengesi',
        expert_3: '🔗 LoRA: Düşük Sıralı Uyarlama – yalnızca parametrelerin %0,1\'i yeniden eğitildi'
      },
      ethics: {
        kids_1: '🌍 Yapay zeka internetteki görsellerden öğrenir – bu yüzden diğer insanların sanatına karşı adil olmak önemlidir!',
        kids_2: '⚖️ Tüm sanatçılara yapay zekanın onlardan öğrenip öğrenemeyeceği sorulmadı.',
        kids_3: '🤝 İyi yapay zeka insanların çalışmalarına saygı gösterir!',
        youth_1: '📜 Eğitim verileri genellikle internetten gelir. Sanatçılar tartışıyor: Adil Kullanım mı yoksa kopyalama mı?',
        youth_2: '🏛️ AB Yapay Zeka Yasası şeffaflık talep eder: Eğitim verileri nereden geliyor?',
        youth_3: '💭 Soru: Yapay zeka tarafından oluşturulan bir görsele aslında kim sahip olur?',
        expert_1: '⚠️ LAION-5B kısmen yaratıcı rızası olmadan oluşturuldu – hukuki gri alan.',
        expert_2: '📋 AB Yapay Zeka Yasası Md. 52: Yapay zeka tarafından oluşturulan içerik için etiketleme zorunluluğu',
        expert_3: '🔍 Model Kartları & Veri Sayfaları: ML şeffaflığı için en iyi uygulama'
      },
      environment: {
        kids_1: '☁️ Her yapay zeka görseli biraz CO₂ üretir – arabayla sürüş gibi ama daha az!',
        kids_2: '🌱 Düşün: Bu görsel elektriğe değer mi?',
        kids_3: '🌞 Yapay zeka enerjisi genellikle güç santrallerinden gelir – bir kısmı temiz, bir kısmı değil.',
        youth_1: '🏭 Alman elektrik şebekesi: kWh başına ~400g CO₂ – bu birikir!',
        youth_2: '📈 Bu görsel için {co2}g CO₂ – 1000 görselde bu {totalKg} kg olur!',
        youth_3: '💡 İpucu: Daha az görsel oluşturun ama daha düşünceli – enerji ve CO₂ tasarrufu sağlar.',
        expert_1: '📊 Hesaplama: {watts}W × {seconds}s ÷ 3600 × 400g/kWh = {co2}g CO₂',
        expert_2: '🔬 Kapsam 2 emisyonları: veri merkezi konumu belirleyicidir',
        expert_3: '⚡ PUE (Güç Kullanım Etkinliği): Soğutma için ek enerji yükü'
      },
      iceberg: {
        drawPrompt: 'Yapay zeka üretimi çok fazla enerji kullanır. Buzdağları çizin ve ne olduğunu görün...',
        redraw: 'Yeniden Çiz',
        startMelting: 'Erimeyi başlat',
        melting: 'Buzdağı eriyor...',
        melted: 'Eridi!',
        meltedMessage: '{co2}g CO₂ üretildi',
        comparison: 'Bu CO₂ miktarı yaklaşık {volume} cm³ Arktik buzunu eritir.',
        comparisonInfo: '(Her ton CO₂ ≈ 6m³ deniz buzu kaybı)',
        gpuPower: 'Ekran kartı güç tüketimi',
        gpuTemp: 'Ekran kartı sıcaklığı',
        co2Info: 'Güç tüketiminden CO₂ emisyonları (Alman enerji karışımına göre)',
        drawAgain: 'Daha fazla buzdağı çizin...'
      },
      pixel: {
        grafikkarte: 'Ekran Kartı',
        energieverbrauch: 'Enerji Kullanımı',
        co2Menge: 'CO₂ Miktarı',
        smartphoneComparison: 'Bu CO₂ kullanımını telafi etmek için telefonunuzu {minutes} dakika kapalı tutmanız gerekirdi!',
        clickToProcess: 'Mini görsel oluşturmak için veri piksellerine tıklayın!'
      },
      forest: {
        trees: 'Ağaçlar',
        clickToPlant: 'Ağaç dikmek için tıklayın! Ağaç diktiğiniz yerde fabrika kaybolacak.',
        gameOver: 'Orman kayboldu!',
        treesPlanted: '{count} ağaç diktiniz.',
        complete: 'Oluşturma tamamlandı',
        comparison: 'Ortalama bir ağacın bu CO₂ miktarını absorbe etmesi {minutes} dakika gerektirir.'
      },
      rareearth: {
        clickToClean: 'Zehirli çamuru temizlemek için gölü tıklayın!',
        sludgeRemoved: 'Çamur temizlendi',
        environmentHealth: 'Çevre',
        gameOverInactive: 'Vazgeçtiniz... madencilik devam ediyor',
        infoBanner: 'GPU çipleri için nadir toprak madenciliği zehirli çamur bırakır ve ekosistemleri yok eder. Temizleme çabalarınız çıkarım hızına yetişemez.',
        instructionsCooldown: '⏳ {seconds}s',
        statsGpu: 'GPU',
        statsHealth: 'Çevre',
        statsSludge: 'Çamur temizlendi'
      }
    }

  }
}

// Cast locale to SupportedLanguage so vue-i18n accepts all supported languages
// (tr messages are intentionally partial — fallbackLocale handles gaps)
export default createI18n({
  legacy: false,
  locale: 'de' as SupportedLanguage,
  fallbackLocale: 'en',
  messages
})
