# DevServer Architecture
**AI4ArtsEd Development Server - Complete Technical Reference**

> **Last Updated:** 2025-11-09
> **Status:** AUTHORITATIVE - 4-Stage Orchestration Architecture Documented (v2.0.0-alpha.1)
> **Version:** 3.0 (4-Stage Flow + Components Reference - Fully Consolidated)

---
THIS IS A MULTI-PART-DOCUMENTATION:
Pattern: ARCHITECTURE PART XX - SUBJECTMATTER.md

Use it accordingly.

THIS FILE IS ABOUT THE 4-STAGE-ORCHESTRATION.

---

## ğŸ—ï¸ MANDATORY: File Structure Rules

**âš ï¸ CRITICAL: NEVER create new directories without consulting these rules.**

### Root Level (`/ai4artsed_webserver/`)

**ONLY these directories are allowed at project root:**
- `server/` - âš ï¸ LEGACY - DO NOT TOUCH
- `public/` - âœ… Vue-based frontend (new architecture)
- `docs/` - âœ… Project documentation (authoritative architecture docs)
- `devserver/` - âœ… NEW ARCHITECTURE (work here)
- `exports/` - âœ… Pipeline run storage (`/json/`, `/media/`)
- `workflows/` - âœ… ComfyUI workflows (if applicable)

### DevServer Level (`/devserver/`)

**ONLY these directories are allowed at devserver root:**
- **Core files:** `server.py`, `config.py`, `CLAUDE.md`
- `schemas/` - Pipeline system (chunks, pipelines, configs, engine)
- `my_app/` - Application code (routes, services)
- `tests/` - Test files
- `archive/` - Deprecated code (DO NOT EDIT)

**âŒ FORBIDDEN at devserver root:**
- New service modules â†’ Put in `/devserver/my_app/services/`
- New route modules â†’ Put in `/devserver/my_app/routes/`
- Documentation â†’ Use `/docs/` (project root)
- Frontend projects â†’ Use `/public/` (project root)

### Documentation Structure

**ALL documentation MUST go in:** `/docs/` (project root)

**Current structure:**
- `ARCHITECTURE PART XX - *.md` - Technical reference (AUTHORITATIVE)
- `DEVELOPMENT_DECISIONS.md` - Decision history
- `DEVELOPMENT_LOG.md` - Session tracking and costs
- `README_FIRST.md` - Mandatory reading for new sessions (if exists)
- `devserver_todos.md` - Task tracking

**âŒ FORBIDDEN:**
- `/devserver/docs/` - Session-specific docs must go in `/docs/`
- Documentation in random locations

### Service Module Location

**ALL service modules MUST go in:** `/devserver/my_app/services/`

**Examples:**
- âœ… `/devserver/my_app/services/ollama_service.py`
- âœ… `/devserver/my_app/services/comfyui_service.py`
- âœ… `/devserver/my_app/services/media_storage.py`
- âœ… `/devserver/my_app/services/pipeline_recorder/` (if directory)
- âŒ `/devserver/pipeline_recorder/` - WRONG LOCATION

### Frontend Location

**Active frontend:** `/public/` (project root)

**Current status:**
- âœ… `/public/` - Vue-based frontend (new architecture)
- âŒ `/devserver/public_dev/` - DEPRECATED (do not use)

**Server configuration:** `config.py` â†’ `PUBLIC_DIR = Path(__file__).parent.parent / "public"`

### Why These Rules?

1. **Consistency:** All sessions follow same structure
2. **Clarity:** No guessing where files belong
3. **Maintainability:** Easy to find and modify code
4. **Documentation:** Single source of truth in `/docs/`
5. **Architecture Transparency:** Structure reflects design decisions

**If you need to create new directories:** Ask the user first.

---

# PART I: ORCHESTRATION

---

## 1. 4-Stage Orchestration Flow

**â­ AUTHORITATIVE SECTION - Read this first before implementing any flow logic**

**Version:** 2.2 (2025-11-23 - Session 65: Stage 2 Split into 2-Phase Execution)
**Source:** Consolidated from 4_STAGE_ARCHITECTURE.md

### 1.1 Executive Summary

**DevServer orchestrates a 4-stage flow where:**
- **Stage 1** runs once per user input (based on input type, not pipeline declaration)
- **Stage 2** executes the main pipeline (can be complex: loops, branches, multiple outputs)
- **Stage 3-4** run once PER OUTPUT REQUEST from Stage 2 (not once per pipeline)

**Key Principle:** Pipelines are DUMB (declare inputs/outputs), DevServer is SMART (knows safety rules).

### 1.2 Core Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DevServer (schema_pipeline_routes.py)                          â”‚
â”‚ ROLE: Smart Orchestrator - Knows 4-Stage Flow & Safety Rules   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚ STAGE 1: Pre-Interception (Safety Only - Original Language)    â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚   DevServer reads: pipeline.input_requirements                 â”‚
â”‚   - texts: N  â†’ Run text_safety for each (NO translation!)    â”‚
â”‚   - images: M â†’ Run image_safety for each                      â”‚
â”‚   - Bilingual: Works in DE or EN (Â§86a filters both)          â”‚
â”‚                                                                 â”‚
â”‚   Example: {"texts": 2, "images": 1}                           â”‚
â”‚   â†’ text_safety(text1_in_original_language)                   â”‚
â”‚   â†’ text_safety(text2_in_original_language)                   â”‚
â”‚   â†’ image_safety(image1)                                       â”‚
â”‚   Text stays in original language (DE/EN) for Stage 2!        â”‚
â”‚                                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚ STAGE 2: Interception + Optimization (Main Pipeline)           â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚   **2-PHASE EXECUTION** (Session 65, 2025-11-23):             â”‚
â”‚                                                                 â”‚
â”‚   Phase 1: INTERCEPTION (Pedagogical Transformation)          â”‚
â”‚     execute_stage2_with_optimization() â†’ Call 1               â”‚
â”‚     - Uses: config.context (pÃ¤dagogischer Context)            â”‚
â”‚     - Model: STAGE2_INTERCEPTION_MODEL (from config.py)       â”‚
â”‚     - Output: interception_result (editierbar)                â”‚
â”‚                                                                 â”‚
â”‚   Phase 2: OPTIMIZATION (Model-specific Refinement)           â”‚
â”‚     execute_stage2_with_optimization() â†’ Call 2               â”‚
â”‚     - Uses: output_config.optimization_instruction            â”‚
â”‚     - Model: STAGE2_INTERCEPTION_MODEL (from config.py)       â”‚
â”‚     - Input: interception_result (from Phase 1)               â”‚
â”‚     - Output: optimized_prompt (editierbar)                   â”‚
â”‚                                                                 â”‚
â”‚   Why Split?                                                   â”‚
â”‚   - LLM was overwhelmed by dual task (pedagogy + optimization)â”‚
â”‚   - User needs editability BETWEEN phases                     â”‚
â”‚   - Model selection happens BEFORE optimization               â”‚
â”‚                                                                 â”‚
â”‚   PipelineExecutor.execute_pipeline(config, inputs)            â”‚
â”‚   - DUMB: Just executes chunks                                 â”‚
â”‚   - NO pre-processing, NO safety checks, NO translation       â”‚
â”‚   - CAN: loop, branch, request multiple outputs               â”‚
â”‚                                                                 â”‚

#### Media-Specific Optimization (Config Override Pattern)

**Feature:** Extend Stage 2 context with output-specific optimization instructions

**Use Case:** SD3.5 Large uses Dual CLIP architecture (clip_g + t5xxl) requiring specialized prompt optimization

**Implementation Pattern:**
1. Output config declares `OUTPUT_CHUNK` parameter pointing to chunk name
2. DevServer loads chunk JSON directly from filesystem
3. Extract `meta.optimization_instruction` from output chunk
4. Use `dataclasses.replace()` to create modified config with extended context
5. Pass `config_override` to pipeline executor

**Code Example:**
```python
from dataclasses import replace

# Load optimization instruction from output chunk metadata
optimization_instruction = output_chunk['meta'].get('optimization_instruction')

if optimization_instruction:
    # Get original context
    original_context = config.context if hasattr(config, 'context') else ""
    new_context = original_context + "\n\n" + optimization_instruction

    # Create modified config using dataclasses.replace()
    stage2_config = replace(
        config,
        context=new_context,
        meta={**config.meta, 'optimization_added': True}
    )

    # Execute pipeline with overridden config
    result = await pipeline_executor.execute_pipeline(
        config_name=schema_name,
        input_text=input_text,
        config_override=stage2_config
    )
```

**Why This Pattern:**
- âœ… Single LLM call combines interception + optimization (pedagogical constraint)
- âœ… Optimization instruction stored in output chunk where model config lives
- âœ… Fetched at runtime based on selected output config
- âœ… Non-invasive: Original config unchanged, override applied dynamically

**Critical Implementation Notes:**
- âš ï¸ MUST use `dataclasses.replace()`, NOT `Config.from_dict()` (doesn't exist)
- âš ï¸ MUST load chunks directly from filesystem, NOT via `ConfigLoader.get_chunk()` (doesn't exist)
- âš ï¸ MUST use `await` for pipeline execution, NOT `asyncio.run()` (can't nest event loops)

**Bug History:** Session 64 Part 3 (2025-11-22) - Fixed 3 critical implementation bugs that prevented this feature from working

â”‚   Pipeline returns: PipelineResult {                           â”‚
â”‚     final_output: "transformed + optimized text (orig lang)",  â”‚
â”‚     output_requests: [                                         â”‚
â”‚       {type: "image", prompt: "...", params: {...}},          â”‚
â”‚       {type: "audio", prompt: "...", params: {...}}           â”‚
â”‚     ]                                                          â”‚
â”‚   }                                                            â”‚
â”‚                                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚ STAGE 3-4: For EACH output_request from Stage 2                â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚   FOR EACH request in pipeline_result.output_requests:         â”‚
â”‚                                                                 â”‚
â”‚     STAGE 3a: Translation (if needed)                          â”‚
â”‚       - IF source_language != 'en': Translate DE â†’ EN         â”‚
â”‚       - Media generation requires English prompts             â”‚
â”‚       - User can edit BEFORE this stage (in original lang)    â”‚
â”‚                                                                 â”‚
â”‚     STAGE 3b: Pre-Output Safety                                â”‚
â”‚       - Hybrid: Fast string-match â†’ LLM if needed             â”‚
â”‚       - Check: translated_prompt against safety_level         â”‚
â”‚       - If blocked: Skip Stage 4, return text alternative     â”‚
â”‚                                                                 â”‚
â”‚     STAGE 4: Media Generation                                  â”‚
â”‚       - Execute output config (e.g., gpt5_image)              â”‚
â”‚       - Generate media with English prompt                     â”‚
â”‚       - Return media reference (prompt_id, URL, etc.)         â”‚
â”‚                                                                 â”‚
â”‚   Collect all generated media + metadata                       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 Separation of Concerns

#### What Pipelines Declare (DUMB)

**Pipeline configs declare ONLY input/output structure:**

```json
{
  "name": "complex_interception",
  "input_requirements": {
    "texts": 2,           // "I need 2 text inputs"
    "images": 1           // "I need 1 image input"
  },
  "chunks": ["manipulate", "translate", ...],
  "control_flow": "iterative",
  "meta": {
    "max_iterations": 8,
    "supports_multiple_outputs": true
  }
}
```

**Pipelines DO NOT declare:**
- âŒ Safety requirements (DevServer knows this)
- âŒ Translation needs (DevServer knows: text â†’ translate)
- âŒ Pre-processing steps (DevServer orchestrates Stage 1)
- âŒ When to run safety checks (DevServer orchestrates Stage 3)

#### What DevServer Knows (SMART)

**DevServer has hardcoded safety rules:**

```python
# schema_pipeline_routes.py

STAGE1_RULES = {
    "text": ["translation", "text_safety_stage1"],
    "image": ["image_safety_stage1"],
    "audio": ["audio_safety_stage1"]  # Future
}

STAGE3_RULES = {
    "image": "text_safety_check_{safety_level}",  # kids/youth/off
    "audio": "audio_safety_check_{safety_level}",  # Future
    "video": "video_safety_check_{safety_level}"   # Future
}
```

**Why non-redundant?**
- If pipeline says `"texts": 2`, DevServer runs Stage 1 safety for BOTH texts
- No duplication in pipeline configs
- Change safety rules in ONE place (DevServer)
- Prevents inconsistencies

### 1.4 Stage-by-Stage Flow

[See above, 1.2 Core Architecture and IMPLEMENTATION_PLAN_4_STAGE_REFACTORING.md]

### 1.5 Complex Pipeline Examples

#### Example 1: Simple Text Transformation + Image

**User Input:** "EIne Blume auf der Wiese"
**Config:** overdrive.json
**Execution Mode:** fast
**Safety Level:** kids

**Flow:**
```
â”Œâ”€ STAGE 1 (Run ONCE) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input: "EIne Blume auf der Wiese"                            â”‚
â”‚ â†’ Translation: "One flower on the meadow"                    â”‚
â”‚ â†’ Stage 1 Safety: PASSED (fast-path, no unsafe terms)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ STAGE 2 (Main Pipeline) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pipeline: overdrive (text_transformation)                    â”‚
â”‚ Input: "One flower on the meadow"                            â”‚
â”‚ â†’ manipulate chunk with overdrive context                    â”‚
â”‚ Output: "In the vast, undulating sea of emerald..."         â”‚
â”‚                                                              â”‚
â”‚ Output Requests: [                                           â”‚
â”‚   {type: "image", prompt: "In the vast, undulating..."}     â”‚
â”‚ ]                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ STAGE 3-4 (Run ONCE per output request) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Request #1: image                                            â”‚
â”‚                                                              â”‚
â”‚ STAGE 3: Pre-Output Safety                                  â”‚
â”‚   Prompt: "In the vast, undulating..."                      â”‚
â”‚   â†’ Fast-path check: No unsafe terms â†’ PASSED (0.1ms)       â”‚
â”‚                                                              â”‚
â”‚ STAGE 4: Media Generation                                   â”‚
â”‚   Lookup: image/fast â†’ gpt5_image                           â”‚
â”‚   â†’ execute_pipeline('gpt5_image', prompt) [NO STAGE 1-3!]  â”‚
â”‚   â†’ Returns: prompt_id "abc123"                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Current Bug (What Happens Now):**
```
âœ… Stage 1 runs once â†’ Good
âœ… Stage 2 runs once â†’ Good
âœ… Stage 3 runs once â†’ Good
âŒ Stage 4 calls execute_pipeline('gpt5_image', ...)
   â†’ execute_pipeline() runs Stage 1-3 AGAIN! â†’ BAD
   â†’ Translation runs on already-English text
   â†’ Safety runs twice
   â†’ Wasted time + API calls
```

#### Example 3: Multi-Output (Model Comparison)

**Scenario:** Generate same prompt with multiple models for comparison

**Config:** `image_comparison.json`
```json
{
  "pipeline": "text_transformation",
  "context": "Pass through unchanged",
  "media_preferences": {
    "output_configs": ["sd35_large", "gpt5_image"]
  }
}
```

**Flow:**
```
Input: "Eine Blume auf der Wiese"
  â†“
Stage 1: translation + text_safety (once)
  â†’ "A flower on the meadow" + PASSED
  â†“
Stage 2: text_transformation (once)
  â†’ "A flower on the meadow" (pass-through)
  â†“
Stage 3-4 Loop: FOR EACH output_config
  â”œâ”€ Iteration 1: sd35_large
  â”‚  â”œâ”€ Stage 3: Pre-Output Safety âœ…
  â”‚  â””â”€ Stage 4: ComfyUI workflow â†’ image_1.png
  â””â”€ Iteration 2: gpt5_image
     â”œâ”€ Stage 3: Pre-Output Safety âœ…
     â””â”€ Stage 4: OpenRouter API â†’ image_2.png

Response: {
  "media_outputs": [
    {"config": "sd35_large", "output": "prompt_id_1"},
    {"config": "gpt5_image", "output": "base64_data"}
  ]
}
```

**Key Points:**
âœ… Stage 1 runs once (not 2x) - no redundant translation
âœ… Stage 2 runs once (not 2x) - no redundant pipeline
âœ… Stage 3-4 loop per output - each gets independent safety check
âœ… Efficient: Only outputs require duplication, not inputs

**Use Cases:**
- Model comparison (SD3.5 vs GPT-5)
- Multi-format output (image + audio)
- Multi-resolution output (1024px + 2048px)

**Implementation:** See `schema_pipeline_routes.py` Stage 3-4 Loop

#### Example 4: Recursive Pipeline (Stille Post)

**Scenario:** 8-iteration translation loop (Chinese Whispers)

**Config:** `stillepost.json`
```json
{
  "pipeline": "text_transformation_recursive",
  "parameters": {
    "iterations": 8,
    "use_random_languages": true,
    "final_language": "en"
  }
}
```

**Flow:**
```
Input: "Eine Blume auf der Wiese"
  â†“
Stage 1: translation + text_safety (once)
  â†’ "A flower on the meadow" + PASSED
  â†“
Stage 2: text_transformation_recursive (once, loops internally)
  Iteration 1: translate to Hindi
  Iteration 2: translate to Polish
  ...
  Iteration 8: translate to English
  â†’ "The Dutch translation of..." (mangled text)
  â†“
Stage 3: Pre-Output Safety âœ…
  â†“
Stage 4: Media Generation â†’ image.png
```

**Key Points:**
âœ… Stage 1 runs once (not 8x) - Critical test PASSED
âœ… Loop runs INSIDE Stage 2 pipeline
âœ… Config controls loop behavior (iterations, languages, final_language)
âŒ Does NOT call execute_pipeline() recursively (would trigger Stage 1-3 redundancy)

**Pedagogical Goal:**
- Students see prompt degradation over iterations
- "Stille Post" (Chinese Whispers) workflow
- User-editable configs for different iteration counts

**Implementation:** See `pipeline_executor.py` _execute_recursive_pipeline_steps()

### 1.6 Implementation Status

**Current (v2.0.0-alpha.1 - 2025-11-09):**
- âœ… Stage 1-3 logic in `schema_pipeline_routes.py` - CORRECT (Session 9)
- âœ… PipelineExecutor is DUMB engine - CORRECT (Session 9)
- âœ… Non-redundant safety rules - IMPLEMENTED (Session 9)
- âœ… Recursive Pipeline System - IMPLEMENTED (Session 11 Part 1)
- âœ… Multi-Output Support - IMPLEMENTED (Session 11 Part 2)
- âœ… LivePipelineRecorder as single source of truth - IMPLEMENTED (Session 37)
- âœ… stage4_only feature for fast regeneration - FIXED (Session 39)
- âœ… Stage 2 split into 2-phase execution - IMPLEMENTED (Session 65)
- âœ… execution_mode parameter REMOVED - REFACTORED (Session 65)
- âœ… Centralized model selection via config.py - IMPLEMENTED (Session 65)

**Validation Tests:**
- âœ… Stillepost (8 iterations): Stage 1 ran once (not 8x) - PASSED
- âœ… Image Comparison (2 outputs): Stage 1 ran once (not 2x) - PASSED
- âœ… Simple config (dada): Stage 1-4 all ran once - PASSED
- âœ… Logs confirm clean execution (no redundancy) - PASSED
- âœ… media_type UnboundLocalError fixed (Session 39) - PASSED

**Architecture Proven Correct:**
- DevServer = Smart Orchestrator âœ…
- PipelineExecutor = Dumb Engine âœ…
- Non-Redundant Safety Rules âœ…
- Scalable to Complex Flows âœ…
- LivePipelineRecorder = Single Source of Truth âœ…

**See:**
- `docs/DEVELOPMENT_DECISIONS.md` for design rationale
- `docs/DEVELOPMENT_LOG.md` (Sessions 9, 11, 37, 39, 65) for implementation details
- `schema_pipeline_routes.py` for orchestration code
- `pipeline_executor.py` for execution engine

---

## 2. Model Selection Architecture (Session 65 Refactoring)

**â­ AUTHORITATIVE SECTION - Centralized model configuration**

**Version:** 1.0 (2025-11-23 - Session 65: execution_mode removal)

### 2.1 Executive Summary

**DEPRECATED Architecture (Pre-Session 65):**
- execution_mode parameter ('eco', 'fast', 'local', 'remote') passed in API calls
- Model selection logic scattered across backend/frontend
- Hardcoded values in multiple locations
- Code duplication and inconsistency

**NEW Architecture (Session 65+):**
- **execution_mode parameter COMPLETELY REMOVED**
- All models configured CENTRALLY in `devserver/config.py`
- NO hardcoded model names in Vue components or API routes
- Single source of truth for model selection

### 2.2 Centralized Model Configuration

**Location:** `/home/joerissen/ai/ai4artsed_webserver/devserver/config.py`

**Model Constants:**
```python
# Stage-specific model assignments
STAGE1_TEXT_MODEL = "llama-guard-3-8b"           # Safety check (local only)
STAGE2_INTERCEPTION_MODEL = "mistral-nemo"       # Pedagogical transformation
STAGE3_MODEL = "llama-guard-3-8b"                # Pre-output safety
# ... etc
```

**Why This Pattern:**
- âœ… Change models in ONE location (config.py)
- âœ… NO hardcoded values in application code
- âœ… Consistent model usage across all flows
- âœ… Easy to switch between local/cloud models
- âœ… Follows "NO WORKAROUNDS" principle

### 2.3 Migration Summary

**Removed from API Calls:**
- `/pipeline/stage2` endpoint: No longer accepts execution_mode
- Frontend (text_transformation.vue): Removed execution_mode from 3 API calls
- Backend routes: Model selection via config.py constants ONLY

**Chunk Configuration:**
- OLD: `"model": "mistral-nemo"` (hardcoded)
- NEW: `"model": "STAGE2_INTERCEPTION_MODEL"` (references config.py)

**Example: manipulate.json**
```json
{
  "chunk_name": "manipulate",
  "model": "STAGE2_INTERCEPTION_MODEL",
  "meta": {
    "task_type": "standard"
  }
}
```

### 2.4 Future Model Switching

**To change models system-wide:**
1. Edit `devserver/config.py`
2. Change constant value (e.g., `STAGE2_INTERCEPTION_MODEL = "claude-3.5-haiku"`)
3. NO code changes required in routes, Vue components, or chunks
4. Restart devserver

**To switch between local/cloud:**
- Update config.py constants to point to cloud models
- Ensure API keys configured in environment
- NO architectural changes required

### 2.5 Architectural Principles

This refactoring follows the project's core principles:
- âŒ **NO prefix hacks** - Model names not prefixed with "00-" for load order
- âŒ **NO temporary fixes** - execution_mode removed, not "improved"
- âœ… **Fix root problems** - Centralized configuration eliminates duplication
- âœ… **Clean, maintainable code** - Single source of truth in config.py

**See:** `/home/joerissen/.claude/CLAUDE.md` - Project instructions enforcing this pattern

---

**Document Version:** 2.2
**Last Updated:** 2025-11-23 (Session 65)
**Status:** v2.1.0-alpha - 2-Phase Stage 2 Execution + Centralized Model Selection
**Authors:** Joerissen + Claude collaborative design
