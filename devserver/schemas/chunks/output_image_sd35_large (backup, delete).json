{
  "name": "output_image_sd35_large",
  "type": "output_chunk",
  "backend_type": "comfyui",
  "media_type": "image",
  "description": "Stable Diffusion 3.5 Large - High quality image generation with Dual CLIP",

  "workflow": {
    "1": {
      "inputs": {
        "text": ["10", 0],
        "clip": ["9", 0]
      },
      "class_type": "CLIPTextEncode",
      "_meta": {
        "title": "CLIP Text Encode (Prompt)"
      }
    },
    "2": {
      "inputs": {
        "samples": ["8", 0],
        "vae": ["5", 2]
      },
      "class_type": "VAEDecode",
      "_meta": {
        "title": "VAE Decode"
      }
    },
    "3": {
      "inputs": {
        "width": 1024,
        "height": 1024,
        "batch_size": 1
      },
      "class_type": "EmptyLatentImage",
      "_meta": {
        "title": "Empty Latent Image"
      }
    },
    "4": {
      "inputs": {
        "filename_prefix": "ComfyUI",
        "images": ["2", 0]
      },
      "class_type": "SaveImage",
      "_meta": {
        "title": "Save Image"
      }
    },
    "5": {
      "inputs": {
        "ckpt_name": "OfficialStableDiffusion/sd3.5_large.safetensors"
      },
      "class_type": "CheckpointLoaderSimple",
      "_meta": {
        "title": "Choose your checkpoint ( = image generation model)"
      }
    },
    "6": {
      "inputs": {
        "text": ["11", 0],
        "clip": ["9", 0]
      },
      "class_type": "CLIPTextEncode",
      "_meta": {
        "title": "CLIP Text Encode (Prompt)"
      }
    },
    "7": {
      "inputs": {
        "clip_name1": "clip_l.safetensors",
        "clip_name2": "clip_g.safetensors",
        "clip_name3": "t5xxl_enconly.safetensors"
      },
      "class_type": "TripleCLIPLoader",
      "_meta": {
        "title": "TripleCLIPLoader"
      }
    },
    "8": {
      "inputs": {
        "seed": 123456789,
        "steps": 25,
        "cfg": 5.5,
        "sampler_name": "euler",
        "scheduler": "normal",
        "denoise": 1,
        "model": ["5", 0],
        "positive": ["1", 0],
        "negative": ["6", 0],
        "latent_image": ["3", 0]
      },
      "class_type": "KSampler",
      "_meta": {
        "title": "KSampler"
      }
    },
    "9": {
      "inputs": {
        "clip_name1": "clip_g.safetensors",
        "clip_name2": "t5xxl_enconly.safetensors",
        "type": "sd3",
        "device": "default"
      },
      "class_type": "DualCLIPLoader",
      "_meta": {
        "title": "DualCLIPLoader"
      }
    },
    "10": {
      "inputs": {
        "value": ""
      },
      "class_type": "PrimitiveString",
      "_meta": {
        "title": "positive_prompt"
      }
    },
    "11": {
      "inputs": {
        "value": ""
      },
      "class_type": "PrimitiveString",
      "_meta": {
        "title": "negative_prompt"
      }
    }
  },

  "input_mappings": {
    "prompt": {
      "node_id": "10",
      "field": "inputs.value",
      "source": "{{PREVIOUS_OUTPUT}}",
      "description": "Positive prompt - main image description"
    },
    "negative_prompt": {
      "node_id": "11",
      "field": "inputs.value",
      "default": "blurry, bad quality, watermark, text, distorted",
      "description": "Negative prompt - what to avoid"
    },
    "width": {
      "node_id": "3",
      "field": "inputs.width",
      "default": 1024,
      "description": "Image width in pixels"
    },
    "height": {
      "node_id": "3",
      "field": "inputs.height",
      "default": 1024,
      "description": "Image height in pixels"
    },
    "steps": {
      "node_id": "8",
      "field": "inputs.steps",
      "default": 25,
      "description": "Number of sampling steps (higher = better quality, slower)"
    },
    "cfg": {
      "node_id": "8",
      "field": "inputs.cfg",
      "default": 5.5,
      "description": "Classifier Free Guidance scale (how closely to follow prompt)"
    },
    "sampler_name": {
      "node_id": "8",
      "field": "inputs.sampler_name",
      "default": "euler",
      "description": "Sampling algorithm"
    },
    "scheduler": {
      "node_id": "8",
      "field": "inputs.scheduler",
      "default": "normal",
      "description": "Noise scheduling algorithm"
    },
    "denoise": {
      "node_id": "8",
      "field": "inputs.denoise",
      "default": 1.0,
      "description": "Denoise strength (1.0 = full denoise from noise, 0.7-0.8 for img2img)"
    },
    "seed": {
      "node_id": "8",
      "field": "inputs.seed",
      "default": 123456789,
      "description": "Seed for reproducibility (default: 123456789)"
    },
    "checkpoint": {
      "node_id": "5",
      "field": "inputs.ckpt_name",
      "default": "OfficialStableDiffusion/sd3.5_large.safetensors",
      "description": "Model checkpoint file"
    }
  },

  "output_mapping": {
    "node_id": "4",
    "output_type": "image",
    "format": "png",
    "field": "filename_prefix",
    "description": "SaveImage node that outputs the generated image"
  },

  "meta": {
    "estimated_duration_seconds": "20-60",
    "requires_gpu": true,
    "gpu_vram_mb": 8000,
    "model_file": "sd3.5_large.safetensors",
    "clip_models": [
      "clip_g.safetensors",
      "t5xxl_enconly.safetensors"
    ],
    "recommended_resolution": "1024x1024",
    "supported_resolutions": [
      "512x512",
      "768x768",
      "1024x1024",
      "1280x1280"
    ],
    "notes": "SD3.5 Large requires Dual CLIP (clip_g + t5xxl). Works best at 1024x1024 resolution.",
    "optimization_instruction": "Transform any input text into a professional generative AI PROMPT optimized for clip_g+T5 encoding and conditioning of Stable Diffusion 3.5. DO NOT CHANGE LANGUAGE. Retain all original words with only minimal syntactic adjustments. Convert the entire content into a description of a scene, a constellation, or spatial and sonical arrangement. If INPUT is narrative material, THEN select one key moment and render it visually without telling or interpreting. Translate metaphors into spatial or material compositions. YOU MUST FOLLOW THESE PRINCIPLES >>> THE PROMPT IS NOT A NARRATIVE, IT IS A TEXT FOR A MACHINE. DO NOT BLABBER. CONSTRUCT A VERY STRUCTURED TEXT USING ONLY VISUAL DESCRIPTION, NO ABSTRACT TERMS. >>> You MUST follow this order WITHOUT ANY REDUNDANCIES and you MUST ENUMERATE YOUR OUTPUT ACCORDING TO THESE CATEGORIES. (1) Foreground the core subject as expressed in the input text; (2) embed all verbs and dynamic relations as visual actions; (3) translate spatial, historical, cultural, ecological, and temporal references into visual structures grounded strictly in the text; (4) express atmosphere through light, texture, material behaviour, sound, spatial arrangement, and environmental traces while avoiding generic aesthetic adjectives and genre or mood clichÃ©s; (5) retain any explicitly mentioned materials, styles, or medial references without adding new ones. Output must be a single paragraph without formatting or commentary. --> CORE CONTENT: FIRST 50 WORDS. 250 WORD MAXIMUM OUTPUT LENGTH. "
  }
}
