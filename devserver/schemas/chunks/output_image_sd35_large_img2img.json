{
  "name": "output_image_sd35_large_img2img",
  "type": "output_chunk",
  "backend_type": "comfyui",
  "media_type": "image",
  "description": "Stable Diffusion 3.5 Large IMG2IMG - Image-to-image transformation with Dual CLIP",

  "workflow": {
    "1": {
      "inputs": {
        "text": ["10", 0],
        "clip": ["9", 0]
      },
      "class_type": "CLIPTextEncode",
      "_meta": {
        "title": "CLIP Text Encode (Prompt)"
      }
    },
    "2": {
      "inputs": {
        "samples": ["8", 0],
        "vae": ["5", 2]
      },
      "class_type": "VAEDecode",
      "_meta": {
        "title": "VAE Decode"
      }
    },
    "3": {
      "inputs": {
        "width": 1024,
        "height": 1024,
        "batch_size": 1
      },
      "class_type": "EmptyLatentImage",
      "_meta": {
        "title": "Empty Latent Image (unused in img2img)"
      }
    },
    "4": {
      "inputs": {
        "filename_prefix": "ComfyUI",
        "images": ["2", 0]
      },
      "class_type": "SaveImage",
      "_meta": {
        "title": "Save Image"
      }
    },
    "5": {
      "inputs": {
        "ckpt_name": "OfficialStableDiffusion/sd3.5_large.safetensors"
      },
      "class_type": "CheckpointLoaderSimple",
      "_meta": {
        "title": "Choose your checkpoint ( = image generation model)"
      }
    },
    "6": {
      "inputs": {
        "text": ["11", 0],
        "clip": ["9", 0]
      },
      "class_type": "CLIPTextEncode",
      "_meta": {
        "title": "CLIP Text Encode (Negative Prompt)"
      }
    },
    "7": {
      "inputs": {
        "clip_name1": "clip_l.safetensors",
        "clip_name2": "clip_g.safetensors",
        "clip_name3": "t5xxl_enconly.safetensors"
      },
      "class_type": "TripleCLIPLoader",
      "_meta": {
        "title": "TripleCLIPLoader"
      }
    },
    "8": {
      "inputs": {
        "seed": 123456789,
        "steps": 25,
        "cfg": 5.5,
        "sampler_name": "euler",
        "scheduler": "normal",
        "denoise": 0.75,
        "model": ["5", 0],
        "positive": ["1", 0],
        "negative": ["6", 0],
        "latent_image": ["13", 0]
      },
      "class_type": "KSampler",
      "_meta": {
        "title": "KSampler"
      }
    },
    "9": {
      "inputs": {
        "clip_name1": "clip_g.safetensors",
        "clip_name2": "t5xxl_enconly.safetensors",
        "type": "sd3",
        "device": "default"
      },
      "class_type": "DualCLIPLoader",
      "_meta": {
        "title": "DualCLIPLoader"
      }
    },
    "10": {
      "inputs": {
        "value": ""
      },
      "class_type": "PrimitiveString",
      "_meta": {
        "title": "positive_prompt"
      }
    },
    "11": {
      "inputs": {
        "value": ""
      },
      "class_type": "PrimitiveString",
      "_meta": {
        "title": "negative_prompt"
      }
    },
    "12": {
      "inputs": {
        "image": ""
      },
      "class_type": "LoadImage",
      "_meta": {
        "title": "Load Input Image"
      }
    },
    "13": {
      "inputs": {
        "pixels": ["12", 0],
        "vae": ["5", 2]
      },
      "class_type": "VAEEncode",
      "_meta": {
        "title": "Encode Input Image to Latent"
      }
    }
  },

  "input_mappings": {
    "input_image": {
      "node_id": "12",
      "field": "inputs.image",
      "source": "{{INPUT_IMAGE}}",
      "description": "Path to input image for img2img transformation"
    },
    "prompt": {
      "node_id": "10",
      "field": "inputs.value",
      "source": "{{PREVIOUS_OUTPUT}}",
      "description": "Positive prompt - describes desired transformation"
    },
    "negative_prompt": {
      "node_id": "11",
      "field": "inputs.value",
      "default": "blurry, bad quality, watermark, text, distorted",
      "description": "Negative prompt - what to avoid"
    },
    "width": {
      "node_id": "3",
      "field": "inputs.width",
      "default": 1024,
      "description": "Image width in pixels (unused in img2img, size from input image)"
    },
    "height": {
      "node_id": "3",
      "field": "inputs.height",
      "default": 1024,
      "description": "Image height in pixels (unused in img2img, size from input image)"
    },
    "steps": {
      "node_id": "8",
      "field": "inputs.steps",
      "default": 25,
      "description": "Number of sampling steps (higher = better quality, slower)"
    },
    "cfg": {
      "node_id": "8",
      "field": "inputs.cfg",
      "default": 5.5,
      "description": "Classifier Free Guidance scale (how closely to follow prompt)"
    },
    "sampler_name": {
      "node_id": "8",
      "field": "inputs.sampler_name",
      "default": "euler",
      "description": "Sampling algorithm"
    },
    "scheduler": {
      "node_id": "8",
      "field": "inputs.scheduler",
      "default": "normal",
      "description": "Noise scheduling algorithm"
    },
    "denoise": {
      "node_id": "8",
      "field": "inputs.denoise",
      "default": 0.75,
      "description": "Denoise strength for img2img (0.5-0.9). Higher = more transformation, lower = closer to original"
    },
    "seed": {
      "node_id": "8",
      "field": "inputs.seed",
      "default": 123456789,
      "description": "Seed for reproducibility (default: 123456789)"
    },
    "checkpoint": {
      "node_id": "5",
      "field": "inputs.ckpt_name",
      "default": "OfficialStableDiffusion/sd3.5_large.safetensors",
      "description": "Model checkpoint file"
    }
  },

  "output_mapping": {
    "node_id": "4",
    "output_type": "image",
    "format": "png",
    "field": "filename_prefix",
    "description": "SaveImage node that outputs the generated image"
  },

  "meta": {
    "img2img": true,
    "estimated_duration_seconds": "20-60",
    "requires_gpu": true,
    "gpu_vram_mb": 8000,
    "model_file": "sd3.5_large.safetensors",
    "clip_models": [
      "clip_g.safetensors",
      "t5xxl_enconly.safetensors"
    ],
    "recommended_resolution": "1024x1024",
    "supported_resolutions": [
      "512x512",
      "768x768",
      "1024x1024",
      "1280x1280"
    ],
    "denoise_strength": "0.75 (default for img2img, adjustable 0.5-0.9)",
    "notes": "IMG2IMG version: Requires input image. Denoise controls transformation strength (0.5=subtle, 0.75=moderate, 0.9=strong). SD3.5 Large requires Dual CLIP (clip_g + t5xxl). Works best at 1024x1024 resolution.",
    "optimization_instruction": "Do not change the language of the INPUT! E.g. if the input is GERMAN, answer in GERMAN. Transform the INPUT TEXT into a professional generative-AI prompt optimized for clip_g + T5 encoding in Stable Diffusion 3.5 IMG2IMG. For img2img, focus on TRANSFORMATION INSTRUCTIONS rather than complete scene descriptions. Describe what should CHANGE in the input image: style shifts, material transformations, lighting adjustments, compositional modifications. Preserve all semantic content about desired changes, but condense and reorder for clarity. Write in machine-parsable visual-structural mode using explicit transformation directives, material substitutions, stylistic shifts, and perceptual adjustments. Use constructive, transformation-focused language. Prioritize clear modification instructions that guide the model on how to alter the input image while maintaining its core structure. Respect encoder token limits: place essential transformation directives within the first ~75 tokens for clip_g/clip_L encoding. After that, T5 can interpret up to ~512 tokens for contextual refinement. Focus on: 1. STYLE TRANSFORMATIONS: specific art styles, techniques, or visual treatments to apply. 2. MATERIAL SHIFTS: changes in textures, surfaces, or substance (e.g., 'wood to metal', 'photo to painting'). 3. LIGHTING ADJUSTMENTS: modifications to illumination, shadows, color temperature. 4. COMPOSITIONAL CHANGES: spatial rearrangements, scale shifts, perspective adjustments. 5. DETAIL ENHANCEMENTS: specific areas to refine, clarify, or elaborate. Output a single paragraph, 150 words maximum, focused on transformation directives."
  }
}
