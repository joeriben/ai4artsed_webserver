{
  "name": "output_image_qwen",
  "type": "output_chunk",
  "backend_type": "comfyui",
  "media_type": "image_workflow",
  "description": "Qwen Image - High quality image generation with Qwen 2.5 VL 7B",

  "workflow": {
    "3": {
      "inputs": {
        "seed": 1125488487853216,
        "steps": 20,
        "cfg": 2.5,
        "sampler_name": "euler",
        "scheduler": "simple",
        "denoise": 1,
        "model": [
          "66",
          0
        ],
        "positive": [
          "6",
          0
        ],
        "negative": [
          "7",
          0
        ],
        "latent_image": [
          "58",
          0
        ]
      },
      "class_type": "KSampler",
      "_meta": {
        "title": "KSampler"
      }
    },
    "6": {
      "inputs": {
        "text": [
          "75",
          0
        ],
        "clip": [
          "38",
          0
        ]
      },
      "class_type": "CLIPTextEncode",
      "_meta": {
        "title": "CLIP Text Encode (Positive Prompt)"
      }
    },
    "7": {
      "inputs": {
        "text": "",
        "clip": [
          "38",
          0
        ]
      },
      "class_type": "CLIPTextEncode",
      "_meta": {
        "title": "CLIP Text Encode (Negative Prompt)"
      }
    },
    "8": {
      "inputs": {
        "samples": [
          "3",
          0
        ],
        "vae": [
          "39",
          0
        ]
      },
      "class_type": "VAEDecode",
      "_meta": {
        "title": "VAE Decode"
      }
    },
    "37": {
      "inputs": {
        "unet_name": "qwen_image_fp8_e4m3fn.safetensors",
        "weight_dtype": "default"
      },
      "class_type": "UNETLoader",
      "_meta": {
        "title": "Load Diffusion Model"
      }
    },
    "38": {
      "inputs": {
        "clip_name": "qwen_2.5_vl_7b_fp8_scaled.safetensors",
        "type": "qwen_image",
        "device": "default"
      },
      "class_type": "CLIPLoader",
      "_meta": {
        "title": "Load CLIP"
      }
    },
    "39": {
      "inputs": {
        "vae_name": "qwen_image_vae.safetensors"
      },
      "class_type": "VAELoader",
      "_meta": {
        "title": "Load VAE"
      }
    },
    "58": {
      "inputs": {
        "width": 1328,
        "height": 1328,
        "batch_size": 1
      },
      "class_type": "EmptySD3LatentImage",
      "_meta": {
        "title": "EmptySD3LatentImage"
      }
    },
    "60": {
      "inputs": {
        "filename_prefix": "ai4artsed_qwen",
        "images": [
          "8",
          0
        ]
      },
      "class_type": "SaveImage",
      "_meta": {
        "title": "Save Image"
      }
    },
    "66": {
      "inputs": {
        "shift": 3.1,
        "model": [
          "37",
          0
        ]
      },
      "class_type": "ModelSamplingAuraFlow",
      "_meta": {
        "title": "ModelSamplingAuraFlow"
      }
    },
    "75": {
      "inputs": {
        "value": ""
      },
      "class_type": "PrimitiveStringMultiline",
      "_meta": {
        "title": "Ai4ArtsEd Prompt Input"
      }
    }
  },

  "input_mappings": {
    "prompt": {
      "node_id": "75",
      "field": "inputs.value",
      "source": "{{PREVIOUS_OUTPUT}}",
      "description": "Main prompt - image description"
    },
    "negative_prompt": {
      "node_id": "7",
      "field": "inputs.text",
      "default": "",
      "description": "Negative prompt - what to avoid"
    },
    "width": {
      "node_id": "58",
      "field": "inputs.width",
      "default": 1328,
      "description": "Image width in pixels"
    },
    "height": {
      "node_id": "58",
      "field": "inputs.height",
      "default": 1328,
      "description": "Image height in pixels"
    },
    "steps": {
      "node_id": "3",
      "field": "inputs.steps",
      "default": 20,
      "description": "Number of sampling steps"
    },
    "cfg": {
      "node_id": "3",
      "field": "inputs.cfg",
      "default": 2.5,
      "description": "Classifier Free Guidance scale"
    },
    "sampler_name": {
      "node_id": "3",
      "field": "inputs.sampler_name",
      "default": "euler",
      "description": "Sampling algorithm"
    },
    "scheduler": {
      "node_id": "3",
      "field": "inputs.scheduler",
      "default": "simple",
      "description": "Noise scheduling algorithm"
    },
    "denoise": {
      "node_id": "3",
      "field": "inputs.denoise",
      "default": 1.0,
      "description": "Denoise strength"
    },
    "seed": {
      "node_id": "3",
      "field": "inputs.seed",
      "default": 1125488487853216,
      "description": "Seed for reproducibility"
    }
  },

  "output_mapping": {
    "node_id": "60",
    "output_type": "image",
    "format": "png",
    "field": "filename_prefix",
    "description": "SaveImage node that outputs the generated image"
  },

  "meta": {
    "estimated_duration_seconds": "15-45",
    "requires_gpu": true,
    "gpu_vram_mb": 6000,
    "model_file": "qwen_image_fp8_e4m3fn.safetensors",
    "clip_models": [
      "qwen_2.5_vl_7b_fp8_scaled.safetensors"
    ],
    "vae_model": "qwen_image_vae.safetensors",
    "recommended_resolution": "1328x1328",
    "supported_resolutions": [
      "512x512",
      "768x768",
      "1024x1024",
      "1280x1280",
      "1328x1328"
    ],
    "notes": "Qwen Image uses Qwen 2.5 VL 7B for text encoding with AuraFlow sampling. Optimized for FP8 precision."
  }
}
