{
  "name": "output_image_sd35_large",
  "type": "output_chunk",
  "backend_type": "comfyui",
  "media_type": "image",
  "description": "Stable Diffusion 3.5 Large - High quality image generation with Dual CLIP",

  "workflow": {
    "1": {
      "inputs": {
        "text": ["10", 0],
        "clip": ["9", 0]
      },
      "class_type": "CLIPTextEncode",
      "_meta": {
        "title": "CLIP Text Encode (Prompt)"
      }
    },
    "2": {
      "inputs": {
        "samples": ["8", 0],
        "vae": ["5", 2]
      },
      "class_type": "VAEDecode",
      "_meta": {
        "title": "VAE Decode"
      }
    },
    "3": {
      "inputs": {
        "width": 1024,
        "height": 1024,
        "batch_size": 1
      },
      "class_type": "EmptyLatentImage",
      "_meta": {
        "title": "Empty Latent Image"
      }
    },
    "4": {
      "inputs": {
        "filename_prefix": "ComfyUI",
        "images": ["2", 0]
      },
      "class_type": "SaveImage",
      "_meta": {
        "title": "Save Image"
      }
    },
    "5": {
      "inputs": {
        "ckpt_name": "OfficialStableDiffusion/sd3.5_large.safetensors"
      },
      "class_type": "CheckpointLoaderSimple",
      "_meta": {
        "title": "Choose your checkpoint ( = image generation model)"
      }
    },
    "6": {
      "inputs": {
        "text": ["11", 0],
        "clip": ["9", 0]
      },
      "class_type": "CLIPTextEncode",
      "_meta": {
        "title": "CLIP Text Encode (Prompt)"
      }
    },
    "7": {
      "inputs": {
        "clip_name1": "clip_l.safetensors",
        "clip_name2": "clip_g.safetensors",
        "clip_name3": "t5xxl_enconly.safetensors"
      },
      "class_type": "TripleCLIPLoader",
      "_meta": {
        "title": "TripleCLIPLoader"
      }
    },
    "8": {
      "inputs": {
        "seed": 123456789,
        "steps": 25,
        "cfg": 5.5,
        "sampler_name": "euler",
        "scheduler": "normal",
        "denoise": 1,
        "model": ["5", 0],
        "positive": ["1", 0],
        "negative": ["6", 0],
        "latent_image": ["3", 0]
      },
      "class_type": "KSampler",
      "_meta": {
        "title": "KSampler"
      }
    },
    "9": {
      "inputs": {
        "clip_name1": "clip_g.safetensors",
        "clip_name2": "t5xxl_enconly.safetensors",
        "type": "sd3",
        "device": "default"
      },
      "class_type": "DualCLIPLoader",
      "_meta": {
        "title": "DualCLIPLoader"
      }
    },
    "10": {
      "inputs": {
        "value": ""
      },
      "class_type": "PrimitiveString",
      "_meta": {
        "title": "positive_prompt"
      }
    },
    "11": {
      "inputs": {
        "value": ""
      },
      "class_type": "PrimitiveString",
      "_meta": {
        "title": "negative_prompt"
      }
    }
  },

  "input_mappings": {
    "prompt": {
      "node_id": "10",
      "field": "inputs.value",
      "source": "{{PREVIOUS_OUTPUT}}",
      "description": "Positive prompt - main image description"
    },
    "negative_prompt": {
      "node_id": "11",
      "field": "inputs.value",
      "default": "blurry, bad quality, watermark, text, distorted",
      "description": "Negative prompt - what to avoid"
    },
    "width": {
      "node_id": "3",
      "field": "inputs.width",
      "default": 1024,
      "description": "Image width in pixels"
    },
    "height": {
      "node_id": "3",
      "field": "inputs.height",
      "default": 1024,
      "description": "Image height in pixels"
    },
    "steps": {
      "node_id": "8",
      "field": "inputs.steps",
      "default": 25,
      "description": "Number of sampling steps (higher = better quality, slower)"
    },
    "cfg": {
      "node_id": "8",
      "field": "inputs.cfg",
      "default": 5.5,
      "description": "Classifier Free Guidance scale (how closely to follow prompt)"
    },
    "sampler_name": {
      "node_id": "8",
      "field": "inputs.sampler_name",
      "default": "euler",
      "description": "Sampling algorithm"
    },
    "scheduler": {
      "node_id": "8",
      "field": "inputs.scheduler",
      "default": "normal",
      "description": "Noise scheduling algorithm"
    },
    "denoise": {
      "node_id": "8",
      "field": "inputs.denoise",
      "default": 1.0,
      "description": "Denoise strength (1.0 = full denoise from noise, 0.7-0.8 for img2img)"
    },
    "seed": {
      "node_id": "8",
      "field": "inputs.seed",
      "default": 123456789,
      "description": "Seed for reproducibility (default: 123456789)"
    },
    "checkpoint": {
      "node_id": "5",
      "field": "inputs.ckpt_name",
      "default": "OfficialStableDiffusion/sd3.5_large.safetensors",
      "description": "Model checkpoint file"
    }
  },

  "output_mapping": {
    "node_id": "4",
    "output_type": "image",
    "format": "png",
    "field": "filename_prefix",
    "description": "SaveImage node that outputs the generated image"
  },

  "meta": {
    "estimated_duration_seconds": "20-60",
    "requires_gpu": true,
    "gpu_vram_mb": 8000,
    "model_file": "sd3.5_large.safetensors",
    "clip_models": [
      "clip_g.safetensors",
      "t5xxl_enconly.safetensors"
    ],
    "recommended_resolution": "1024x1024",
    "supported_resolutions": [
      "512x512",
      "768x768",
      "1024x1024",
      "1280x1280"
    ],
    "notes": "SD3.5 Large requires Dual CLIP (clip_g + t5xxl). Works best at 1024x1024 resolution.",
    "optimization_instruction": "Do not change the language of the INPUT! E.g. if the input is GERMAN, answer in GERMAN. Transform the INPUT TEXT into a professional generative-AI prompt optimized for clip_g + T5 encoding in Stable Diffusion 3.5. Preserve all semantic content, but you may condense, reorder, and rephrase for visual clarity. This output is NOT intended for a human reader. Do NOT produce human-readable prose, narrative flow, rhetorical cohesion, or interpretive phrasing. Write in a strictly machine-parsable visual-structural mode using explicit spatial relations, material descriptions, observable configurations, physical transformations, and environmental gradients; no implications, metaphors, symbolism, or emotion terms. Use constructive, spatially decisive language. Prioritize clear arrangement, concrete geometry, material presence, and perceptible transformations. Aim for a coherent, unambiguous visual structure that a diffusion model can encode efficiently. Compress meaning into essential spatial, material, and perceptual cues. Treat the INPUT as raw semantic material to be reorganized into a precise visual configuration. Respect encoder token limits: all essential semantic and visual information MUST be placed within the first ~75 tokens to be captured by clip_g/clip_L (hard cutoff). After that, T5 can interpret up to ~512 tokens for contextualization and global structure; later segments may shift text mode as long as they remain non-narrative and visually grounded. Convert the INPUT into a purely visual, spatially ordered description. No abstract concepts, no genre labels, no aesthetic clich√©s. Translate all content into observable spatial, material, environmental, and perceptual structures. Follow this order strictly, without redundancy, and enumerate using these 5 categories, but output everything as a single paragraph, 250 words maximum, beginning with the first 50 words of core content extracted from the INPUT. 1. SPATIAL ANCHORS: foreground the core subject and place all entities from the INPUT in explicit spatial relations; define viewpoint, distances, scale, orientation, boundaries. 2. OBSERVABLE TRANSFORMATIONS: convert all verbs and dynamic relations from the INPUT into visible physical transformations, gestures, movements, or changes in arrangement. 3. CONTEXT CARRIERS: translate all spatial, cultural, historical, ecological, organizational, or temporal references strictly into material, architectural, environmental, or structural features grounded in the INPUT, without symbolic interpretation. 4. PERCEPTUAL ATMOSPHERICS: express atmosphere only through light behavior, shadow distribution, texture, surface response, density, sound traces, spatial depth, and environmental gradients; avoid emotional or stylistic terms. 5. MEDIA-REFERENTS: retain any explicitly mentioned materials, tools, devices, or media from the INPUT, converting them into concrete visual or physical properties without adding new styles or media. Output a single paragraph, no formatting, no commentary."
  }
}
