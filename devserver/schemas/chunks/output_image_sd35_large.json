{
  "name": "output_image_sd35_large",
  "type": "output_chunk",
  "backend_type": "comfyui",
  "media_type": "image",
  "requires_workflow": false,
  "description": "Stable Diffusion 3.5 Large - High quality image generation with Dual CLIP",

  "workflow": {
    "1": {
      "inputs": {
        "text": ["10", 0],
        "clip": ["9", 0]
      },
      "class_type": "CLIPTextEncode",
      "_meta": {
        "title": "CLIP Text Encode (Prompt)"
      }
    },
    "2": {
      "inputs": {
        "samples": ["8", 0],
        "vae": ["5", 2]
      },
      "class_type": "VAEDecode",
      "_meta": {
        "title": "VAE Decode"
      }
    },
    "3": {
      "inputs": {
        "width": 1024,
        "height": 1024,
        "batch_size": 1
      },
      "class_type": "EmptyLatentImage",
      "_meta": {
        "title": "Empty Latent Image"
      }
    },
    "4": {
      "inputs": {
        "filename_prefix": "ComfyUI",
        "images": ["2", 0]
      },
      "class_type": "SaveImage",
      "_meta": {
        "title": "Save Image"
      }
    },
    "5": {
      "inputs": {
        "ckpt_name": "OfficialStableDiffusion/sd3.5_large.safetensors"
      },
      "class_type": "CheckpointLoaderSimple",
      "_meta": {
        "title": "Choose your checkpoint ( = image generation model)"
      }
    },
    "6": {
      "inputs": {
        "text": ["11", 0],
        "clip": ["9", 0]
      },
      "class_type": "CLIPTextEncode",
      "_meta": {
        "title": "CLIP Text Encode (Prompt)"
      }
    },
    "7": {
      "inputs": {
        "clip_name1": "clip_l.safetensors",
        "clip_name2": "clip_g.safetensors",
        "clip_name3": "t5xxl_enconly.safetensors"
      },
      "class_type": "TripleCLIPLoader",
      "_meta": {
        "title": "TripleCLIPLoader"
      }
    },
    "8": {
      "inputs": {
        "seed": 123456789,
        "steps": 25,
        "cfg": 5.5,
        "sampler_name": "euler",
        "scheduler": "normal",
        "denoise": 1,
        "model": ["5", 0],
        "positive": ["1", 0],
        "negative": ["6", 0],
        "latent_image": ["3", 0]
      },
      "class_type": "KSampler",
      "_meta": {
        "title": "KSampler"
      }
    },
    "9": {
      "inputs": {
        "clip_name1": "clip_g.safetensors",
        "clip_name2": "t5xxl_enconly.safetensors",
        "type": "sd3",
        "device": "default"
      },
      "class_type": "DualCLIPLoader",
      "_meta": {
        "title": "DualCLIPLoader"
      }
    },
    "10": {
      "inputs": {
        "value": ""
      },
      "class_type": "PrimitiveString",
      "_meta": {
        "title": "positive_prompt"
      }
    },
    "11": {
      "inputs": {
        "value": ""
      },
      "class_type": "PrimitiveString",
      "_meta": {
        "title": "negative_prompt"
      }
    }
  },

  "input_mappings": {
    "prompt": {
      "node_id": "10",
      "field": "inputs.value",
      "source": "{{PREVIOUS_OUTPUT}}",
      "description": "Positive prompt - main image description"
    },
    "negative_prompt": {
      "node_id": "11",
      "field": "inputs.value",
      "default": "blurry, bad quality, watermark, text, distorted",
      "description": "Negative prompt - what to avoid"
    },
    "width": {
      "node_id": "3",
      "field": "inputs.width",
      "default": 1024,
      "description": "Image width in pixels"
    },
    "height": {
      "node_id": "3",
      "field": "inputs.height",
      "default": 1024,
      "description": "Image height in pixels"
    },
    "steps": {
      "node_id": "8",
      "field": "inputs.steps",
      "default": 25,
      "description": "Number of sampling steps (higher = better quality, slower)"
    },
    "cfg": {
      "node_id": "8",
      "field": "inputs.cfg",
      "default": 5.5,
      "description": "Classifier Free Guidance scale (how closely to follow prompt)"
    },
    "sampler_name": {
      "node_id": "8",
      "field": "inputs.sampler_name",
      "default": "euler",
      "description": "Sampling algorithm"
    },
    "scheduler": {
      "node_id": "8",
      "field": "inputs.scheduler",
      "default": "normal",
      "description": "Noise scheduling algorithm"
    },
    "denoise": {
      "node_id": "8",
      "field": "inputs.denoise",
      "default": 1.0,
      "description": "Denoise strength (1.0 = full denoise from noise, 0.7-0.8 for img2img)"
    },
    "seed": {
      "node_id": "8",
      "field": "inputs.seed",
      "default": 123456789,
      "description": "Seed for reproducibility (default: 123456789)"
    },
    "checkpoint": {
      "node_id": "5",
      "field": "inputs.ckpt_name",
      "default": "OfficialStableDiffusion/sd3.5_large.safetensors",
      "description": "Model checkpoint file"
    }
  },

  "output_mapping": {
    "node_id": "4",
    "output_type": "image",
    "format": "png",
    "field": "filename_prefix",
    "description": "SaveImage node that outputs the generated image"
  },

  "meta": {
    "estimated_duration_seconds": "12",
    "quality_rating": 2,
    "requires_gpu": true,
    "gpu_vram_mb": 8000,
    "model_file": "sd3.5_large.safetensors",
    "clip_models": [
      "clip_g.safetensors",
      "t5xxl_enconly.safetensors"
    ],
    "recommended_resolution": "1024x1024",
    "supported_resolutions": [
      "512x512",
      "768x768",
      "1024x1024",
      "1280x1280"
    ],
    "notes": "SD3.5 Large requires Dual CLIP (clip_g + t5xxl). Works best at 1024x1024 resolution.",
    "optimization_instruction": "YOUR OUTPUT WILL BE IN THE LANGUAGE OF THE INPUT!\n\n=== CORE TRANSFORMATION: SCENE TO 2D IMAGE ===\nTransform scenic/narrative descriptions into a FLAT 2D IMAGE specification.\nKEY: A 2D image has no story, no before/after - only WHAT IS VISIBLE in ONE FROZEN FRAME.\n\nTRANSFORMATION RULES:\n1. Remove temporal language (before, after, then, suddenly, while)\n2. Remove causality (because, therefore, which causes)\n3. Convert actions to static poses (running → runner mid-stride)\n4. Convert atmosphere to visible elements (mysterious → fog, dim lighting, shadow patterns)\n\n=== TOKEN ARCHITECTURE (SD3.5 Triple CLIP) ===\nFirst 20-30 tokens = 70% of image determination. Order matters exponentially.\n- Tokens 1-10: SUBJECT (main focus)\n- Tokens 11-25: POSE/STATE + STYLE/MEDIUM\n- Tokens 26-50: CONTEXT/SETTING + KEY ATTRIBUTES\n- Tokens 51-75: TECHNICAL (lighting, camera) - CLIP hard cutoff\n- Tokens 76-512: T5 expansion (atmospheric depth)\n\n=== ADVANCED CLIP TECHNIQUES ===\n\nKEYWORD WEIGHTING:\n- (keyword:1.2) = 20% more emphasis\n- (keyword:0.8) = 20% less emphasis\n- ((keyword)) = ~1.21x emphasis (stacks)\n- Example: \"(dramatic side lighting:1.3), figure in doorway\"\n\nKEYWORD BLENDING:\n- [keyword1:keyword2:0.5] = blend at 50% of sampling\n- Example: \"[rough sketch:finished drawing:0.6]\" = loose to refined\n\nASSOCIATION EFFECTS (culturally neutral descriptors ONLY):\n- FORBIDDEN: \"style of [artist]\" or art-historical technique names from any tradition\n- Light: \"dramatic side lighting\", \"soft diffused light\", \"backlit silhouette\"\n- Texture: \"visible grain\", \"smooth gradation\", \"rough surface\", \"glossy finish\"\n- Space: \"shallow depth of field\", \"atmospheric perspective\", \"layered planes\"\n- Color: \"muted earth tones\", \"high contrast\", \"saturated primaries\"\n\nNEGATIVE PROMPTS: NEVER in main prompt (CLIP reads \"no dog\" as \"dog\"). Handle separately.\n\n=== FORMATTING ===\nDO: comma-separated tokens, concrete nouns, (keyword:1.2) emphasis, neutral visual descriptors\nDON'T: negative constructions, abstract adjectives (beautiful, epic), artist names, art-historical terms\n\n=== EXAMPLE ===\nINPUT: \"A mysterious forest where ancient trees whisper secrets to travelers at twilight\"\nOUTPUT: \"ancient forest, massive gnarled trees, twilight sky, orange-purple light through canopy, solitary figure on winding path, mist between trees, dramatic backlighting, deep shadows\"\n\nSingle paragraph output, no headers. First 75 tokens = all critical visual info."
  }
}
