# Backend-Registry for AI4ArtsEd Workshop-Server
# Easily configurable for different hardware setups
#
# Usage:
#   - Set enabled: true/false to enable/disable backends
#   - VRAM requirements are checked at startup (auto_detect_vram)
#   - Priorities determine which backend is used for each media type
#
# For workshop servers: Just edit this file, no config.py changes needed!

backends:
  # === DIRECT BACKENDS (no SwarmUI dependency) ===

  diffusers:
    enabled: true
    description: "HuggingFace Diffusers for Image Generation"
    media_types: [image]
    requirements:
      min_vram_gb: 8
      packages: [diffusers, transformers, accelerate]
    config:
      cache_dir: null  # null = HuggingFace default (~/.cache/huggingface/hub)
      torch_dtype: float16
      device: cuda
      enable_attention_slicing: true
      enable_vae_tiling: false

  heartmula:
    enabled: true
    description: "HeartMuLa Music Generation (lyrics + tags to music)"
    media_types: [music]
    requirements:
      min_vram_gb: 12
      packages: [heartlib]
      model_path: ~/ai/heartlib/ckpt
    config:
      device: cuda
      version: "3B"
      lazy_load: true

  stable_audio:
    enabled: true
    description: "Stable Audio Open for Sound Effects and Ambient"
    media_types: [audio]
    requirements:
      min_vram_gb: 8
      packages: [diffusers]
      model_id: stabilityai/stable-audio-open-1.0
    config:
      torch_dtype: float16
      device: cuda
      max_duration_seconds: 47
      default_steps: 100
      default_cfg_scale: 7.0

  ltx_video:
    enabled: false  # Not yet implemented
    description: "LTX-Video Text-to-Video & Image-to-Video"
    media_types: [video]
    requirements:
      min_vram_gb: 10
      packages: [diffusers, imageio, imageio-ffmpeg]
      model_id: Lightricks/LTX-Video
    config:
      torch_dtype: bfloat16
      device: cuda

  wan_video:
    enabled: false  # Not yet implemented
    description: "WAN 2.2 Video Generation (5B/14B models)"
    media_types: [video]
    requirements:
      min_vram_gb: 16  # 5B model
      packages: [diffusers, imageio, imageio-ffmpeg]
    config:
      t2v_model_id: Wan-AI/Wan2.1-T2V-5B-Diffusers
      i2v_model_id: Wan-AI/Wan2.2-I2V-14B-Diffusers
      torch_dtype: float16
      device: cuda

  # === COMFYUI/SWARMUI BACKENDS (Fallback) ===

  comfyui:
    enabled: true
    description: "ComfyUI via SwarmUI (Fallback for all media types)"
    media_types: [image, audio, video, music]
    requirements:
      external_service: true
      ports: [7801, 7821]
    config:
      api_port: 7801
      comfy_port: 7821
      auto_start: true
      startup_timeout: 120

  # === INFERENCE SERVERS ===

  triton:
    enabled: false
    description: "NVIDIA Triton Inference Server (batched inference)"
    media_types: [image]
    requirements:
      external_service: true
      ports: [8000, 8001, 8002]
    config:
      http_url: localhost:8000
      grpc_url: localhost:8001
      metrics_url: localhost:8002

  # === CLOUD BACKENDS ===

  openai_images:
    enabled: false  # Requires API key
    description: "OpenAI GPT-Image-1 / DALL-E"
    media_types: [image]
    requirements:
      api_key_file: openai.key
    config:
      model: gpt-image-1

  openrouter:
    enabled: false  # Requires API key
    description: "OpenRouter (Gemini, etc.)"
    media_types: [image]
    requirements:
      api_key_file: openrouter.key

# Priority chains: First available backend is used
# Override per media type
priorities:
  image:
    - diffusers      # Faster, less overhead
    - triton         # Batched inference for workshops
    - openai_images  # Cloud fallback
    - comfyui        # Fallback for complex workflows
  music:
    - heartmula      # Dedicated music generator
    - comfyui        # ACE-Step via ComfyUI as fallback
  audio:
    - stable_audio   # Direct via Diffusers
    - comfyui        # StableAudio via ComfyUI as fallback
  video:
    - ltx_video      # LTX via Diffusers (smaller)
    - wan_video      # WAN via Diffusers (larger)
    - comfyui        # ComfyUI for all video models

# Global settings
settings:
  # Check VRAM at startup and disable backends that exceed available memory
  auto_detect_vram: true

  # On direct backend error -> try ComfyUI
  fallback_to_comfyui: true

  # Log which backend was selected for each request
  log_backend_selection: true

  # Cache ComfyUI node list for workflow requirement checks
  comfyui_node_cache_ttl: 300  # 5 minutes
