#!/usr/bin/env python3
"""
LIVE OLLAMA TEST - Echter Pipeline-Testlauf
Input: "Ein Pferd steht auf einer grÃ¼nen Wiese"
Interception: "make every color = grey"
"""

import sys
import asyncio
import logging
from pathlib import Path

# Pfad fÃ¼r Schema-Imports anpassen
sys.path.insert(0, str(Path(__file__).parent))

from schemas.engine.prompt_interception_engine import (
    PromptInterceptionEngine, 
    PromptInterceptionRequest
)

# Logging konfigurieren fÃ¼r Live-Output
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

async def test_live_ollama_pipeline():
    """Live Ollama Pipeline Test mit echten API-Aufrufen"""
    
    print("=" * 80)
    print("ğŸš€ LIVE OLLAMA PIPELINE TEST - Schema-System mit echten KI-Aufrufen")
    print("=" * 80)
    
    # Test-Parameter
    test_input = "Ein Pferd steht auf einer grÃ¼nen Wiese"
    interception_instruction = "make every color = grey"
    
    print(f"\nğŸ“ Test-Input: '{test_input}'")
    print(f"ğŸ¯ Interception: '{interception_instruction}'")
    
    # 1. Prompt Interception Engine initialisieren
    print("\n1. ğŸ”§ Prompt Interception Engine Setup:")
    engine = PromptInterceptionEngine()
    
    available_ollama = engine._get_ollama_models()
    if not available_ollama:
        print("   âŒ Keine Ollama-Modelle verfÃ¼gbar!")
        print("   ğŸ’¡ Starten Sie Ollama: 'ollama serve'")
        print("   ğŸ’¡ Installieren Sie ein Modell: 'ollama pull gemma2:9b'")
        return
    
    print(f"   âœ… {len(available_ollama)} Ollama-Modelle verfÃ¼gbar: {available_ollama[:3]}...")
    
    # WÃ¤hle bestes verfÃ¼gbares Modell
    preferred_models = ["gemma2:9b", "llama3.2:1b", "llama3.1:8b", "phi3:mini"]
    selected_model = None
    
    for model in preferred_models:
        if model in available_ollama:
            selected_model = model
            break
    
    if not selected_model:
        selected_model = available_ollama[0]
    
    print(f"   ğŸ¯ GewÃ¤hltes Modell: {selected_model}")
    
    # 2. Schritt 1: Translation (Deutsch â†’ Englisch)
    print(f"\n2. ğŸŒ TRANSLATION-SCHRITT:")
    print("   Request: Deutsch â†’ Englisch")
    
    translation_instructions = """Translate the following text to English. CRITICAL RULES:
1. Preserve ALL brackets exactly as they appear
2. Translate with maximal semantic preservation
3. Keep original structure and formatting
4. Output ONLY the translated text, nothing else
5. If text is already in English, return it unchanged"""
    
    translation_request = PromptInterceptionRequest(
        input_prompt=test_input,
        input_context="",
        style_prompt=translation_instructions,
        model=f"local/{selected_model}",
        debug=True,
        unload_model=False
    )
    
    print("   ğŸ”„ Sende Translation-Request an Ollama...")
    translation_response = await engine.process_request(translation_request)
    
    if not translation_response.success:
        print(f"   âŒ Translation fehlgeschlagen: {translation_response.error}")
        return
    
    translated_text = translation_response.output_str.strip()
    print(f"   âœ… Translation-Ergebnis: '{translated_text}'")
    
    # 3. Schritt 2: Color Manipulation (Alle Farben â†’ Grau)
    print(f"\n3. ğŸ¨ MANIPULATION-SCHRITT:")
    print("   Request: Alle Farben â†’ Grau")
    
    color_instructions = """Your task is to transform the input text to make every color mentioned grey.

RULES:
1. Replace ALL color words with "grey" or "gray"
2. Keep the original sentence structure intact
3. Maintain all other descriptive elements
4. Only change color-related words (red, green, blue, yellow, white, black, brown, etc.)
5. Output ONLY the transformed text, no explanations

Examples:
- "red car" â†’ "grey car"
- "blue sky and green grass" â†’ "grey sky and grey grass"
- "white horse on green meadow" â†’ "grey horse on grey meadow"

Transform the following text by making every color grey:"""
    
    manipulation_request = PromptInterceptionRequest(
        input_prompt=translated_text,
        input_context="",
        style_prompt=color_instructions,
        model=f"local/{selected_model}",
        debug=True,
        unload_model=True  # Modell nach dem letzten Schritt entladen
    )
    
    print("   ğŸ”„ Sende Manipulation-Request an Ollama...")
    manipulation_response = await engine.process_request(manipulation_request)
    
    if not manipulation_response.success:
        print(f"   âŒ Manipulation fehlgeschlagen: {manipulation_response.error}")
        return
    
    final_result = manipulation_response.output_str.strip()
    print(f"   âœ… Manipulation-Ergebnis: '{final_result}'")
    
    # 4. Pipeline-Zusammenfassung
    print(f"\n4. ğŸ“‹ PIPELINE-ZUSAMMENFASSUNG:")
    print("   " + "=" * 60)
    print(f"   ğŸ“¥ Input:       '{test_input}'")
    print(f"   ğŸŒ Translation: '{translated_text}'") 
    print(f"   ğŸ¨ Manipulation: '{final_result}'")
    print("   " + "=" * 60)
    
    # 5. Erfolg-Validierung
    print(f"\n5. âœ… ERFOLGS-VALIDIERUNG:")
    
    # PrÃ¼fe ob Translation erfolgreich
    if "pferd" not in translated_text.lower() and ("horse" in translated_text.lower() or "pony" in translated_text.lower()):
        print("   âœ… Translation: Deutsch â†’ Englisch erfolgreich")
    else:
        print("   âš ï¸  Translation: MÃ¶glicherweise unvollstÃ¤ndig")
    
    # PrÃ¼fe ob Color-Manipulation erfolgreich
    color_words = ["green", "grÃ¼n", "red", "blue", "yellow", "white", "black", "brown"]
    colors_found = [word for word in color_words if word in final_result.lower()]
    greys_found = ["grey", "gray"]
    grey_count = sum(1 for grey in greys_found if grey in final_result.lower())
    
    if not colors_found and grey_count > 0:
        print("   âœ… Color-Manipulation: Alle Farben zu Grau erfolgreich")
    elif colors_found:
        print(f"   âš ï¸  Color-Manipulation: Noch Farben gefunden: {colors_found}")
    else:
        print("   âš ï¸  Color-Manipulation: Keine Grau-WÃ¶rter erkannt")
    
    print(f"\n6. ğŸ¯ SCHEMA-SYSTEM VALIDIERUNG:")
    print("   âœ… Prompt Interception Engine: Funktional")
    print("   âœ… Multi-Step-Pipeline: Funktional") 
    print("   âœ… Ollama-Integration: Funktional")
    print("   âœ… Task+Context+Prompt-Format: Funktional")
    print("   âœ… Custom-Instructions-System: Funktional")
    
    print("\n" + "=" * 80)
    print("ğŸ‰ LIVE OLLAMA TEST ERFOLGREICH ABGESCHLOSSEN!")
    print("Schema-basierte Pipeline-Architektur arbeitet mit echten KI-Models!")
    print("=" * 80)

if __name__ == "__main__":
    asyncio.run(test_live_ollama_pipeline())
