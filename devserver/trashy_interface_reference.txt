AI4ARTSED KNOWLEDGE BASE FOR TRASHY - COMPREHENSIVE USER GUIDE
Last updated: 2026-01-29

================================================================================
SECTION 1: PLATFORM OVERVIEW
================================================================================

AI4ArtsEd is a pedagogical-scientific experimentation platform for critical and creative engagement with generative AI in cultural education (ages 8-17). It is used in art-AI workshops with educators present.

CORE CONCEPT: The Large Language Model (LLM) acts as a co-actor in the creative process, not just a tool. It processes input based on its training data and creates something new. This is fascinating but also problematic - we don't know exactly how or why the model makes certain decisions.

KEY QUESTIONS WE EXPLORE:
- How do different models react to different inputs?
- What happens with detailed, nuanced descriptions instead of short prompts?
- How can we learn to truly understand what we want?
- How can we understand and change our image idea from many different perspectives?

================================================================================
SECTION 2: PLATFORM STRUCTURE
================================================================================

START PAGE (SELECT VIEW)
------------------------
Location: Route "/" (root)
Purpose: Users select RULES that will apply to their experiment.

CENTRAL ELEMENT: "Du bestimmst!" / "Your Call!"
- This is in the CENTER of the page
- It starts with an EMPTY rules box
- Users can write their own rules from scratch here

SURROUNDING ELEMENTS: Pre-made rule suggestions
- Everything around "Du bestimmst!" are INSPIRATIONS
- Users can select one to use as a starting point
- Or they can use these as examples to learn how rules work

The selected rules define:
- Creative constraints
- Style approaches
- Material specifications
- Perspective guidelines

IMPORTANT: The selected rules are automatically applied to all three modes - users don't need to re-enter them.

WHAT HAPPENS WITH THE RULES:
- A text AI will process the user's idea (WHAT) together with the rules (HOW)
- This creates an expanded, differentiated prompt
- The result can be:
  - Edited directly in the box
  - Regenerated by clicking "Start" again
  - Copied and pasted into the prompt box for further work

FOUR MODES
----------

IMPORTANT: The selected RULES apply in the first THREE modes!
The difference between modes is WHAT you input (text vs images).
Canvas mode is different - you define the entire process yourself.

1. TEXT MODE (text_transformation.vue, route /pipeline-execution/:configId)
   Icon: "A" with arrow (translation symbol)
   Input: TEXT (your idea)
   How it works:
   - User enters an IDEA as text (WAS/WHAT should be created)
   - The text AI processes the idea together with the RULES
   - Creates an expanded prompt
   - User can observe, understand, and modify all intermediate steps
   - User selects output medium (image, video, audio, music)
   - User selects a generation model
   - Media is generated

2. IMAGE MODE (image_transformation.vue, route /image-transformation/:configId)
   Icon: Landscape/image symbol
   Input: ONE IMAGE (replaces the text prompt)
   How it works:
   - User uploads ONE image - this REPLACES the text prompt
   - User describes what should happen to it
   - The RULES still apply to the transformation
   - Vision model analyzes the uploaded image
   - Model creates a new version based on description + rules

3. MULTI-IMAGE MODE (multi_image.vue)
   Icon: Grid with plus symbol
   Input: UP TO THREE IMAGES (replace the text prompt together)
   How it works:
   - User uploads UP TO THREE images - these REPLACE the text prompt
   - User describes how they should be combined
   - The RULES still apply to the combination
   - Vision-capable model (QWEN 2511) processes all images together
   - Creates fusion based on description + rules

4. CANVAS MODE (canvas_workflow.vue, route /canvas)
   Icon: Account tree / branching diagram
   Input: USER-DEFINED WORKFLOW (nodes connected by lines)
   How it works:
   - NO pre-selected rules apply - user defines the ENTIRE process
   - Visual node-based workflow builder
   - Drag-and-drop nodes, connect with lines
   - Define data flow from input through transformations to output
   - Supports parallel generation, evaluation, and recursive workflows
   - Designed for RESEARCH and SYSTEMATIC EXPLORATION

================================================================================
SECTION 3: TEXT MODE - DETAILED WORKFLOW
================================================================================

STEP 1: INPUT FIELDS (Two side-by-side bubbles)

LEFT BUBBLE - "Deine Idee" (Your Idea)
- Label: "Deine Idee: Worum soll es gehen?" / "Your Idea: What should it be about?"
- Purpose: User describes the SUBJECT MATTER - WHAT should be created
- Always editable, no restrictions
- When filled: border changes from white (low opacity) to blue with blue background

RIGHT BUBBLE - "Regeln/Eigenschaften" (Rules/Properties)
- Label: "Bestimme Regeln, Material, Besonderheiten" / "Define rules, materials, specifics"
- Purpose: Contains the meta-prompt defining HOW it should be realized
- Initially loaded from selected configuration in user's language
- Always editable - user can modify or write completely new rules
- IMPORTANT: The goal is for users to learn to formulate their OWN rules

STEP 2: INTERCEPTION (Verarbeitung)

What happens:
- User clicks first START button (triple arrow)
- LLM (Language Model) processes BOTH inputs together
- Combines the IDEA with the RULES
- Creates an expanded, differentiated prompt

Result display:
- Label: "Idee + Regeln = Prompt"
- Shows the LLM's output in a green-tinted box
- FULLY EDITABLE - user can modify everything
- This is a SUGGESTION, not a final result

WHY INTERCEPTION EXISTS:
1. Awareness of rules and material - By explicitly formulating rules, users become aware of what decisions they're making
2. Learning to understand prompts - Same basic idea can produce completely different results with different rules
3. Critical exploration of AI - How do models react to detailed, expressive prompts? Where are their limits?

STEP 3: MEDIA AND MODEL SELECTION

The platform is MULTIMODAL - not just images, but also:
- Images (various models)
- Video (LTX Video, Wan 2.2)
- Audio/Sound (Stable Audio)
- Music/Instrumental (ACE Instrumental)

AVAILABLE IMAGE MODELS:
- Stable Diffusion 3.5 Large (sd35_large) - blue, local
- GPT Image 1 (gpt_image_1) - yellow, OpenAI
- Gemini 3 Pro Image (gemini_3_pro_image) - blue, Google
- QWEN (qwen) - purple, vision-capable

VIDEO MODELS:
- LTX Video - fast local video generation
- Wan 2.2 Video - high-quality 720p video (5B model)

AUDIO MODELS:
- ACE Instrumental - AI music generation for instrumentals
- Stable Audio Open - open-source audio generation (max 47s)

THE EXCITING PART: Users can try the SAME prompt with DIFFERENT models - within one medium or even across media. This teaches how differently models react to the same input.

STEP 4: PROMPT OPTIMIZATION (depends on the model)

Not all models need the same prompt style. Here's what happens:

STABLE DIFFUSION 3.5:
- NEEDS optimization
- Prompt is converted to "classic" prompt style (keywords, weightings, tags)
- This is a good opportunity to LEARN how traditional prompting works
- Users can see how a natural description becomes a technical prompt

GPT IMAGE, GEMINI, QWEN:
- DON'T NEED optimization
- These models have their own POWERFUL LANGUAGE MODULE built-in
- They understand natural language directly
- The interception result goes directly to the model

VIDEO MODELS (LTX, Wan 2.2):
- Receive optimization for SCENIC descriptions
- Prompts are adapted for movement, timing, camera work

AUDIO MODELS (ACE, Stable Audio):
- Receive optimization for AUDITIVE descriptions
- Prompts are adapted for sound, rhythm, musical structure

P5.JS (Special case):
- This generates CODE, not images
- Optimization prepares the prompt for code generation
- Output is a p5.js sketch that runs in the browser

Display:
- Label: "Modell-Optimierter Prompt"
- Shows in orange-tinted box
- FULLY EDITABLE - the optimized prompt is just a suggestion
- Users can always modify it before generation

STEP 5: TRANSLATION TO ENGLISH

At the end, the prompt is translated to English (if not already).
WHY? Because generative AI models usually only understand English really well.
We want users to work in their own language - so the system handles translation.
This step is currently not visible but happens in the background.

STEP 6: GENERATION

User clicks second START button to generate media.
- Progress animation shows during generation
- Duration: typically 20 seconds to 2 minutes depending on model
- Result displays in output frame
- Images can be clicked for fullscreen view
- Videos/audio have playback controls

================================================================================
SECTION 3b: CANVAS MODE - DETAILED GUIDE
================================================================================

WHAT IS CANVAS?

Canvas is a visual node-based workflow builder for creating custom AI pipelines.
Unlike the other three modes where pre-selected RULES apply, in Canvas you define
the ENTIRE process yourself - from input through transformations to output.

WHO IS CANVAS FOR?

Canvas is designed for:
- Older children (12+): Structured exploration of AI behavior
- Educators: Professional development for understanding genAI
- Researchers: Systematic investigation of model behavior, bias, creativity
- Workshop leaders: Creating research datasets through bulk generation

CANVAS BASICS

Every Canvas workflow has:
- ONE Input node (where text goes in)
- ONE Collector node (where results are gathered)
- Any number of processing nodes in between
- Connections (lines) showing data flow

HOW TO USE CANVAS

STEP 1: Open Canvas
- Click the Canvas icon (branching diagram) in the navigation
- You see a grid workspace with a toolbar on the left

STEP 2: Add Nodes
- Click nodes in the toolbar to add them to the canvas
- Drag nodes to position them
- Click a node to select it and see its settings

STEP 3: Connect Nodes
- Click the output connector (right side) of a node
- Then click the input connector (left side) of another node
- A line appears showing the connection

STEP 4: Configure Nodes
- Select a node to see its settings on the right
- Each node type has different options (see below)

STEP 5: Execute
- Click the Execute button
- Watch the workflow run node by node
- Results appear in the Collector

AVAILABLE NODE TYPES

1. INPUT PROMPT (Blue)
   - Entry point for your text
   - Type your prompt here
   - Every workflow needs exactly one

2. RANDOM PROMPT (Pink)
   - LLM generates creative content for you
   - Presets: Scenic Description, Photo Prompt, Artform, Instruction, Language
   - Good for exploring unexpected ideas

3. INTERCEPTION (Purple)
   - Pedagogical transformation of text
   - Same presets as the other modes (Bauhaus, Renaissance, etc.)
   - OR write your own transformation rules
   - This is where creative rules are applied

4. TRANSLATION (Amber)
   - Translates text to another language
   - Useful for multilingual experiments
   - Can be chained for "telephone game" effects

5. MODEL ADAPTION (Teal)
   - Optimizes prompts for specific AI models
   - Presets: None (pass-through), SD3.5, Flux, Video, Audio
   - Different models need different prompt styles

6. GENERATION (Green)
   - Creates the actual media (image, audio, video)
   - Select which AI model to use
   - Can have MULTIPLE generation nodes for comparison

7. EVALUATION (Amber)
   - LLM judges the output
   - Types: Fairness, Creativity, Bias, Quality, Custom
   - Can BRANCH the workflow based on results
   - Enables recursive workflows (feedback loops)

8. PREVIEW (Green)
   - Shows intermediate results inline
   - Good for debugging and understanding

9. MEDIA OUTPUT / COLLECTOR (Cyan)
   - Collects ALL workflow outputs
   - Shows results with source attribution
   - Every workflow needs exactly one

SPECIAL CANVAS CAPABILITIES

PARALLEL GENERATION:
- Connect one Interception to MULTIPLE Generation nodes
- Compare how different models interpret the same prompt
- All results appear in the Collector

EVALUATION WITH BRANCHING:
When branching is enabled, Evaluation has THREE outputs:
- P (Passthrough): Original text if evaluation PASSED (green)
- C (Commented): Text + feedback if evaluation FAILED (orange)
- → (Commentary): Just the feedback, always active (cyan)

RECURSIVE WORKFLOWS:
- Connect the C (Commented) output back to an earlier node
- Workflow ITERATES until evaluation passes
- Great for bias exploration and quality improvement

EXAMPLE WORKFLOWS

SIMPLE:
Input → Interception → Generation → Collector

COMPARISON:
Input → Interception ─┬→ Generation (SD3.5) ─┬→ Collector
                      └→ Generation (Flux)  ─┘

WITH FEEDBACK LOOP:
Input → Interception → Generation → Evaluation ─┬→ (P) Collector
              ↑                                  └→ (C) back to Interception

WHY CANVAS IS DIFFERENT

Canvas deliberately differs from technical systems like ComfyUI:

| Aspect          | ComfyUI (Technical)    | Canvas (Educational)     |
|-----------------|------------------------|--------------------------|
| Primary Focus   | Product optimization   | Process exploration      |
| Parameters      | Deep technical control | Minimal technical params |
| Complexity      | Technical complexity   | Structural complexity    |
| User Model      | Expert operator        | Researcher/Learner       |

Canvas REDUCES technical complexity to make STRUCTURAL complexity visible.
Users who need fine-grained control can use ComfyUI directly.

THE PEDAGOGICAL IDEA BEHIND CANVAS

Canvas challenges the conventional human-AI interaction model:

CONVENTIONAL (rejected):
User → Command → AI → Optimized Output
"User as operator commanding the machine"

CANVAS APPROACH:
User → Input → [LLM Interception] → Transformed Prompt → Generation
                     ↓
         Introduces novelty, unpredictability
         Machine serves creative becoming

In Canvas:
- User inputs are NOT technical directives to be optimized
- The LLM introduces NOVELTY and UNPREDICTABILITY
- Users and LLM create new feedback loops TOGETHER
- The machine serves the user's creative becoming, not vice versa

TIPS FOR CANVAS USERS

- Start simple: Input → Interception → Generation → Collector
- Add complexity gradually
- Use Preview nodes to see what's happening at each step
- Try parallel generation to compare models
- Experiment with evaluation for quality control
- Don't be afraid to create complex workflows - that's the point!

================================================================================
SECTION 4: CIRCULARITY CONCEPT
================================================================================

Even though the workflow LOOKS linear (top to bottom), the platform is designed to be CIRCULAR:

- Go back at any time
- Copy any text and paste it elsewhere
- Load generated images into Image Mode or Multi-Image Mode
- Continue processing and iterating
- Experiment!

EXAMPLE OF CIRCULAR WORKFLOW:
You can transform a prompt with the "Cutifier" (Verniedlicher), copy the result back up to the input,
then go back to the start page and apply a completely different rule like "De-Kitschifier" (Entkitscher),
then do the same again with "Exaggerator" (Übertreiber), and so on.
And in between, YOU can always change something yourself - add words, remove parts, refine ideas.
This chain of transformations creates unique, layered results.

The seed logic supports both:
- VARIATION: Same prompt → new seed → different result
- ITERATION: Modified prompt → same seed → refined result

================================================================================
SECTION 5: THE WAS/WIE PRINCIPLE (WHAT/HOW)
================================================================================

In text mode, users work with TWO SEPARATE inputs:
- WAS (WHAT): The idea - what should be created
- WIE (HOW): The rules - how should it be realized

There are pre-made configurations as assistance, but the GOAL is for users to learn to formulate their OWN rules.

EXAMPLE RULES:
- "Beschreibe alles aus der Perspektive der Vögel auf den Bäumen" (Describe everything from the perspective of birds in the trees)
- "Verwende nur Farben, die man in der Natur findet" (Use only colors found in nature)
- "Stelle dir vor, du erklärst es jemandem, der noch nie ein Bild gesehen hat" (Imagine explaining it to someone who has never seen an image)

================================================================================
SECTION 6: EXPERIMENTAL WORKFLOWS
================================================================================

These approaches work at a deeper technical level and make the "black box" somewhat visible:

SURREALIZER
-----------
Uses TWO DIFFERENT text encoders (CLIP and T5) and merges their results.
- Each encoder "understands" text differently
- Mixing them creates unexpected, often surreal results
- Reveals how meaning is encoded differently by different systems

SPLIT & COMBINE / PARTIAL ELIMINATION
-------------------------------------
Work directly at the mathematical level of semantic vectors.
- What happens when we merge meaning spaces?
- What happens when we remove parts of meaning?
- These experiments make AI's internal representations somewhat visible

LATENT LAB — ATTENTION CARTOGRAPHY
-----------------------------------
Location: Route "/latent-lab" → Tab "Attention Cartography"

What it does:
- Generates an image from a prompt AND simultaneously records which word
  influenced which image region (cross-attention maps)
- Users click on words to see colored heatmap overlays on the generated image
- Multiple words can be selected simultaneously (different colors)

How it works technically:
- SD3.5's MMDiT transformer has 24 layers where text and image tokens attend
  to each other via "joint attention"
- We replace the default fast attention (SDPA) with a manual softmax(QK^T/√d)
  processor at 3 selected layers to extract the text→image attention submatrix
- Maps have 64×64 resolution (patch grid), upscaled to image resolution

Key concepts to explain to users:
- Attention is NOT "one word = one region". It distributes influence contextually.
  A "house" in a farm scene also influences the area around animals because the
  model understands the scene holistically.
- TWO INDEPENDENT axes: Denoising step (WHEN in the 25-step generation process)
  and Network depth (WHERE in the 24-layer transformer stack).
- Shallow layers capture global composition planning, middle layers semantic
  assignment, deep layers fine detail. ALL layers are active at EVERY step.
- Early denoising steps show rough layout, late steps show detail refinement.
- Subword tokenization: The CLIP-L encoder splits unknown words into subtokens
  (e.g., "Kugel" → "Ku"+"gel"). The UI automatically groups these back into
  whole words, but users can see the subtoken count in parentheses.
- The B/W mode desaturates the base image so heatmap colors stand out clearly.
  "Off" hides the base image entirely to show pure attention patterns.

Common questions users might ask:
- "Why does 'house' also light up at the animals?" → The model processes the
  entire scene context, not isolated words. Semantically related concepts
  (farmhouse + animals) share attention. Try deeper layers for more precise localization.
- "Why is the heatmap so blurry?" → Attention maps are 64×64 (patch resolution),
  not pixel-precise. This IS the actual resolution the model works at internally.
- "What's the difference between the step slider and the depth buttons?" →
  Step = when in the noise-to-image process. Depth = where in the neural network.
  They are completely independent — explore all combinations.
- "Why does the same word look different at different steps?" → Early steps
  plan rough layout (broad, diffuse attention), late steps refine details
  (more focused attention). The model's "attention strategy" changes over time.

LATENT LAB — FEATURE PROBING
-----------------------------
Location: Route "/latent-lab" → Tab "Feature Probing"

What it does:
- Compares two prompts (A and B) to find which embedding dimensions encode
  the semantic difference between them
- Shows a bar chart of the top dimensions with the largest difference
- Users can selectively transfer specific dimensions from B into A
- Generates two side-by-side images: original A vs modified A (same seed)
- This reveals what specific embedding dimensions "encode" visually

How it works technically:
- SD3.5 has three text encoders: CLIP-L (768d), CLIP-G (1280d), T5-XXL (4096d)
- Users choose which encoder to probe
- Both prompts are encoded, then the per-dimension absolute difference is
  computed (averaged across all token positions)
- For transfer: selected dimensions in prompt A's embedding are replaced with
  the corresponding values from prompt B's embedding
- Same seed ensures the only visual change comes from the embedding modification

Key concepts to explain to users:
- Embeddings are HIGH-DIMENSIONAL vectors (4096 dimensions for T5). Each
  dimension is not a named "feature" — meaning is distributed across many
  dimensions simultaneously.
- Changing one word (e.g., "red" → "blue") changes MANY dimensions, not just
  one. The bar chart shows which changed MOST, but even small changes in
  many dims can have cumulative effects.
- The threshold slider is a quick way to select "all dimensions above X
  difference". Lower threshold = more dimensions transferred = closer to
  fully replacing A with B.
- T5-XXL (4096d) typically shows the richest semantic encoding. CLIP-L (768d)
  is more compressed. CLIP-G (1280d) is in between.
- Not all high-difference dimensions produce visible image changes. Some
  dimensions encode subtleties that the DiT denoiser doesn't strongly respond to.

Common questions users might ask:
- "Why doesn't transferring the top dimension change anything?" → Meaning is
  distributed. Try transferring the top 10 or 20 dimensions together. Often
  a semantic concept is spread across a cluster of dimensions.
- "Why do different encoders show different results?" → Each encoder learned
  different representations. T5 is a language model (4096d, token-level
  context), CLIP models are vision-language aligned (768/1280d, global
  sentence embedding). They encode semantics differently.
- "What does dimension 2847 'mean'?" → Individual dimensions don't have fixed
  semantic meanings. They represent directions in a learned vector space.
  The same dimension might encode color in one context and texture in another.
- "Can I transfer dimensions from a completely different prompt?" → Yes! That's
  the experimental spirit. Try transferring "ocean" dimensions into a "desert"
  prompt and see what happens.

================================================================================
SECTION 7: LORA TRAINING STUDIO
================================================================================

Users can train their OWN styles using the LoRA Training Studio.

How it works:
- Upload example images (your own artwork or photos)
- The system trains a LoRA (Low-Rank Adaptation) model
- The trained style becomes available for image generation
- Can be combined with any prompt

This allows users to:
- Create consistent personal styles
- Understand how AI learns from examples
- Explore transfer learning concepts

================================================================================
SECTION 8: SAFETY LEVELS AND CONTENT PROTECTION
================================================================================

Four safety levels exist:
- KIDS (Kinder): Most restricted, appropriate for ages 6-12
- YOUTH (Jugend): Default level, appropriate for ages 13-17
- ADULT (Erwachsene): Minimal filtering (18+), only illegal content blocked (§86a StGB)
- OFF: No filtering (development/testing only!)

The safety level is set by the supervising educator, not by students.
It determines what content can be generated.

SAFETY IS NOT ONE THING - IT HAS DIFFERENT MEANINGS AT DIFFERENT STAGES:

1. §86a StGB Compliance (Stage 1 - always active):
   - Blocks Nazi symbols, terrorist organization symbols, extremist codes
   - Required by German law, applies at ALL safety levels (even "adult")
   - Uses fast keyword filter + LLM verification

2. DSGVO/Privacy Protection (Stage 1 - always active):
   - Detects personal data (names, addresses) via SpaCy NER
   - Prevents personal data from reaching image generators
   - Legal requirement, applies at ALL safety levels

3. Age-Appropriate Content (Jugendschutz) (Stage 1 + Stage 3):
   - Checks if prompts contain age-inappropriate content
   - Different thresholds for kids vs youth
   - Skipped for "adult" and "off" levels

4. Post-Generation Image Safety (VLM check - after Stage 4):
   - NEW: A Vision-Language Model (qwen3-vl:2b) analyzes the GENERATED IMAGE
   - Text-based checks cannot predict what an image generator actually produces
   - A harmless prompt can produce a disturbing image
   - The VLM looks at the actual image and decides: "safe" or "unsafe"
   - Only runs for images (not code/audio/video) at kids/youth levels
   - If the VLM flags the image as unsafe, it is blocked before reaching the user
   - If the VLM service is unavailable, the image passes through (fail-open)

WHY MULTIPLE LAYERS?
- Stage 1+3 catch bad PROMPTS before wasting generation time
- VLM catches bad IMAGES that slipped through text checks
- §86a and DSGVO are LEGAL requirements, always active
- Jugendschutz is PEDAGOGICAL, adjustable per safety level

================================================================================
SECTION 9: DATA AND PRIVACY
================================================================================

- Generated content is saved for research purposes (helps improve the platform)
- Uploaded images are NOT permanently stored
- No personal data is collected
- The platform is used in supervised educational contexts

================================================================================
SECTION 10: FREQUENTLY ASKED QUESTIONS
================================================================================

Q: How long does generation take?
A: Depends on model and complexity. Usually 20 seconds to 2 minutes. Progress is shown during generation.

Q: Can I edit the AI's suggestions?
A: YES! Everything is editable - the interception result, the optimized prompt, even the rules. The AI provides suggestions, but YOU have full control.

Q: Why does my result look different each time?
A: The seed changes for variation. If you want to iterate on the same result, modify the prompt but keep the seed.

Q: What if I don't understand something?
A: Ask me (Trashy) for help, or ask the course instructor who is present. It's completely okay not to know everything.

Q: Can I use my own images?
A: Yes, in Image Mode (one image) or Multi-Image Mode (up to three images). You can also use images you generated on the platform.

Q: What is Canvas and when should I use it?
A: Canvas is a visual workflow builder where you connect nodes to create custom AI pipelines. Use Canvas when you want to: explore how different transformations affect results, compare multiple AI models side-by-side, create evaluation loops for quality control, or do systematic research on AI behavior. It's more advanced than the other modes but gives you complete control.

Q: What's the difference between Canvas and the other modes?
A: In Text/Image/Multi-Image modes, you select rules that apply automatically to your input. In Canvas, NO pre-selected rules apply - you build the entire process yourself using nodes. You decide what transformations happen, in what order, and can even create feedback loops.

Q: How do I start with Canvas?
A: Start simple! Create: Input → Interception → Generation → Collector. That's the basic pattern. Then add complexity: try parallel generations to compare models, add Preview nodes to see intermediate results, or experiment with Evaluation for quality control.

Q: What are evaluation nodes good for in Canvas?
A: Evaluation nodes let an LLM judge the output based on criteria you define (fairness, creativity, bias, quality, or custom). With branching enabled, you can create FEEDBACK LOOPS: if evaluation fails, the workflow can iterate and try again. This is powerful for bias exploration and systematic research.

Q: Can I create multiple images at once in Canvas?
A: Yes! Connect one node to MULTIPLE Generation nodes (fan-out). All results appear in the Collector. This is great for comparing how different AI models interpret the same prompt.

================================================================================
SECTION 11: INTERFACE ELEMENTS
================================================================================

HEADER ICONS (for mode recognition):
- Dots pattern: Start page / Selection view
- "A" with arrow: Text Mode
- Landscape image: Image Mode
- Grid with plus: Multi-Image Mode
- Account tree (branching diagram): Canvas Mode

INPUT FIELD STATES:
- Empty: White dashed border, pulsing yellow when required
- Filled: Blue solid border with blue background tint

BUTTONS:
- START buttons: Triple arrow animation, green when enabled, gray when disabled
- Return button: Top-left, returns to previous page or mode selection

OUTPUT FRAME STATES:
- Empty: Dashed border, placeholder text
- Generating: Solid green border, animated progress sprite
- Complete: Shows generated media (image, video, audio player)

================================================================================
SECTION 12: TIPS FOR USERS
================================================================================

FOR BETTER RESULTS:
- Be specific in your descriptions
- Try different rules for the same idea
- Compare results across different models
- Use the circularity - iterate and refine
- Don't be afraid to experiment with experimental workflows

FOR LEARNING:
- Observe what the LLM does with your input
- Compare your original idea with the expanded prompt
- Try to understand WHY certain results happen
- Formulate your own rules instead of only using presets

REMEMBER:
- This is an EXPERIMENTATION platform, not a production tool
- There are no "wrong" results - every output teaches something
- Ask questions! To me (Trashy) or to the instructor present.

================================================================================
SECTION 13: AVAILABLE PROPERTY CONFIGURATIONS
================================================================================

These are the pre-made configurations users can select on the start page.
Each configuration defines rules (HOW) that transform the user's idea (WHAT).

CATEGORIES:
- Freestyle: User defines their own rules
- Art History: Historical artistic movements
- Creative Transformations: Linguistic and semantic experiments
- Research/Experimental: Technical deep-dive workflows

---

FREESTYLE / USER-DEFINED
------------------------

"Du bestimmst!" / "Your Call!"
- Purpose: User writes their own rules from scratch
- Rules field starts EMPTY
- Maximum creative freedom
- Good for advanced users who want to experiment

---

ART HISTORY CONFIGURATIONS
--------------------------

"Bauhaus"
- Reduces everything to geometric shapes and primary colors
- Focuses on function and structure (Form follows function)
- Industrial materials: steel, glass, concrete
- Removes decoration and ornament

"Renaissance"
- Classical perspective and composition
- Emphasis on proportion and harmony
- Renaissance color palettes
- Humanistic ideals

"Confucian Literati" / "Konfuzianische Gelehrte"
- East Asian scholarly aesthetic
- Ink brush traditions
- Philosophical depth
- Contemplative compositions

---

CREATIVE TRANSFORMATIONS
------------------------

"Stille Post" / "Telephone"
- Passes text through 8 random languages
- Like the children's game "telephone"
- Creates surprising meaning shifts
- Reveals how translation changes concepts

"Das Gegenteil" / "The Opposite"
- Inverts the meaning of everything
- What was bright becomes dark
- Positive becomes negative
- Reveals hidden assumptions in prompts

"Hunky Dory Harmonizer"
- Makes everything positive and harmonious
- Removes conflict and tension
- Creates idealized, peaceful versions
- Shows how "positivity" changes meaning

"Clichéfilter"
- Detects and transforms visual clichés
- Finds alternative visual solutions
- Challenges default imagery
- Promotes original thinking

"Jugendsprache" / "Youth Language"
- Transforms into contemporary youth slang
- Current expressions and style
- Shows how language changes meaning

"Pig Latin" / "Schweinelatein"
- Linguistic transformation game
- Creates playful text modifications

---

PHOTOGRAPHY PERSPECTIVES
------------------------

"Analoge Fotografie 1870er" / "Analog Photography 1870s"
- Early photography aesthetics
- Long exposure limitations
- Chemical processes
- Historical techniques

"Analoge Fotografie 1970er" / "Analog Photography 1970s"
- Film photography aesthetics
- Color film characteristics
- Analog grain and texture

"Digitale Fotografie" / "Digital Photography"
- Modern digital aesthetics
- Sharp focus, high dynamic range
- Contemporary camera capabilities

---

TECHNICAL / ARTISTIC APPROACHES
-------------------------------

"Technische Zeichnung" / "Technical Drawing"
- Engineering drawing style
- Precise lines, measurements
- Schematic representations
- Blueprint aesthetics

"P5.js Simplifier"
- Creates p5.js code sketches
- Generative art approach
- Programmable visual output

"Planetarizer"
- Cosmic, planetary perspectives
- Scale transformations
- Astronomical viewpoints

---

EXPERIMENTAL RESEARCH WORKFLOWS
-------------------------------

These are advanced workflows that work at the technical level of AI systems:

"Surrealizer" / "Surrealisierer"
- Uses TWO text encoders (CLIP and T5) simultaneously
- Merges their different "understandings" of text
- Creates dreamlike, unexpected compositions
- SKIPS normal interception - prompt goes directly to specialized workflow
- Reveals how different AI systems encode meaning differently

"Split and Combine" / "Teilen und Kombinieren"
- Splits prompt into TWO semantic parts
- Creates FOUR images:
  1. Original scene with both elements
  2. First element alone
  3. Second element alone
  4. Recombined in new way
- Explores how concepts blend in neural networks
- SKIPS normal interception - direct workflow

"Partial Elimination" / "Partielle Elimination"
- "Switches off" parts of AI's understanding
- Creates THREE images:
  1. Normal version
  2. First half of semantic dimensions removed
  3. Second half removed
- Shows which features (objects, colors, moods) are stored where
- Makes AI's internal representations visible
- SKIPS normal interception - direct workflow

NOTE ON EXPERIMENTAL WORKFLOWS:
These skip the normal LLM interception step. The user's prompt goes directly
to specialized ComfyUI workflows that manipulate vectors at the mathematical
level. They are excellent for understanding HOW AI "thinks" about prompts.

---

MOOD/PERSPECTIVE CONFIGURATIONS
-------------------------------

"One World"
- Global, interconnected perspective
- Unity and diversity themes
- Cross-cultural elements

"Mad World"
- Chaotic, surreal perspective
- Unexpected combinations
- Breaking normal rules

"Sensibel" / "Sensitive"
- Delicate, nuanced approach
- Subtle details
- Careful, considerate perspective

"Kraftvoll" / "Forceful"
- Strong, powerful approach
- Bold statements
- High impact visuals

"Tell a Story" / "Erzähl eine Geschichte"
- Narrative focus
- Story elements
- Sequential thinking

---

IMAGE/MULTI-IMAGE CONFIGURATIONS
--------------------------------

"Image Transformation"
- For Image Mode (single image upload)
- User describes transformation to apply
- Vision model analyzes uploaded image

"Multi-Image Transformation"
- For Multi-Image Mode (up to 3 images)
- User describes how images should combine
- QWEN 2511 processes multiple images together

================================================================================
SECTION 14: TROUBLESHOOTING
================================================================================

COMMON ISSUES:

"Generation takes very long"
- Some models (especially video) need more time
- Check the progress indicator
- Wait for at least 2 minutes before assuming failure

"Result doesn't match my idea"
- Try being more specific in your description
- Modify the rules to add constraints
- Try a different model for comparison

"I don't understand the interception result"
- The LLM has combined your idea with the rules
- Compare it with your original input
- You can always edit it before generating

"The model keeps making similar images"
- Change the seed by clicking generate again (with same prompt)
- Or modify the prompt slightly for intentional variation

"My image upload doesn't work"
- Check file format (PNG, JPG, WEBP supported)
- File size should be reasonable
- Try a different image

================================================================================
SECTION 15: STAGE2 PROMPT QUALITY CRITERIA
================================================================================

The configurations (rules) that users select are called "Stage2 Interception Prompts".
They determine HOW user input is transformed. This section explains what tends to make
transformation prompts work well and can help explain to users why certain rules work better.

NOTE: These are guidelines for orientation, not rigid rules. Practical workshop experience
matters more than structural completeness. Use this as a thinking framework, not a checklist.

THE WAS/WIE PRINCIPLE
---------------------
- WAS (WHAT): The user's input - the core idea
- WIE (HOW): The config.context - the transformation rules

The LLM acts as a "Co-Akteur" (co-actor), not just a tool:
- It interprets user input according to explicit rules
- Applies the transformation framework consistently
- Adds its own "handwriting" through interpretive choices
- Results are collaborative, not mechanical

Stage2 prompts are NOT about making "better" prompts.
Stage2 prompts ARE about:
- Making visible HOW rules transform ideas
- Making editable - users can intervene at every step
- Making comparable - different rules on same idea
- Making criticalizable - students discuss transformation choices

THE 3-PART TEMPLATE STRUCTURE
-----------------------------
All Stage2 prompts work within this structure:

Task:        Generic instruction (HOW to transform)
Context:     Specific rules from the config
Important:   Respond in the same language as the input prompt below.
Prompt:      User's original text

QUALITY CRITERIA FOR TRANSFORMATION RULES
-----------------------------------------

1. PERSPECTIVE-TAKING OVER IMITATION
   GOOD: "DU BIST eine Künstlerin der 1870er Jahre. Du lebst in..."
   BAD:  "Generate in vintage style"
   FORBIDDEN: "in the style of [artist name]"

2. CLEAR METHODOLOGY WITH STEPS
   GOOD: "1. Analysiere... 2. Wende an... 3. Reduziere... 4. Beschreibe..."
   BAD:  "Be creative and make something beautiful"

3. SPECIFIC PROHIBITIONS
   GOOD: "FORBIDDEN: romanticizing terms ('nostalgic glow'), digital terminology"
   BAD:  "Don't be too kitschy"

4. OUTPUT FORMAT SPECIFICATION
   GOOD: "One paragraph, 120-180 words"
   BAD:  No length hint at all

5. META-COMMENTARY PROHIBITION
   GOOD: "NO meta-commentary ('I will...', 'This shows...')"
   Missing this causes LLM to explain instead of transform

RED FLAGS (require immediate revision)
--------------------------------------
- Placeholder text ("professional translator", "prompting expert")
- Artist name as style reference ("in the style of Monet")
- Empty context fields
- No prohibition of meta-commentary
- Optimization framing ("make better", "improve")

QUALITY LEVELS (rough orientation)
-----------------------------------
Excellent: Complete methodology, examples, prohibitions, output format
Good: Functional with minor improvements possible
Weak: Missing key elements, unclear transformation
Problematic: Placeholder text, no guidance, or poor practical results

GOLD STANDARD EXAMPLES
----------------------

PHOTOGRAPHY (analog_photography_1870s):
- Role identity: "Du bist eine professionelle Fotografin..."
- Material constraints: "silver-iodized copper plates, mercury development"
- Prohibitions: "FORBIDDEN: romanticizing terms, 'in the style of [photographer]'"
- Format: "One paragraph, 120-180 words"

ART HISTORY (bauhaus):
- 4-step methodology: Analysis → Functionalism → Geometric Reduction → Material/Color
- Perspective prohibition: "ES IST DIR VERBOTEN... 'Im Stil von'"
- Transformation examples: "Vase → functional prototype for cylindrical container"

CRITICAL THINKING (planetarizer):
- Clear framework: Anthropocene thinking, planetary perspective
- Strict prohibitions: "Othering, exoticization STRICTLY forbidden"
- Named theoretical basis: "Frantz Fanon, Homi Bhabha, Gayatri Spivak"

SPECIAL CASES
-------------

PASSTHROUGH CONFIGS (skip Stage2):
- Surrealizer, Split & Combine, Partial Elimination
- These have minimal/empty contexts BY DESIGN
- They pass prompts directly to specialized ComfyUI workflows

USER-DEFINED CONFIG:
- "Du bestimmst!" has empty context BY DESIGN
- User provides their own rules via UI

LORA-TRIGGER CONFIGS (e.g., Cooked Negatives):
- Must include trigger term instruction to activate the LORA
- Can be brief if LORA handles the aesthetic

WHY THIS MATTERS FOR USERS
--------------------------
When users ask about why certain configurations work better:
- Good configs ADOPT A PERSPECTIVE, not just surface style
- Good configs have CLEAR STEPS the LLM can follow
- Good configs PROHIBIT common failure modes explicitly
- Good configs specify OUTPUT FORMAT so results are usable

When users want to write their own rules:
- Start with role identity: "Du bist..."
- Add numbered steps or phases
- Include specific prohibitions for what NOT to do
- Specify output length and format
- Ensure the rules work for ANY input (WAS), not just specific content

================================================================================
END OF KNOWLEDGE BASE
================================================================================
