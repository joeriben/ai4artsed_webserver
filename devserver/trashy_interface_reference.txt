AI4ARTSED KNOWLEDGE BASE FOR TRASHY - COMPREHENSIVE USER GUIDE
Last updated: 2026-01-19

================================================================================
SECTION 1: PLATFORM OVERVIEW
================================================================================

AI4ArtsEd is a pedagogical-scientific experimentation platform for critical and creative engagement with generative AI in cultural education (ages 8-17). It is used in art-AI workshops with educators present.

CORE CONCEPT: The Large Language Model (LLM) acts as a co-actor in the creative process, not just a tool. It processes input based on its training data and creates something new. This is fascinating but also problematic - we don't know exactly how or why the model makes certain decisions.

KEY QUESTIONS WE EXPLORE:
- How do different models react to different inputs?
- What happens with detailed, nuanced descriptions instead of short prompts?
- How can we learn to truly understand what we want?
- How can we understand and change our image idea from many different perspectives?

================================================================================
SECTION 2: PLATFORM STRUCTURE
================================================================================

START PAGE (SELECT VIEW)
------------------------
Location: Route "/" (root)
Purpose: Users select RULES that will apply to their experiment.

CENTRAL ELEMENT: "Du bestimmst!" / "Your Call!"
- This is in the CENTER of the page
- It starts with an EMPTY rules box
- Users can write their own rules from scratch here

SURROUNDING ELEMENTS: Pre-made rule suggestions
- Everything around "Du bestimmst!" are INSPIRATIONS
- Users can select one to use as a starting point
- Or they can use these as examples to learn how rules work

The selected rules define:
- Creative constraints
- Style approaches
- Material specifications
- Perspective guidelines

IMPORTANT: The selected rules are automatically applied to all three modes - users don't need to re-enter them.

WHAT HAPPENS WITH THE RULES:
- A text AI will process the user's idea (WHAT) together with the rules (HOW)
- This creates an expanded, differentiated prompt
- The result can be:
  - Edited directly in the box
  - Regenerated by clicking "Start" again
  - Copied and pasted into the prompt box for further work

THREE MODES
-----------

IMPORTANT: The selected RULES apply in ALL THREE modes!
The difference between modes is WHAT you input (text vs images).

1. TEXT MODE (text_transformation.vue, route /pipeline-execution/:configId)
   Icon: "A" with arrow (translation symbol)
   Input: TEXT (your idea)
   How it works:
   - User enters an IDEA as text (WAS/WHAT should be created)
   - The text AI processes the idea together with the RULES
   - Creates an expanded prompt
   - User can observe, understand, and modify all intermediate steps
   - User selects output medium (image, video, audio, music)
   - User selects a generation model
   - Media is generated

2. IMAGE MODE (image_transformation.vue, route /image-transformation/:configId)
   Icon: Landscape/image symbol
   Input: ONE IMAGE (replaces the text prompt)
   How it works:
   - User uploads ONE image - this REPLACES the text prompt
   - User describes what should happen to it
   - The RULES still apply to the transformation
   - Vision model analyzes the uploaded image
   - Model creates a new version based on description + rules

3. MULTI-IMAGE MODE (multi_image.vue)
   Icon: Grid with plus symbol
   Input: UP TO THREE IMAGES (replace the text prompt together)
   How it works:
   - User uploads UP TO THREE images - these REPLACE the text prompt
   - User describes how they should be combined
   - The RULES still apply to the combination
   - Vision-capable model (QWEN 2511) processes all images together
   - Creates fusion based on description + rules

================================================================================
SECTION 3: TEXT MODE - DETAILED WORKFLOW
================================================================================

STEP 1: INPUT FIELDS (Two side-by-side bubbles)

LEFT BUBBLE - "Deine Idee" (Your Idea)
- Label: "Deine Idee: Worum soll es gehen?" / "Your Idea: What should it be about?"
- Purpose: User describes the SUBJECT MATTER - WHAT should be created
- Always editable, no restrictions
- When filled: border changes from white (low opacity) to blue with blue background

RIGHT BUBBLE - "Regeln/Eigenschaften" (Rules/Properties)
- Label: "Bestimme Regeln, Material, Besonderheiten" / "Define rules, materials, specifics"
- Purpose: Contains the meta-prompt defining HOW it should be realized
- Initially loaded from selected configuration in user's language
- Always editable - user can modify or write completely new rules
- IMPORTANT: The goal is for users to learn to formulate their OWN rules

STEP 2: INTERCEPTION (Verarbeitung)

What happens:
- User clicks first START button (triple arrow)
- LLM (Language Model) processes BOTH inputs together
- Combines the IDEA with the RULES
- Creates an expanded, differentiated prompt

Result display:
- Label: "Idee + Regeln = Prompt"
- Shows the LLM's output in a green-tinted box
- FULLY EDITABLE - user can modify everything
- This is a SUGGESTION, not a final result

WHY INTERCEPTION EXISTS:
1. Awareness of rules and material - By explicitly formulating rules, users become aware of what decisions they're making
2. Learning to understand prompts - Same basic idea can produce completely different results with different rules
3. Critical exploration of AI - How do models react to detailed, expressive prompts? Where are their limits?

STEP 3: MEDIA AND MODEL SELECTION

The platform is MULTIMODAL - not just images, but also:
- Images (various models)
- Video (LTX Video, Wan 2.2)
- Audio/Sound (Stable Audio)
- Music/Instrumental (ACE Instrumental)

AVAILABLE IMAGE MODELS:
- Stable Diffusion 3.5 Large (sd35_large) - blue, local
- GPT Image 1 (gpt_image_1) - yellow, OpenAI
- Gemini 3 Pro Image (gemini_3_pro_image) - blue, Google
- QWEN (qwen) - purple, vision-capable

VIDEO MODELS:
- LTX Video - fast local video generation
- Wan 2.2 Video - high-quality 720p video (5B model)

AUDIO MODELS:
- ACE Instrumental - AI music generation for instrumentals
- Stable Audio Open - open-source audio generation (max 47s)

THE EXCITING PART: Users can try the SAME prompt with DIFFERENT models - within one medium or even across media. This teaches how differently models react to the same input.

STEP 4: PROMPT OPTIMIZATION (depends on the model)

Not all models need the same prompt style. Here's what happens:

STABLE DIFFUSION 3.5:
- NEEDS optimization
- Prompt is converted to "classic" prompt style (keywords, weightings, tags)
- This is a good opportunity to LEARN how traditional prompting works
- Users can see how a natural description becomes a technical prompt

GPT IMAGE, GEMINI, QWEN:
- DON'T NEED optimization
- These models have their own POWERFUL LANGUAGE MODULE built-in
- They understand natural language directly
- The interception result goes directly to the model

VIDEO MODELS (LTX, Wan 2.2):
- Receive optimization for SCENIC descriptions
- Prompts are adapted for movement, timing, camera work

AUDIO MODELS (ACE, Stable Audio):
- Receive optimization for AUDITIVE descriptions
- Prompts are adapted for sound, rhythm, musical structure

P5.JS (Special case):
- This generates CODE, not images
- Optimization prepares the prompt for code generation
- Output is a p5.js sketch that runs in the browser

Display:
- Label: "Modell-Optimierter Prompt"
- Shows in orange-tinted box
- FULLY EDITABLE - the optimized prompt is just a suggestion
- Users can always modify it before generation

STEP 5: TRANSLATION TO ENGLISH

At the end, the prompt is translated to English (if not already).
WHY? Because generative AI models usually only understand English really well.
We want users to work in their own language - so the system handles translation.
This step is currently not visible but happens in the background.

STEP 6: GENERATION

User clicks second START button to generate media.
- Progress animation shows during generation
- Duration: typically 20 seconds to 2 minutes depending on model
- Result displays in output frame
- Images can be clicked for fullscreen view
- Videos/audio have playback controls

================================================================================
SECTION 4: CIRCULARITY CONCEPT
================================================================================

Even though the workflow LOOKS linear (top to bottom), the platform is designed to be CIRCULAR:

- Go back at any time
- Copy any text and paste it elsewhere
- Load generated images into Image Mode or Multi-Image Mode
- Continue processing and iterating
- Experiment!

EXAMPLE OF CIRCULAR WORKFLOW:
You can transform a prompt with the "Cutifier" (Verniedlicher), copy the result back up to the input,
then go back to the start page and apply a completely different rule like "De-Kitschifier" (Entkitscher),
then do the same again with "Exaggerator" (Übertreiber), and so on.
And in between, YOU can always change something yourself - add words, remove parts, refine ideas.
This chain of transformations creates unique, layered results.

The seed logic supports both:
- VARIATION: Same prompt → new seed → different result
- ITERATION: Modified prompt → same seed → refined result

================================================================================
SECTION 5: THE WAS/WIE PRINCIPLE (WHAT/HOW)
================================================================================

In text mode, users work with TWO SEPARATE inputs:
- WAS (WHAT): The idea - what should be created
- WIE (HOW): The rules - how should it be realized

There are pre-made configurations as assistance, but the GOAL is for users to learn to formulate their OWN rules.

EXAMPLE RULES:
- "Beschreibe alles aus der Perspektive der Vögel auf den Bäumen" (Describe everything from the perspective of birds in the trees)
- "Verwende nur Farben, die man in der Natur findet" (Use only colors found in nature)
- "Stelle dir vor, du erklärst es jemandem, der noch nie ein Bild gesehen hat" (Imagine explaining it to someone who has never seen an image)

================================================================================
SECTION 6: EXPERIMENTAL WORKFLOWS
================================================================================

These approaches work at a deeper technical level and make the "black box" somewhat visible:

SURREALIZER
-----------
Uses TWO DIFFERENT text encoders (CLIP and T5) and merges their results.
- Each encoder "understands" text differently
- Mixing them creates unexpected, often surreal results
- Reveals how meaning is encoded differently by different systems

SPLIT & COMBINE / PARTIAL ELIMINATION
-------------------------------------
Work directly at the mathematical level of semantic vectors.
- What happens when we merge meaning spaces?
- What happens when we remove parts of meaning?
- These experiments make AI's internal representations somewhat visible

================================================================================
SECTION 7: LORA TRAINING STUDIO
================================================================================

Users can train their OWN styles using the LoRA Training Studio.

How it works:
- Upload example images (your own artwork or photos)
- The system trains a LoRA (Low-Rank Adaptation) model
- The trained style becomes available for image generation
- Can be combined with any prompt

This allows users to:
- Create consistent personal styles
- Understand how AI learns from examples
- Explore transfer learning concepts

================================================================================
SECTION 8: SAFETY LEVELS
================================================================================

Three safety levels exist:
- KIDS (Kinder): Most restricted, appropriate for ages 8-12
- YOUTH (Jugend): Default level, appropriate for ages 12-17
- OPEN (Erwachsene/Adults): Least restricted, for adult supervision contexts

The safety level is set by the supervising educator, not by students.
It determines what content can be generated.

================================================================================
SECTION 9: DATA AND PRIVACY
================================================================================

- Generated content is saved for research purposes (helps improve the platform)
- Uploaded images are NOT permanently stored
- No personal data is collected
- The platform is used in supervised educational contexts

================================================================================
SECTION 10: FREQUENTLY ASKED QUESTIONS
================================================================================

Q: How long does generation take?
A: Depends on model and complexity. Usually 20 seconds to 2 minutes. Progress is shown during generation.

Q: Can I edit the AI's suggestions?
A: YES! Everything is editable - the interception result, the optimized prompt, even the rules. The AI provides suggestions, but YOU have full control.

Q: Why does my result look different each time?
A: The seed changes for variation. If you want to iterate on the same result, modify the prompt but keep the seed.

Q: What if I don't understand something?
A: Ask me (Trashy) for help, or ask the course instructor who is present. It's completely okay not to know everything.

Q: Can I use my own images?
A: Yes, in Image Mode (one image) or Multi-Image Mode (up to three images). You can also use images you generated on the platform.

================================================================================
SECTION 11: INTERFACE ELEMENTS
================================================================================

HEADER ICONS (for mode recognition):
- Dots pattern: Start page / Selection view
- "A" with arrow: Text Mode
- Landscape image: Image Mode
- Grid with plus: Multi-Image Mode

INPUT FIELD STATES:
- Empty: White dashed border, pulsing yellow when required
- Filled: Blue solid border with blue background tint

BUTTONS:
- START buttons: Triple arrow animation, green when enabled, gray when disabled
- Return button: Top-left, returns to previous page or mode selection

OUTPUT FRAME STATES:
- Empty: Dashed border, placeholder text
- Generating: Solid green border, animated progress sprite
- Complete: Shows generated media (image, video, audio player)

================================================================================
SECTION 12: TIPS FOR USERS
================================================================================

FOR BETTER RESULTS:
- Be specific in your descriptions
- Try different rules for the same idea
- Compare results across different models
- Use the circularity - iterate and refine
- Don't be afraid to experiment with experimental workflows

FOR LEARNING:
- Observe what the LLM does with your input
- Compare your original idea with the expanded prompt
- Try to understand WHY certain results happen
- Formulate your own rules instead of only using presets

REMEMBER:
- This is an EXPERIMENTATION platform, not a production tool
- There are no "wrong" results - every output teaches something
- Ask questions! To me (Trashy) or to the instructor present.

================================================================================
SECTION 13: AVAILABLE PROPERTY CONFIGURATIONS
================================================================================

These are the pre-made configurations users can select on the start page.
Each configuration defines rules (HOW) that transform the user's idea (WHAT).

CATEGORIES:
- Freestyle: User defines their own rules
- Art History: Historical artistic movements
- Creative Transformations: Linguistic and semantic experiments
- Research/Experimental: Technical deep-dive workflows

---

FREESTYLE / USER-DEFINED
------------------------

"Du bestimmst!" / "Your Call!"
- Purpose: User writes their own rules from scratch
- Rules field starts EMPTY
- Maximum creative freedom
- Good for advanced users who want to experiment

---

ART HISTORY CONFIGURATIONS
--------------------------

"Bauhaus"
- Reduces everything to geometric shapes and primary colors
- Focuses on function and structure (Form follows function)
- Industrial materials: steel, glass, concrete
- Removes decoration and ornament

"Renaissance"
- Classical perspective and composition
- Emphasis on proportion and harmony
- Renaissance color palettes
- Humanistic ideals

"Confucian Literati" / "Konfuzianische Gelehrte"
- East Asian scholarly aesthetic
- Ink brush traditions
- Philosophical depth
- Contemplative compositions

---

CREATIVE TRANSFORMATIONS
------------------------

"Stille Post" / "Telephone"
- Passes text through 8 random languages
- Like the children's game "telephone"
- Creates surprising meaning shifts
- Reveals how translation changes concepts

"Das Gegenteil" / "The Opposite"
- Inverts the meaning of everything
- What was bright becomes dark
- Positive becomes negative
- Reveals hidden assumptions in prompts

"Hunky Dory Harmonizer"
- Makes everything positive and harmonious
- Removes conflict and tension
- Creates idealized, peaceful versions
- Shows how "positivity" changes meaning

"Clichéfilter"
- Detects and transforms visual clichés
- Finds alternative visual solutions
- Challenges default imagery
- Promotes original thinking

"Jugendsprache" / "Youth Language"
- Transforms into contemporary youth slang
- Current expressions and style
- Shows how language changes meaning

"Pig Latin" / "Schweinelatein"
- Linguistic transformation game
- Creates playful text modifications

---

PHOTOGRAPHY PERSPECTIVES
------------------------

"Analoge Fotografie 1870er" / "Analog Photography 1870s"
- Early photography aesthetics
- Long exposure limitations
- Chemical processes
- Historical techniques

"Analoge Fotografie 1970er" / "Analog Photography 1970s"
- Film photography aesthetics
- Color film characteristics
- Analog grain and texture

"Digitale Fotografie" / "Digital Photography"
- Modern digital aesthetics
- Sharp focus, high dynamic range
- Contemporary camera capabilities

---

TECHNICAL / ARTISTIC APPROACHES
-------------------------------

"Technische Zeichnung" / "Technical Drawing"
- Engineering drawing style
- Precise lines, measurements
- Schematic representations
- Blueprint aesthetics

"P5.js Simplifier"
- Creates p5.js code sketches
- Generative art approach
- Programmable visual output

"Planetarizer"
- Cosmic, planetary perspectives
- Scale transformations
- Astronomical viewpoints

---

EXPERIMENTAL RESEARCH WORKFLOWS
-------------------------------

These are advanced workflows that work at the technical level of AI systems:

"Surrealizer" / "Surrealisierer"
- Uses TWO text encoders (CLIP and T5) simultaneously
- Merges their different "understandings" of text
- Creates dreamlike, unexpected compositions
- SKIPS normal interception - prompt goes directly to specialized workflow
- Reveals how different AI systems encode meaning differently

"Split and Combine" / "Teilen und Kombinieren"
- Splits prompt into TWO semantic parts
- Creates FOUR images:
  1. Original scene with both elements
  2. First element alone
  3. Second element alone
  4. Recombined in new way
- Explores how concepts blend in neural networks
- SKIPS normal interception - direct workflow

"Partial Elimination" / "Partielle Elimination"
- "Switches off" parts of AI's understanding
- Creates THREE images:
  1. Normal version
  2. First half of semantic dimensions removed
  3. Second half removed
- Shows which features (objects, colors, moods) are stored where
- Makes AI's internal representations visible
- SKIPS normal interception - direct workflow

NOTE ON EXPERIMENTAL WORKFLOWS:
These skip the normal LLM interception step. The user's prompt goes directly
to specialized ComfyUI workflows that manipulate vectors at the mathematical
level. They are excellent for understanding HOW AI "thinks" about prompts.

---

MOOD/PERSPECTIVE CONFIGURATIONS
-------------------------------

"One World"
- Global, interconnected perspective
- Unity and diversity themes
- Cross-cultural elements

"Mad World"
- Chaotic, surreal perspective
- Unexpected combinations
- Breaking normal rules

"Sensibel" / "Sensitive"
- Delicate, nuanced approach
- Subtle details
- Careful, considerate perspective

"Kraftvoll" / "Forceful"
- Strong, powerful approach
- Bold statements
- High impact visuals

"Tell a Story" / "Erzähl eine Geschichte"
- Narrative focus
- Story elements
- Sequential thinking

---

IMAGE/MULTI-IMAGE CONFIGURATIONS
--------------------------------

"Image Transformation"
- For Image Mode (single image upload)
- User describes transformation to apply
- Vision model analyzes uploaded image

"Multi-Image Transformation"
- For Multi-Image Mode (up to 3 images)
- User describes how images should combine
- QWEN 2511 processes multiple images together

================================================================================
SECTION 14: TROUBLESHOOTING
================================================================================

COMMON ISSUES:

"Generation takes very long"
- Some models (especially video) need more time
- Check the progress indicator
- Wait for at least 2 minutes before assuming failure

"Result doesn't match my idea"
- Try being more specific in your description
- Modify the rules to add constraints
- Try a different model for comparison

"I don't understand the interception result"
- The LLM has combined your idea with the rules
- Compare it with your original input
- You can always edit it before generating

"The model keeps making similar images"
- Change the seed by clicking generate again (with same prompt)
- Or modify the prompt slightly for intentional variation

"My image upload doesn't work"
- Check file format (PNG, JPG, WEBP supported)
- File size should be reasonable
- Try a different image

================================================================================
SECTION 15: STAGE2 PROMPT QUALITY CRITERIA
================================================================================

The configurations (rules) that users select are called "Stage2 Interception Prompts".
They determine HOW user input is transformed. This section explains what tends to make
transformation prompts work well and can help explain to users why certain rules work better.

NOTE: These are guidelines for orientation, not rigid rules. Practical workshop experience
matters more than structural completeness. Use this as a thinking framework, not a checklist.

THE WAS/WIE PRINCIPLE
---------------------
- WAS (WHAT): The user's input - the core idea
- WIE (HOW): The config.context - the transformation rules

The LLM acts as a "Co-Akteur" (co-actor), not just a tool:
- It interprets user input according to explicit rules
- Applies the transformation framework consistently
- Adds its own "handwriting" through interpretive choices
- Results are collaborative, not mechanical

Stage2 prompts are NOT about making "better" prompts.
Stage2 prompts ARE about:
- Making visible HOW rules transform ideas
- Making editable - users can intervene at every step
- Making comparable - different rules on same idea
- Making criticalizable - students discuss transformation choices

THE 3-PART TEMPLATE STRUCTURE
-----------------------------
All Stage2 prompts work within this structure:

Task:        Generic instruction (HOW to transform)
Context:     Specific rules from the config
Important:   Respond in the same language as the input prompt below.
Prompt:      User's original text

QUALITY CRITERIA FOR TRANSFORMATION RULES
-----------------------------------------

1. PERSPECTIVE-TAKING OVER IMITATION
   GOOD: "DU BIST eine Künstlerin der 1870er Jahre. Du lebst in..."
   BAD:  "Generate in vintage style"
   FORBIDDEN: "in the style of [artist name]"

2. CLEAR METHODOLOGY WITH STEPS
   GOOD: "1. Analysiere... 2. Wende an... 3. Reduziere... 4. Beschreibe..."
   BAD:  "Be creative and make something beautiful"

3. SPECIFIC PROHIBITIONS
   GOOD: "FORBIDDEN: romanticizing terms ('nostalgic glow'), digital terminology"
   BAD:  "Don't be too kitschy"

4. OUTPUT FORMAT SPECIFICATION
   GOOD: "One paragraph, 120-180 words"
   BAD:  No length hint at all

5. META-COMMENTARY PROHIBITION
   GOOD: "NO meta-commentary ('I will...', 'This shows...')"
   Missing this causes LLM to explain instead of transform

RED FLAGS (require immediate revision)
--------------------------------------
- Placeholder text ("professional translator", "prompting expert")
- Artist name as style reference ("in the style of Monet")
- Empty context fields
- No prohibition of meta-commentary
- Optimization framing ("make better", "improve")

QUALITY LEVELS (rough orientation)
-----------------------------------
Excellent: Complete methodology, examples, prohibitions, output format
Good: Functional with minor improvements possible
Weak: Missing key elements, unclear transformation
Problematic: Placeholder text, no guidance, or poor practical results

GOLD STANDARD EXAMPLES
----------------------

PHOTOGRAPHY (analog_photography_1870s):
- Role identity: "Du bist eine professionelle Fotografin..."
- Material constraints: "silver-iodized copper plates, mercury development"
- Prohibitions: "FORBIDDEN: romanticizing terms, 'in the style of [photographer]'"
- Format: "One paragraph, 120-180 words"

ART HISTORY (bauhaus):
- 4-step methodology: Analysis → Functionalism → Geometric Reduction → Material/Color
- Perspective prohibition: "ES IST DIR VERBOTEN... 'Im Stil von'"
- Transformation examples: "Vase → functional prototype for cylindrical container"

CRITICAL THINKING (planetarizer):
- Clear framework: Anthropocene thinking, planetary perspective
- Strict prohibitions: "Othering, exoticization STRICTLY forbidden"
- Named theoretical basis: "Frantz Fanon, Homi Bhabha, Gayatri Spivak"

SPECIAL CASES
-------------

PASSTHROUGH CONFIGS (skip Stage2):
- Surrealizer, Split & Combine, Partial Elimination
- These have minimal/empty contexts BY DESIGN
- They pass prompts directly to specialized ComfyUI workflows

USER-DEFINED CONFIG:
- "Du bestimmst!" has empty context BY DESIGN
- User provides their own rules via UI

LORA-TRIGGER CONFIGS (e.g., Cooked Negatives):
- Must include trigger term instruction to activate the LORA
- Can be brief if LORA handles the aesthetic

WHY THIS MATTERS FOR USERS
--------------------------
When users ask about why certain configurations work better:
- Good configs ADOPT A PERSPECTIVE, not just surface style
- Good configs have CLEAR STEPS the LLM can follow
- Good configs PROHIBIT common failure modes explicitly
- Good configs specify OUTPUT FORMAT so results are usable

When users want to write their own rules:
- Start with role identity: "Du bist..."
- Add numbered steps or phases
- Include specific prohibitions for what NOT to do
- Specify output length and format
- Ensure the rules work for ANY input (WAS), not just specific content

================================================================================
END OF KNOWLEDGE BASE
================================================================================
