#!/usr/bin/env python3
"""
Full 4-Stage Pipeline Statistics Test
Tests 1500 prompts (3 x 500) through COMPLETE pipeline:
  Stage 1a: Translation (DE‚ÜíEN)
  Stage 1b: Safety Filter (hybrid: fast filter + llama-guard)
  Stage 2:  Interception (dada)
  Stage 3:  Pre-Output Safety (hybrid: fast filter + llama-guard)

This tests the REAL system behavior with translation!
"""
import sys
from pathlib import Path
import time
from collections import Counter
import json
from datetime import datetime
import asyncio
sys.path.insert(0, str(Path(__file__).parent.parent))

from schemas.engine.pipeline_executor import PipelineExecutor

def load_prompts(filepath):
    """Load prompts from text file (one per line)"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f if line.strip()]

async def test_prompt_set(prompts, set_name, executor, safety_level='kids'):
    """Test a set of prompts through full 4-stage pipeline"""
    passed = 0
    blocked_stage1 = 0
    blocked_stage3 = 0
    errors = 0

    stage1_triggered_count = 0
    stage3_triggered_count = 0

    translations = []
    blocked_prompts = []
    error_prompts = []

    total_time = 0
    translation_time = 0
    stage1_time = 0
    stage2_time = 0
    stage3_time = 0

    print(f"\n{'='*80}")
    print(f"Testing: {set_name} ({len(prompts)} prompts)")
    print(f"{'='*80}")

    for i, prompt in enumerate(prompts, 1):
        if i % 50 == 0:
            print(f"  Progress: {i}/{len(prompts)} ({i/len(prompts)*100:.1f}%)")

        start_time = time.time()

        try:
            result = await executor.execute_pipeline(
                'dada',
                prompt,
                execution_mode='eco',
                safety_level=safety_level
            )

            elapsed = time.time() - start_time
            total_time += elapsed

            # Extract timing from metadata if available
            if 'stage_1a_time' in result.metadata:
                translation_time += result.metadata['stage_1a_time']
            if 'stage_1_time' in result.metadata:
                stage1_time += result.metadata['stage_1_time']
            if 'stage_2_time' in result.metadata:
                stage2_time += result.metadata['stage_2_time']
            if 'stage_3_time' in result.metadata:
                stage3_time += result.metadata['stage_3_time']

            if result.success:
                passed += 1

                # Track if Stage 1 filter was triggered (slow path)
                if result.metadata.get('stage_1_method') == 'llama_guard':
                    stage1_triggered_count += 1

                # Track if Stage 3 filter was triggered (slow path)
                if result.metadata.get('stage_3_method') == 'llama_guard':
                    stage3_triggered_count += 1

                # Store translation for analysis
                if 'stage_1a_translation' in result.metadata:
                    translations.append({
                        'original': prompt,
                        'translated': result.metadata['stage_1a_translation']
                    })
            else:
                # Check which stage blocked
                error_msg = result.error or ''

                if 'stage 1' in error_msg.lower() or 'harmful content' in error_msg.lower():
                    blocked_stage1 += 1
                elif 'stage 3' in error_msg.lower() or 'age-inappropriate' in error_msg.lower():
                    blocked_stage3 += 1
                else:
                    errors += 1

                blocked_prompts.append({
                    'prompt': prompt,
                    'stage': 'stage1' if 'stage 1' in error_msg.lower() else 'stage3' if 'stage 3' in error_msg.lower() else 'unknown',
                    'error': error_msg,
                    'translation': result.metadata.get('stage_1a_translation', 'N/A')
                })

        except Exception as e:
            errors += 1
            error_prompts.append({
                'prompt': prompt,
                'error': str(e)
            })

    print(f"  ‚úì Completed: {len(prompts)} prompts in {total_time:.1f}s")

    return {
        'set_name': set_name,
        'total': len(prompts),
        'passed': passed,
        'blocked_stage1': blocked_stage1,
        'blocked_stage3': blocked_stage3,
        'errors': errors,
        'stage1_triggered_count': stage1_triggered_count,
        'stage3_triggered_count': stage3_triggered_count,
        'total_time': total_time,
        'avg_time_per_prompt': total_time / len(prompts) if prompts else 0,
        'translation_time': translation_time,
        'stage1_time': stage1_time,
        'stage2_time': stage2_time,
        'stage3_time': stage3_time,
        'translations': translations[:10],  # First 10
        'blocked_prompts': blocked_prompts,
        'error_prompts': error_prompts
    }

async def main():
    testfiles_dir = Path(__file__).parent.parent / 'testfiles'

    files = {
        'harmlos': testfiles_dir / 'harmlos_kinder_jugend_prompts_500.txt',
        'probe_safe': testfiles_dir / 'probe_prompts_500_safe.txt',
        'provokant_safe': testfiles_dir / 'provokant_probe_prompts_500_safe.txt'
    }

    print("="*80)
    print("FULL 4-STAGE PIPELINE STATISTICS TEST")
    print("="*80)
    print("\nPipeline stages:")
    print("  Stage 1a: Translation (DE‚ÜíEN) via mistral-nemo")
    print("  Stage 1b: Safety Filter (hybrid: fast filter ‚Üí llama-guard if triggered)")
    print("  Stage 2:  Interception (dada)")
    print("  Stage 3:  Pre-Output Safety (hybrid: fast filter ‚Üí llama-guard if triggered)")
    print()
    print("‚ö†Ô∏è  This will take 1-2 hours for 1500 prompts!")
    print()

    # Initialize executor once
    executor = PipelineExecutor(Path(__file__).parent.parent / 'schemas')

    results = {}
    all_blocked = []
    all_errors = []

    global_start = time.time()

    for set_name, filepath in files.items():
        if not filepath.exists():
            print(f"‚ùå File not found: {filepath}")
            continue

        print(f"\nüìÇ Loading: {set_name} ({filepath.name})")
        prompts = load_prompts(filepath)
        print(f"   Loaded: {len(prompts)} prompts")

        result = await test_prompt_set(prompts, set_name, executor, safety_level='kids')
        results[set_name] = result

        # Collect blocked prompts with category info
        for blocked in result['blocked_prompts']:
            all_blocked.append({
                'category': set_name,
                'prompt': blocked['prompt'],
                'stage': blocked['stage'],
                'error': blocked['error'],
                'translation': blocked['translation']
            })

        # Collect errors
        all_errors.extend(result['error_prompts'])

    global_elapsed = time.time() - global_start

    print("\n" + "="*80)
    print("DETAILED STATISTICS")
    print("="*80)

    for set_name, result in results.items():
        print(f"\nüìä {set_name.upper()}")
        print(f"   Total prompts: {result['total']}")
        print(f"   Passed: {result['passed']} ({result['passed']/result['total']*100:.1f}%)")
        print(f"   Blocked Stage 1: {result['blocked_stage1']} ({result['blocked_stage1']/result['total']*100:.1f}%)")
        print(f"   Blocked Stage 3: {result['blocked_stage3']} ({result['blocked_stage3']/result['total']*100:.1f}%)")
        print(f"   Errors: {result['errors']} ({result['errors']/result['total']*100:.1f}%)")
        print()
        print(f"   Stage 1 LLM calls: {result['stage1_triggered_count']} ({result['stage1_triggered_count']/result['total']*100:.1f}%)")
        print(f"   Stage 3 LLM calls: {result['stage3_triggered_count']} ({result['stage3_triggered_count']/result['total']*100:.1f}%)")
        print()
        print(f"   Total time: {result['total_time']:.1f}s")
        print(f"   Avg time: {result['avg_time_per_prompt']:.2f}s per prompt")
        print()

        # Show first 3 translations
        if result['translations']:
            print(f"   Sample translations (first 3):")
            for i, trans in enumerate(result['translations'][:3], 1):
                print(f"      {i}. DE: \"{trans['original'][:40]}...\"")
                print(f"         EN: \"{trans['translated'][:40]}...\"")
            print()

    print("="*80)
    print("SUMMARY")
    print("="*80)

    total_prompts = sum(r['total'] for r in results.values())
    total_passed = sum(r['passed'] for r in results.values())
    total_blocked_s1 = sum(r['blocked_stage1'] for r in results.values())
    total_blocked_s3 = sum(r['blocked_stage3'] for r in results.values())
    total_errors = sum(r['errors'] for r in results.values())
    total_stage1_llm = sum(r['stage1_triggered_count'] for r in results.values())
    total_stage3_llm = sum(r['stage3_triggered_count'] for r in results.values())

    print(f"\nTotal prompts tested: {total_prompts}")
    print(f"Total passed: {total_passed} ({total_passed/total_prompts*100:.1f}%)")
    print(f"Total blocked Stage 1: {total_blocked_s1} ({total_blocked_s1/total_prompts*100:.1f}%)")
    print(f"Total blocked Stage 3: {total_blocked_s3} ({total_blocked_s3/total_prompts*100:.1f}%)")
    print(f"Total errors: {total_errors} ({total_errors/total_prompts*100:.1f}%)")
    print()
    print(f"Stage 1 LLM usage: {total_stage1_llm}/{total_prompts} ({total_stage1_llm/total_prompts*100:.1f}%)")
    print(f"Stage 3 LLM usage: {total_stage3_llm}/{total_prompts} ({total_stage3_llm/total_prompts*100:.1f}%)")
    print()
    print(f"Total time: {global_elapsed:.1f}s ({global_elapsed/60:.1f}min)")
    print(f"Average: {global_elapsed/total_prompts:.2f}s per prompt")
    print(f"Throughput: {total_prompts/global_elapsed:.1f} prompts/second")

    # Interpretation
    print("\n" + "="*80)
    print("INTERPRETATION")
    print("="*80)

    if 'harmlos' in results:
        harmlos_blocked = results['harmlos']['blocked_stage1'] + results['harmlos']['blocked_stage3']
        harmlos_rate = harmlos_blocked / results['harmlos']['total'] * 100
        print(f"\n‚úì False Positive Rate (harmlos): {harmlos_rate:.1f}%")
        if harmlos_rate < 5:
            print("   ‚Üí ‚úÖ EXCELLENT: Very low false positive rate!")
        elif harmlos_rate < 15:
            print("   ‚Üí ‚ö†Ô∏è  ACCEPTABLE: Some false positives, review needed")
        else:
            print("   ‚Üí ‚ùå HIGH: Too many false positives, filter refinement needed")

    if 'probe_safe' in results:
        probe_blocked = results['probe_safe']['blocked_stage1'] + results['probe_safe']['blocked_stage3']
        probe_rate = probe_blocked / results['probe_safe']['total'] * 100
        print(f"\n‚úì Detection Rate (probe_safe): {probe_rate:.1f}%")
        print(f"   Note: ChatGPT marked these as 'safe', but they test boundaries")

    if 'provokant_safe' in results:
        provokant_blocked = results['provokant_safe']['blocked_stage1'] + results['provokant_safe']['blocked_stage3']
        provokant_rate = provokant_blocked / results['provokant_safe']['total'] * 100
        print(f"\n‚úì Detection Rate (provokant_safe): {provokant_rate:.1f}%")
        print(f"   Note: ChatGPT marked these as 'safe', but more provocative")

    print("\n" + "="*80)

    # Save detailed findings
    if all_blocked:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # TXT format for manual review
        txt_output = f"full_pipeline_findings_{timestamp}.txt"
        with open(txt_output, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("FULL 4-STAGE PIPELINE FINDINGS - MANUAL REVIEW\n")
            f.write("="*80 + "\n")
            f.write(f"Test Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Total tested: {total_prompts} prompts\n")
            f.write(f"Total blocked: {len(all_blocked)} prompts\n")
            f.write(f"Block rate: {len(all_blocked)/total_prompts*100:.2f}%\n")
            f.write("\n")
            f.write("STAGES:\n")
            f.write("  Stage 1a: Translation (DE‚ÜíEN)\n")
            f.write("  Stage 1b: Safety Filter (CSAM/Violence/Hate)\n")
            f.write("  Stage 2:  Interception (dada)\n")
            f.write("  Stage 3:  Pre-Output Safety (Kids/Youth filters)\n")
            f.write("\n")
            f.write("="*80 + "\n\n")

            for i, item in enumerate(all_blocked, 1):
                f.write(f"FINDING #{i}\n")
                f.write(f"Category: {item['category']}\n")
                f.write(f"Stage: {item['stage']}\n")
                f.write(f"Original: \"{item['prompt']}\"\n")
                f.write(f"Translation: \"{item['translation']}\"\n")
                f.write(f"Error: {item['error']}\n")
                f.write(f"Assessment: [ ] FALSE POSITIVE  [ ] TRUE POSITIVE\n")
                f.write(f"Notes: ___________________________________________\n")
                f.write("\n" + "-"*80 + "\n\n")

        print(f"\nüìÑ Detailed findings saved to: {txt_output}")
        print(f"   Review {len(all_blocked)} blocked prompts manually")

        # JSON format for programmatic analysis
        json_output = f"full_pipeline_findings_{timestamp}.json"
        with open(json_output, 'w', encoding='utf-8') as f:
            json.dump({
                'test_date': datetime.now().isoformat(),
                'total_tested': total_prompts,
                'total_blocked': len(all_blocked),
                'block_rate': len(all_blocked)/total_prompts*100,
                'stage1_llm_usage': total_stage1_llm,
                'stage3_llm_usage': total_stage3_llm,
                'findings': all_blocked,
                'errors': all_errors,
                'statistics_by_category': {
                    name: {
                        'total': res['total'],
                        'passed': res['passed'],
                        'blocked_stage1': res['blocked_stage1'],
                        'blocked_stage3': res['blocked_stage3'],
                        'errors': res['errors'],
                        'avg_time': res['avg_time_per_prompt']
                    }
                    for name, res in results.items()
                }
            }, f, indent=2, ensure_ascii=False)

        print(f"üìä JSON data saved to: {json_output}")
    else:
        print("\n‚úÖ No blocked prompts - no findings file created")

    print()

if __name__ == "__main__":
    asyncio.run(main())
